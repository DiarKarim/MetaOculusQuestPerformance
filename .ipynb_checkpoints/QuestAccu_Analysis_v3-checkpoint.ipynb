{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 920,
     "status": "ok",
     "timestamp": 1635956208880,
     "user": {
      "displayName": "Diar Karim",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "07169669839868301821"
     },
     "user_tz": 0
    },
    "id": "RWfjlFYOWmRx",
    "outputId": "5fb5f312-9826-4788-dfd8-5b6a6585aa13"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numpy.linalg\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import os\n",
    "import random\n",
    "from scipy.optimize import curve_fit\n",
    "from scipy.ndimage import interpolation\n",
    "from statsmodels.stats.anova import AnovaRM\n",
    "import time\n",
    "import sys\n",
    "import math\n",
    "import pylab as py\n",
    "from scipy.signal import savgol_filter\n",
    "from scipy.optimize import least_squares\n",
    "import seaborn as sns\n",
    "\n",
    "import statannot\n",
    "import scipy.stats as sci \n",
    "# from scipy.stats import norm\n",
    "\n",
    "import pingouin as pg\n",
    "from pingouin import power_rm_anova\n",
    "\n",
    "import matplotlib.mlab as mlb\n",
    "from scipy.signal import find_peaks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "plt.rcParams[\"figure.figsize\"] = (5,5)\n",
    "sns.set_theme(style=\"whitegrid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 23188,
     "status": "ok",
     "timestamp": 1635956232066,
     "user": {
      "displayName": "Diar Karim",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "07169669839868301821"
     },
     "user_tz": 0
    },
    "id": "dew3bjn9Ht96",
    "outputId": "99a50bca-ca40-4b48-e537-852310f0591c"
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z6PJaoRUzREW"
   },
   "source": [
    "# Data Extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1635956235324,
     "user": {
      "displayName": "Diar Karim",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "07169669839868301821"
     },
     "user_tz": 0
    },
    "id": "pAcWmKuzOtLF"
   },
   "outputs": [],
   "source": [
    "def DataExtractor(trackedObject, phase):\n",
    "  \n",
    "    df_interp = None\n",
    "    newArrLength = 1000\n",
    "\n",
    "    trialsMask = (df['info'] == trackedObject) & (df['Phase'] == phase) \n",
    "    trialsOfInterest = pd.unique(df[trialsMask].trialNumber)\n",
    "\n",
    "    for i in trialsOfInterest:\n",
    "    # print(df['inShotRegion'].iloc[i:])\n",
    "    # if df['inShotRegion'].iloc[i] == True:\n",
    "\n",
    "        # Set desired masks\n",
    "        objMask = (df['info'] == trackedObject) & (df['Phase'] == phase) & (df['trialNumber'] == i) # & (df['inShotRegion'] == True)\n",
    "        timeMask = df.loc[(df['info'] == trackedObject) & (df['Phase'] == phase) & (df['trialNumber'] == i),['tyme']]\n",
    "        cueBallMask = (df['info'] == 'Cueball') & (df['Phase'] == phase) & (df['trialNumber'] == i) # This variable mask will help us determine impact time \n",
    "        targetBallMask = (df['info'] == 'TargetBall') & (df['Phase'] == phase) & (df['trialNumber'] == i)\n",
    "\n",
    "        # Resize all variables \n",
    "        ##########################################################################\n",
    "\n",
    "        # Extract data from desired objects (Special cases here)\n",
    "        zPos_cueball = ResizeArray(df[cueBallMask].zPos, newArrLength)\n",
    "        zPos_targball = ResizeArray(df[targetBallMask].zPos, newArrLength)\n",
    "\n",
    "        xRot_interp = ResizeArray(df[objMask].xRot, newArrLength)\n",
    "        yRot_interp = ResizeArray(df[objMask].yRot, newArrLength)\n",
    "        zRot_interp = ResizeArray(df[objMask].zRot, newArrLength)\n",
    "\n",
    "        xPos_interp = ResizeArray(df[objMask].xPos, newArrLength)\n",
    "        yPos_interp = ResizeArray(df[objMask].yPos, newArrLength)\n",
    "        zPos_interp = ResizeArray(df[objMask].zPos, newArrLength)\n",
    "\n",
    "        time_interp = ResizeArray(timeMask.tyme, newArrLength)\n",
    "\n",
    "        ##########################################################################\n",
    "\n",
    "        # Compute angular velocities\n",
    "        xRv = savgol_filter(xRot_interp, 75, 4)\n",
    "        xRot_vel = np.gradient(xRv)\n",
    "        yRv = savgol_filter(yRot_interp, 75, 4)\n",
    "        yRot_vel = np.gradient(yRv)\n",
    "        zRv = savgol_filter(zRot_interp, 75, 4)\n",
    "        zRot_vel = np.gradient(zRv)\n",
    "\n",
    "        # Compute linear velocities \n",
    "        x_v = savgol_filter(xPos_interp, 75, 4)\n",
    "        x_vel = np.gradient(x_v)\n",
    "        y_v = savgol_filter(yPos_interp, 75, 4)\n",
    "        y_vel = np.gradient(y_v)\n",
    "        z_v = savgol_filter(zPos_interp, 75, 4)\n",
    "        z_vel = np.gradient(z_v)\n",
    "\n",
    "        # Save data to new dataframe \n",
    "        dataList = zip(xRot_interp, yRot_interp, zRot_interp, time_interp)\n",
    "        tmpDF_int = pd.DataFrame(dataList, columns=['xRot','yRot','zRot', 'time'])\n",
    "\n",
    "        tmpDF_int.insert(0, \"zPos_targBall\", zPos_targball, True)\n",
    "        tmpDF_int.insert(0, \"zPos_cueBall\", zPos_cueball, True)\n",
    "\n",
    "        tmpDF_int.insert(0, \"zAngVel\", zRot_vel, True)\n",
    "        tmpDF_int.insert(0, \"yAngVel\", yRot_vel, True)\n",
    "        tmpDF_int.insert(0, \"xAngVel\", xRot_vel, True)\n",
    "\n",
    "        tmpDF_int.insert(0, \"zVel\", z_vel, True)\n",
    "        tmpDF_int.insert(0, \"yVel\", y_vel, True)\n",
    "        tmpDF_int.insert(0, \"xVel\", x_vel, True)\n",
    "\n",
    "        tmpDF_int.insert(0, \"zPos\", zPos_interp, True)\n",
    "        tmpDF_int.insert(0, \"yPos\", yPos_interp, True)\n",
    "        tmpDF_int.insert(0, \"xPos\", xPos_interp, True)\n",
    "\n",
    "        tmpDF_int.insert(0, \"Phase\", phase, True)\n",
    "        tmpDF_int.insert(0, \"trialNum\", i, True)\n",
    "\n",
    "\n",
    "        if df_interp is None:\n",
    "            df_interp = tmpDF_int\n",
    "        else:\n",
    "            df_interp = pd.concat((df_interp, tmpDF_int))\n",
    "\n",
    "    return df_interp, trialsOfInterest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--------------- Cross-Correlation ---------------------------------------------\n",
    "\n",
    "def CrossCorr(vel_1, vel_2, sampleFreq):    \n",
    "    \n",
    "    corr = np.correlate(vel_1 - np.mean(vel_1), \n",
    "                      vel_2 - np.mean(vel_2),\n",
    "                      mode='full')\n",
    "    \n",
    "    sampleDifference = np.argmax(vel_2) - np.argmax(vel_1) # What is this 20? np.argmax(vel_2[20:]) - np.argmax(vel_1[20:])\n",
    "\n",
    "    lag = (sampleDifference  * sampleFreq) * 1000\n",
    "    \n",
    "    return lag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HwouB4RGzLde"
   },
   "source": [
    "# Metric extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def VisualizeTrajectories(mask, mask_virt, newArrLength = 100, save_plot=False):\n",
    "    \n",
    "    # Figures for illustration\n",
    "#     newArrLength = 400\n",
    "\n",
    "    # Compute actual sampling rate\n",
    "    timetaken = df_all[mask]['time'].values\n",
    "    timetaken2 = ResizeArray(timetaken, newArrLength)\n",
    "    timetaken3 = np.round(timetaken2,1)\n",
    "    timetaken3 = timetaken3.tolist()\n",
    "\n",
    "    # If 0.0 time isn't present, then use the smallest time value as the start of the trial\n",
    "    try:\n",
    "        indexOfStart = timetaken3.index(0.0) # indexOfStart = np.where(timetaken == 0.0)\n",
    "    except Exception as e:\n",
    "        print('IndexErr: ', e)\n",
    "        minTimeVal = np.nanmin(timetaken3)\n",
    "        indexOfStart = timetaken3.index(minTimeVal)\n",
    "\n",
    "    # Check if devision by zero, because the last value happens to be zero and use last largest value instead\n",
    "    try:\n",
    "        lastMaxTimeVal = timetaken3[-1]\n",
    "        samplingRate = np.round(1.0 / ((len(timetaken3)-indexOfStart) / timetaken3[-1]), 4)\n",
    "    except Exception as e:\n",
    "        print('SampleErr: ', e)\n",
    "        lastMaxTimeVal = np.nanmax(timetaken3[-1-10:-1])\n",
    "        samplingRate = np.round(1.0 / ((len(timetaken3)-indexOfStart) / lastMaxTimeVal), 4)\n",
    "\n",
    "#     print('Sampling Rate: ', np.round(1.0/samplingRate))\n",
    "\n",
    "    # Get individual velocities for real hand ---------------------------------------\n",
    "    pos_x = ResizeArray(df_all[mask]['xPos'].values, newArrLength)\n",
    "    pos_xf = savgol_filter(pos_x, 21, 9)\n",
    "    vel_x = np.gradient(pos_xf / samplingRate)\n",
    "\n",
    "    pos_y = ResizeArray(df_all[mask]['yPos'].values, newArrLength)\n",
    "    pos_yf = savgol_filter(pos_y, 21, 9)\n",
    "    vel_y = np.gradient(pos_yf / samplingRate)\n",
    "\n",
    "    pos_z = ResizeArray(df_all[mask]['zPos'].values, newArrLength)\n",
    "    pos_zf = savgol_filter(pos_z, 21, 9)\n",
    "    vel_z = np.gradient(pos_zf / samplingRate)\n",
    "    vel_type_1 = np.sqrt(np.power(vel_x,2) + np.power(vel_y,2) + np.power(vel_z,2))\n",
    "    vel_type_1f = savgol_filter(vel_type_1, 21, 9)\n",
    "\n",
    "    # Get individual velocities for virtual hand ---------------------------------------\n",
    "    pos_xv = ResizeArray(df_all[mask_virt]['xPos'].values, newArrLength)\n",
    "    pos_xfv = savgol_filter(pos_xv, 21, 9)\n",
    "    vel_xv = np.gradient(pos_xfv / samplingRate)\n",
    "\n",
    "    pos_yv = ResizeArray(df_all[mask_virt]['yPos'].values, newArrLength)\n",
    "    pos_yfv = savgol_filter(pos_yv, 21, 9)\n",
    "    vel_yv = np.gradient(pos_yfv / samplingRate)\n",
    "\n",
    "    pos_zv = ResizeArray(df_all[mask_virt]['zPos'].values, newArrLength)\n",
    "    pos_zfv = savgol_filter(pos_zv, 21, 9)\n",
    "    vel_zv = np.gradient(pos_zfv / samplingRate)\n",
    "\n",
    "    vel_type_1v = np.sqrt(np.power(vel_xv,2) + np.power(vel_yv,2) + np.power(vel_zv,2))\n",
    "    vel_type_1fv = savgol_filter(vel_type_1v, 21, 9)\n",
    "\n",
    "    pos_tx = df_all[mask]['xTPos'].values\n",
    "    pos_ty = df_all[mask]['yTPos'].values\n",
    "    pos_tz = df_all[mask]['zTPos'].values\n",
    "\n",
    "    # Velocity around start of trial\n",
    "    startIdx = int(np.round(indexOfStart*0.5))\n",
    "    siMargin = int(np.round(startIdx * 0.95)) # This gurantees that the array is always long enough rather than giving an arbitrary fixed scalar value as margins\n",
    "#     print('Start Index Margin: ', siMargin)\n",
    "\n",
    "    realVel = vel_type_1f[startIdx-siMargin:startIdx+siMargin]\n",
    "    virtVel = vel_type_1fv[startIdx-siMargin:startIdx+siMargin]\n",
    "\n",
    "    # Pos\n",
    "    plt.figure()\n",
    "    #                     plt.subplot(1,3,1)\n",
    "    plt.plot(pos_x, pos_z,'k-o', linewidth=2) # Real Trajectory \n",
    "    plt.plot(pos_xv, pos_zv,'m-o', linewidth=2) # Virtual Trajectory \n",
    "#     plt.plot((pos_x[startIdx] + pos_xv[startIdx])/2,pos_z[startIdx],'cs',linewidth=4) # Start time point\n",
    "    plt.plot([pos_x[startIdx]-0.02, pos_xv[startIdx]+0.02],[pos_z[startIdx], pos_z[startIdx]],'c--',linewidth=4) # Start time point\n",
    "    plt.plot(pos_tx, pos_tz,'r-o',ms=20, alpha=0.5) # Target position\n",
    "    plt.legend(['Marker','Virtual','Start', 'Target'])\n",
    "    plt.plot([pos_x, pos_xv], [pos_z, pos_zv],'b-',alpha=0.5) # Connection line between trajectories to indicate delays\n",
    "    \n",
    "    plt.title('Example Trajectory', fontsize=16.5)\n",
    "    plt.xlabel('X-Axis (m)', fontsize=14)\n",
    "    plt.ylabel('Z-Axis (m)', fontsize=14)\n",
    "    plt.grid(False)\n",
    "\n",
    "    if save_plot:\n",
    "        plt.savefig(str(np.round(time.time())) + '_Trajectory.jpg', dpi=600, bbox_inches='tight')\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "    # Vel\n",
    "    plt.figure()\n",
    "    #                     plt.subplot(1,3,2)\n",
    "#     plt.plot(vel_type_1,'r-')\n",
    "    plt.plot(vel_type_1f,'k-', linewidth=2)\n",
    "    plt.plot(vel_type_1fv,'m-', linewidth=2)\n",
    "    plt.plot([startIdx,startIdx],[0,2.75],'c--',linewidth=4)\n",
    "    plt.title('Vel_Type_1')\n",
    "    plt.legend(['Marker','Virtual','Start'])\n",
    "    #         plt.legend(bbox_to_anchor=(1.05, 1.0), loc='upper left')\n",
    "    #         plt.axis('equal')\n",
    "#     plt.ylim([-0.1, 1.4])\n",
    "\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    print('LastTime: ', lastMaxTimeVal)\n",
    "    timeInSec = np.arange(0,lastMaxTimeVal, (lastMaxTimeVal/6))\n",
    "    timeInSec = np.round(timeInSec, 2)\n",
    "    \n",
    "    plt.title('Resultant Velocity',fontsize=16.5)\n",
    "    plt.xlabel('Time (s)',fontsize=14)\n",
    "    plt.ylim([-0.1, 2.5])\n",
    "    \n",
    "    try:\n",
    "        plt.xticks([0,20,40,60,80,100],timeInSec)\n",
    "    except Exception as e:\n",
    "        print('X-Ticks Error: ', e)\n",
    "        \n",
    "    plt.ylabel(\"Velocity $\\mathregular{ms^{-1}}$\",fontsize=14)\n",
    "    plt.grid(False)\n",
    "\n",
    "    if save_plot:\n",
    "        plt.savefig(str(np.round(time.time())) + '_Velocity.jpg', dpi=600, bbox_inches='tight')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "        # Last positional data point in different colours for visualisation purposes \n",
    "#     plt.figure()\n",
    "#     #                     plt.subplot(1,3,3)\n",
    "#     plt.plot(pos_x[-1-5:-1], pos_z[-1-5:-1],'k-o', ms=8)\n",
    "#     plt.plot(pos_xv[-1-5:-1], pos_zv[-1-5:-1],'m-o', ms=6)\n",
    "#     plt.plot([pos_x[-1], pos_xv[-1]], [pos_z[-1], pos_zv[-1]], 'r-o', ms=15, alpha=0.5)\n",
    "\n",
    "#     #         plt.legend(bbox_to_anchor=(1.05, 1.0), loc='upper left')\n",
    "#     #         plt.axis('equal')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 262,
     "status": "ok",
     "timestamp": 1635956234317,
     "user": {
      "displayName": "Diar Karim",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "07169669839868301821"
     },
     "user_tz": 0
    },
    "id": "v0QsxMgCWwsj"
   },
   "outputs": [],
   "source": [
    "def ResizeArray(data, newSize):\n",
    "    x = data\n",
    "    i = newSize\n",
    "    z = i / len(x)\n",
    "    x_int = interpolation.zoom(x,z)\n",
    "\n",
    "    return x_int\n",
    "\n",
    "def AngularCorrection(data):\n",
    "    outputarr = []\n",
    "    for i in data:\n",
    "        if i>180:\n",
    "            outputarr.append(i - 270)\n",
    "        else:\n",
    "            outputarr.append(i + 90)\n",
    "\n",
    "    return outputarr\n",
    "\n",
    "def AverageCurve(data,col):\n",
    "    # plt.figure()\n",
    "    ydata = np.mean(data,axis=0)\n",
    "    xvls = np.linspace(0,len(ydata),len(ydata)) \n",
    "    yerr = np.std(data, axis=0) / np.sqrt(np.shape(data)[0])\n",
    "    plt.plot(xvls,ydata,color=col)\n",
    "    plt.fill_between(xvls, ydata-yerr, ydata+yerr, alpha=0.5,color=[0.1,0.1,0.1])\n",
    "\n",
    "# marker='s', mfc='red', mec='red', ms=5, mew=2,\n",
    "\n",
    "def PlotErrorBars(dataX = np.tile(np.nan,10), dataY = np.tile(np.nan,10), dataZ = np.tile(np.nan,10), colorz = 'r'):\n",
    "    # Clean up outliers 5x outside the mean  \n",
    "    xData =  [(i * np.nan) if i > (np.nanmean(dataX) * 5.0) else i for i in dataX]\n",
    "    yData =  [(i * np.nan) if i > (np.nanmean(dataY) * 5.0) else i for i in dataY]\n",
    "    zData =  [(i * np.nan) if i > (np.nanmean(dataZ) * 5.0) else i for i in dataZ]\n",
    "\n",
    "    # Compute standard errors\n",
    "    x_SE  = np.std(xData, axis=0) / np.sqrt(np.shape(xData)[0])\n",
    "    y_SE  = np.std(yData, axis=0) / np.sqrt(np.shape(yData)[0])\n",
    "    z_SE  = np.std(zData, axis=0) / np.sqrt(np.shape(zData)[0])\n",
    "    all_SE = [x_SE, y_SE, z_SE]\n",
    "\n",
    "    # Plot data \n",
    "    plt.errorbar([0,1,2], [np.nanmean(xData),np.nanmean(yData),np.nanmean(zData)], all_SE, color = colorz, marker='s')\n",
    "\n",
    "\n",
    "def PlotErrorBars2(dataX = np.tile(np.nan,10), colorz = 'r'):\n",
    "    # Clean up outliers 5x outside the mean  \n",
    "    xData =  [(i * np.nan) if i > (np.nanmean(dataX) * 5.0) else i for i in dataX]\n",
    "\n",
    "    # Compute standard errors\n",
    "    x_SE  = np.std(xData, axis=0) / np.sqrt(np.shape(xData)[0])\n",
    "\n",
    "    # Plot data \n",
    "    plt.errorbar([0], np.nanmean(xData), x_SE, color = colorz, marker='s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1635956236400,
     "user": {
      "displayName": "Diar Karim",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "07169669839868301821"
     },
     "user_tz": 0
    },
    "id": "Xt7GtanCzKm2"
   },
   "outputs": [],
   "source": [
    "\n",
    "def AverageAngPos(df_int, trials):\n",
    "    xRots, yRots, zRots = [], [], []\n",
    "    xPosAv, yPosAv, zPosAv = [],[],[]\n",
    "    for i in trials:\n",
    "        mask_1 = (df_int['trialNum'] == i) \n",
    "        xRots.append(df_int.xRot[mask_1])\n",
    "        yRots.append(df_int.yRot[mask_1])\n",
    "        zRots.append(df_int.zRot[mask_1])\n",
    "        xPosAv.append(df_int.xPos[mask_1])\n",
    "        yPosAv.append(df_int.yPos[mask_1])\n",
    "        zPosAv.append(df_int.zPos[mask_1])\n",
    "        \n",
    "    return xRots, yRots, zRots, xPosAv, yPosAv, zPosAv\n",
    "\n",
    "def AvAngVel(df_int, trials):\n",
    "    xAngVels, yAngVels, zAngVels = [],[],[]\n",
    "    for i in trials:\n",
    "        mask_1 = (df_int['trialNum'] == i) \n",
    "        xAngVels.append(np.gradient(savgol_filter(df_int.xRot[mask_1], 75, 4)))\n",
    "        yAngVels.append(np.gradient(savgol_filter(df_int.yRot[mask_1], 75, 4)))\n",
    "        zAngVels.append(np.gradient(savgol_filter(df_int.zRot[mask_1], 75, 4)))\n",
    "    return xAngVels, yAngVels, zAngVels\n",
    "\n",
    "def AvMaxVels(df_int, trials):\n",
    "  \n",
    "    # cueballZpos, targetballZpos = [], []\n",
    "    xVels, yVels, zVels = [],[],[]\n",
    "    maxXVels, maxYVels, maxZVels = [],[],[]\n",
    "    maxAngVel = []\n",
    "\n",
    "    for i in trials:\n",
    "        mask_1 = (df_int['trialNum'] == i) \n",
    "\n",
    "        xVel = savgol_filter(np.gradient(df_int.xPos[mask_1]), 75,4)\n",
    "        xVels.append(xVel)\n",
    "        yVel = savgol_filter(np.gradient(df_int.yPos[mask_1]), 75,4)\n",
    "        yVels.append(yVel)\n",
    "        zVel = savgol_filter(np.gradient(df_int.zPos[mask_1]), 75,4)\n",
    "        zVels.append(zVel)\n",
    "\n",
    "        maxXVels.append(np.max(xVel[100:400]))\n",
    "        maxYVels.append(np.max(yVel[100:400]))\n",
    "        maxZVels.append(np.max(zVel[100:400]))\n",
    "\n",
    "        # cueballZpos.append(df_int.zPos_cueBall[mask_1])\n",
    "        # targetballZpos.append(df_int.zPos_targBall[mask_1])\n",
    "\n",
    "    return xVels, yVels, zVels, maxXVels, maxYVels, maxZVels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def ExtractMetrics(df_all, tempores, heightes):\n",
    "\n",
    "#     tempores = '80'\n",
    "#     heightes = 'low'\n",
    "\n",
    "    df_metrics = None\n",
    "    np.set_printoptions(suppress=True)\n",
    "    newArrLength = 100\n",
    "    plotting = True\n",
    "    keepPlottingFor = 10\n",
    "\n",
    "    # mask = (df_all['PtxID'] == 'Susan') & (df_all['gameObjectName'] == 'realFingerTip') & (df_all['height'] == 'low') & (df_all['tempos'] == '80') & (df_all['TrialNum'] == '42')\n",
    "    ptxIds = pd.unique(df_all['PtxID'])\n",
    "    print('Participants: ', ptxIds)\n",
    "    # Ptx number: \n",
    "    # 0 = Susan\n",
    "    # 1 = Davide\n",
    "    # 2 = Joe # <- Exluded participant due to missing tempo labels \n",
    "    # 3 = Poppy\n",
    "    # 4 = Katrina\n",
    "    # 5 = Max # <- Exluded participant due to missing tempo labels \n",
    "    # 6 = Pete\n",
    "    includedPtxs = [0,1,3,4,6]\n",
    "\n",
    "    trials = np.arange(0,72)\n",
    "\n",
    "    masktemp = (df_all['gameObjectName'] == 'realFingerTip')\n",
    "    trialNumbs = pd.unique(df_all[masktemp].TrialNum)\n",
    "    \n",
    "    for party in includedPtxs:\n",
    "\n",
    "        ptxNum = party\n",
    "\n",
    "        for trial in trials:\n",
    "\n",
    "            print('\\nTrial number: ', trial)\n",
    "\n",
    "            try:\n",
    "                mask = (df_all['TrialNum'] == str(trial)) & (df_all['PtxID'] == ptxIds[ptxNum]) & (df_all['tempos'] == tempores) & (df_all['gameObjectName'] == 'realFingerTip') & (df_all['height'] == heightes)\n",
    "                mask_virt = (df_all['TrialNum'] == str(trial)) & (df_all['PtxID'] == ptxIds[ptxNum]) & (df_all['tempos'] == tempores) & (df_all['gameObjectName'] == 'r_index_fingernail_marker') & (df_all['height'] == heightes)\n",
    "\n",
    "                # Compute actual sampling rate\n",
    "                timetaken = df_all[mask]['time'].values\n",
    "                timetaken2 = ResizeArray(timetaken, newArrLength)\n",
    "                timetaken3 = np.round(timetaken2,1)\n",
    "                timetaken3 = timetaken3.tolist()\n",
    "\n",
    "                # If 0.0 time isn't present, then use the smallest time value as the start of the trial\n",
    "                try:\n",
    "                    indexOfStart = timetaken3.index(0.0) # indexOfStart = np.where(timetaken == 0.0)\n",
    "                except Exception as e:\n",
    "                    print('IndexErr: ', e)\n",
    "                    minTimeVal = np.nanmin(timetaken3)\n",
    "                    indexOfStart = timetaken3.index(minTimeVal)\n",
    "\n",
    "                # Check if devision by zero, because the last value happens to be zero and use last largest value instead\n",
    "                try:\n",
    "                    samplingRate = np.round(1.0 / ((len(timetaken3)-indexOfStart) / timetaken3[-1]), 4)\n",
    "                except Exception as e:\n",
    "                    print('SampleErr: ', e)\n",
    "                    lastMaxTimeVal = np.nanmax(timetaken3[-1-10:-1])\n",
    "                    samplingRate = np.round(1.0 / ((len(timetaken3)-indexOfStart) / lastMaxTimeVal), 4)\n",
    "\n",
    "                print('Sampling Rate: ', np.round(1.0/samplingRate))\n",
    "\n",
    "                # Get individual velocities for real hand ---------------------------------------\n",
    "                pos_x = ResizeArray(df_all[mask]['xPos'].values, newArrLength)\n",
    "                pos_xf = savgol_filter(pos_x, 21, 9)\n",
    "                vel_x = np.gradient(pos_xf / samplingRate)\n",
    "\n",
    "                pos_y = ResizeArray(df_all[mask]['yPos'].values, newArrLength)\n",
    "                pos_yf = savgol_filter(pos_y, 21, 9)\n",
    "                vel_y = np.gradient(pos_yf / samplingRate)\n",
    "\n",
    "                pos_z = ResizeArray(df_all[mask]['zPos'].values, newArrLength)\n",
    "                pos_zf = savgol_filter(pos_z, 21, 9)\n",
    "                vel_z = np.gradient(pos_zf / samplingRate)\n",
    "                vel_type_1 = np.sqrt(np.power(vel_x,2) + np.power(vel_y,2) + np.power(vel_z,2))\n",
    "                vel_type_1f = savgol_filter(vel_type_1, 21, 9)\n",
    "\n",
    "                # Get individual velocities for virtual hand ---------------------------------------\n",
    "                pos_xv = ResizeArray(df_all[mask_virt]['xPos'].values, newArrLength)\n",
    "                pos_xfv = savgol_filter(pos_xv, 21, 9)\n",
    "                vel_xv = np.gradient(pos_xfv / samplingRate)\n",
    "\n",
    "                pos_yv = ResizeArray(df_all[mask_virt]['yPos'].values, newArrLength)\n",
    "                pos_yfv = savgol_filter(pos_yv, 21, 9)\n",
    "                vel_yv = np.gradient(pos_yfv / samplingRate)\n",
    "\n",
    "                pos_zv = ResizeArray(df_all[mask_virt]['zPos'].values, newArrLength)\n",
    "                pos_zfv = savgol_filter(pos_zv, 21, 9)\n",
    "                vel_zv = np.gradient(pos_zfv / samplingRate)\n",
    "\n",
    "                vel_type_1v = np.sqrt(np.power(vel_xv,2) + np.power(vel_yv,2) + np.power(vel_zv,2))\n",
    "                vel_type_1fv = savgol_filter(vel_type_1v, 21, 9)\n",
    "                # ------------------------------------------------------------------------------\n",
    "\n",
    "                pos_tx = df_all[mask]['xTPos'].values\n",
    "                pos_ty = df_all[mask]['yTPos'].values\n",
    "                pos_tz = df_all[mask]['zTPos'].values\n",
    "\n",
    "                # Velocity around start of trial\n",
    "                startIdx = indexOfStart\n",
    "                siMargin = int(np.round(startIdx * 0.95)) # This gurantees that the array is always long enough rather than giving an arbitrary fixed scalar value as margins\n",
    "                print('Start Index Margin: ', siMargin)\n",
    "\n",
    "                realVel = vel_type_1f[startIdx-siMargin:startIdx+siMargin]\n",
    "                virtVel = vel_type_1fv[startIdx-siMargin:startIdx+siMargin]\n",
    "\n",
    "                \n",
    "#                 timeMask = df_all['TrialNum'] == trialNumbs[trial]\n",
    "#                 times = df_all[timeMask].time.tolist()\n",
    "                \n",
    "                try:    \n",
    "#                     lag = CrossCorr(realVel, virtVel, samplingRate)\n",
    "                    lag = CrossCorr2(realVel, virtVel, samplingRate)\n",
    "                    lagings2 = np.abs(np.round(1000 * (samplingRate * lag), 5)) # sampleRate = sampleTime\n",
    "                except Exception as e:\n",
    "                    print('LagErr: ', e)\n",
    "                    realVel = np.zeros(100)\n",
    "                    virtVel = np.zeros(100)\n",
    "                    lagings2 = np.nan()\n",
    "                    \n",
    "                \n",
    "                lagings2 = np.sqrt(lagings2*lagings2)\n",
    "                print('Lag: ', lagings2, ' ms')\n",
    "\n",
    "\n",
    "                maxVel = np.max(realVel)\n",
    "                maxVelVirt = np.max(virtVel)\n",
    "                print('Max vel: \\n', 'Real: ', np.round(maxVel,2),'\\n', 'Virt: ', np.round(maxVelVirt,2))\n",
    "\n",
    "                real = np.asarray([pos_x,pos_z])\n",
    "                virt = np.asarray([pos_xv,pos_zv])\n",
    "\n",
    "                manhattanErr = np.sum(np.abs(real-virt)) / len(pos_x)\n",
    "\n",
    "                ns = 5\n",
    "                p1m = [np.nanmean(pos_x[-1-ns:-1]), np.nanmean(pos_z[-1-ns:-1])]\n",
    "                p2m = [np.nanmean(pos_xv[-1-ns:-1]), np.nanmean(pos_zv[-1-ns:-1])]\n",
    "                distance2 = np.round(math.sqrt( ((p1m[0]-p2m[0])**2)+((p1m[1]-p2m[1])**2) ) * 100,2)\n",
    "\n",
    "                pathOffset = np.round(manhattanErr * 100,2)\n",
    "                print('Per sample path offset (Manhattan err): ', pathOffset, ' cm')\n",
    "                print('End point err: ', distance2, ' cm')\n",
    "\n",
    "                \n",
    "                # Lag corrected path offset -------------------------------------------------\n",
    "#                 averageLag = np.round(np.nanmean(lagings),3)\n",
    "                # Absolute angular error  \n",
    "                try:\n",
    "                    lag2 = np.sqrt(lag*lag) # Roll back by absolute number of lag samples \n",
    "                    rollVal = int(np.round(lag2))\n",
    "                    offsetDiff = np.roll(real,0) - np.roll(virt,-rollVal)\n",
    "                    manhattanErr_2 = np.sum(np.abs(offsetDiff)) / len(pos_x)\n",
    "                    pathOffset_2 = np.round(manhattanErr_2 * 100,2)\n",
    "                except:\n",
    "                    pathOffset_2 = pathOffset\n",
    "                    \n",
    "                # Lag corrected path offset end ---------------------------------------------\n",
    "                    \n",
    "\n",
    "                \n",
    "                targetID = df_all[mask]['targetID'][0]\n",
    "                lateralPos = ''\n",
    "                if '1' in targetID or '2' in targetID: \n",
    "                    lateralPos = 'left'\n",
    "                elif '5' in targetID or '6' in targetID:\n",
    "                    lateralPos = 'right'\n",
    "                else:\n",
    "                    lateralPos = 'center'\n",
    "                \n",
    "                verticalPos = ''\n",
    "                if 'A' in targetID:\n",
    "                    verticalPos = 'far'\n",
    "                elif 'C' in targetID:\n",
    "                    verticalPos = 'close'\n",
    "                else:\n",
    "                    verticalPos = 'middle'\n",
    "                \n",
    "                data = {'PtxID' : [ptxIds[ptxNum]], \n",
    "                        'TrialNum' : [str(trial)], \n",
    "                        'TargetID' : [targetID],\n",
    "                        'TargetLateral' : [lateralPos],\n",
    "                        'TargetVertical' : [verticalPos],\n",
    "                        'MaxVel_Real': [maxVel], \n",
    "                        'MaxVel_Virt': [maxVelVirt],\n",
    "                        'Lag' : [lagings2], \n",
    "                        'PathOffset' : [pathOffset], \n",
    "                        'PathOffsetNoLag' : [pathOffset_2], \n",
    "                        'EndError' : [distance2]}\n",
    "\n",
    "                tmpDF = pd.DataFrame.from_dict(data)\n",
    "\n",
    "                if df_metrics is None:\n",
    "                    df_metrics = tmpDF\n",
    "                else:\n",
    "                    df_metrics = pd.concat((df_metrics, tmpDF))\n",
    "\n",
    "                if plotting and trial < keepPlottingFor:\n",
    "                    # Pos\n",
    "                    plt.figure()\n",
    "#                     plt.subplot(1,3,1)\n",
    "                    plt.plot(pos_x, pos_z,'k-o') # Real Trajectory \n",
    "                    plt.plot(pos_xv, pos_zv,'m-o') # Virtual Trajectory \n",
    "                    plt.plot(pos_x[startIdx],pos_z[startIdx],'cs',ms=10) # Start time point\n",
    "                    plt.plot(pos_tx, pos_tz,'r-o',ms=10) # Target position\n",
    "                    plt.legend(['Real Trajectory','Virtual Trajectory','Start', 'Target'])\n",
    "                    plt.plot([pos_x, pos_xv], [pos_z, pos_zv],'b-',alpha=0.5) # Connection line between trajectories to indicate delays\n",
    "\n",
    "                    # Last positional data point in different colours for visualisation purposes \n",
    "                    plt.figure()\n",
    "#                     plt.subplot(1,3,3)\n",
    "                    plt.plot(pos_x[-1-5:-1], pos_z[-1-5:-1],'k-o', ms=8)\n",
    "                    plt.plot(pos_xv[-1-5:-1], pos_zv[-1-5:-1],'m-o', ms=6)\n",
    "                    plt.plot([pos_x[-1], pos_xv[-1]], [pos_z[-1], pos_zv[-1]], 'r-o', ms=15, alpha=0.5)\n",
    "\n",
    "            #         plt.legend(bbox_to_anchor=(1.05, 1.0), loc='upper left')\n",
    "            #         plt.axis('equal')\n",
    "\n",
    "                    # Vel\n",
    "                    plt.figure()\n",
    "#                     plt.subplot(1,3,2)\n",
    "                    plt.plot(vel_type_1,'r-')\n",
    "                    plt.plot(vel_type_1f,'g-')\n",
    "                    plt.plot(vel_type_1fv,'m-')\n",
    "                    plt.plot([startIdx,startIdx],[0,2.75],'c--',linewidth=3)\n",
    "                    plt.title('Vel_Type_1')\n",
    "                    plt.legend(['Raw','Filtered','Filt Virt','Start'])\n",
    "            #         plt.legend(bbox_to_anchor=(1.05, 1.0), loc='upper left')\n",
    "            #         plt.axis('equal')\n",
    "                    plt.ylim([-0.1, 5])\n",
    "\n",
    "                    plt.tight_layout()\n",
    "                    plt.show()\n",
    "\n",
    "            except Exception as e:\n",
    "                print('Err: ', e)\n",
    "\n",
    "    return df_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1635956245031,
     "user": {
      "displayName": "Diar Karim",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "07169669839868301821"
     },
     "user_tz": 0
    },
    "id": "gvzUNpenGvxZ"
   },
   "outputs": [],
   "source": [
    "def ReadData(path, file, height, tempo):\n",
    "    # # Define data frame variable\n",
    "    df = None \n",
    "\n",
    "    # Load each file into the data frame \n",
    "    for i in range(len(files)): \n",
    "\n",
    "        if \".json\" in files[i] and \"height\" in files[i] and \"tempo\" in files[i]:  \n",
    "            # if \"txt.json\" in files[i] and \"Phase\" in files[i]:\n",
    "            # print(files[i])\n",
    "\n",
    "            # Extract file name info and add to the dataframe \n",
    "            fileWords = files[i].split(\"_\")\n",
    "\n",
    "            # Extract user ID \n",
    "            idx = fileWords.index(\"bpm\")\n",
    "            userID = fileWords[idx - 2]\n",
    "\n",
    "            # Add trial number to data frame \n",
    "            tmpDF = pd.read_json(path + files[i])\n",
    "            tmpDF.insert(0, \"UserID\", userID, True)\n",
    "\n",
    "            if df is None:\n",
    "                df = tmpDF\n",
    "            else:\n",
    "                df = pd.concat((df, tmpDF))\n",
    "            \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CleanUpnNorm(df_metrics_all, metric='', norm_param=1.0):\n",
    "    \n",
    "    from collections import Counter\n",
    "    from sklearn.preprocessing import PowerTransformer\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from scipy.stats import boxcox\n",
    "    from scipy.stats import yeojohnson\n",
    "    \n",
    "    colors = [\"#D81B60\", \"#0188FF\", \"#FFC107\", \"#B7A2FF\", \"#000000\", \"#2EC5AC\"]\n",
    "    BINS = 30\n",
    "    data = df_metrics_all[metric]\n",
    "\n",
    "#------------------------------------------------------------------------------------------------\n",
    "#-------------------------------- Identify and remove outliers ----------------------------------\n",
    "#------------------------------------------------------------------------------------------------\n",
    "    \n",
    "    # Interquartile method    \n",
    "    q25, q75 = np.percentile(data, 25), np.percentile(data, 75)\n",
    "    iqr = q75 - q25\n",
    "    print('Percentiles: 25th=%.3f, 75th=%.3f, IQR=%.3f' % (q25, q75, iqr))\n",
    "\n",
    "    # calculate the outlier cutoff\n",
    "    cut_off = iqr * 1.5\n",
    "    lower, upper = q25 - cut_off, q75 + cut_off\n",
    "\n",
    "    # # identify and remove outliers\n",
    "    data_or = []\n",
    "    for x in data:\n",
    "        if x >= lower and x <= upper:\n",
    "            data_or.append(x)\n",
    "        else:\n",
    "            data_or.append(np.nan)\n",
    "\n",
    "    print('Original array length: ', len(data_or))\n",
    "    print('Array length without outliers: ', len(data_or)-np.round((data_or.count(np.nan))))\n",
    "    print('Percentage of data excluded: ', np.round((data_or.count(np.nan) / len(data_or)) * 100), ' %')\n",
    "\n",
    "#------------------------------------------------------------------------------------------------\n",
    "#----------------------------- Map data to normal distribution ----------------------------------\n",
    "#------------------------------------------------------------------------------------------------\n",
    "    \n",
    "#     yj = PowerTransformer(method=\"yeo-johnson\")\n",
    "\n",
    "    X = np.asarray(data_or)\n",
    "    X = X.reshape(-1, 1)\n",
    "\n",
    "    # Mapped to normal distribution \n",
    "    data_norm = yeojohnson(X, norm_param)\n",
    "\n",
    "    plt.figure()\n",
    "    plt.subplot(1,2,1)\n",
    "    plt.hist(X, bins=BINS)\n",
    "    plt.title('Original Data')\n",
    "    plt.ylabel('Error')\n",
    "    plt.ylim([0, 300])\n",
    "\n",
    "    plt.subplot(1,2,2)\n",
    "    plt.hist(data_norm, color=colors[0], bins=BINS)\n",
    "    plt.title('Transformed Data')\n",
    "    plt.ylim([0, 300])\n",
    "\n",
    "    W, p = sci.shapiro(data_norm)\n",
    "    print('Shap YT', ' W: ', np.round(W, 4), ' p: ', np.round(p, 10))\n",
    "    \n",
    "    return data_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ComputeLag(Angle_real, Angle_virt, times):\n",
    "    \n",
    "    trialDuration = int(np.round(times[-1], 3))\n",
    "    \n",
    "    # Lag between virtual and real angular signals \n",
    "    # Measure peaks based on average max height using a moving average \n",
    "    mH_real = []\n",
    "    mH_virt = []\n",
    "    lenX = len(Angle_real)\n",
    "    window_size = 100 \n",
    "\n",
    "    for i in range(len(Angle_real) - window_size + 1):\n",
    "        maxHei_real = np.nanmax(Angle_real[i: i + window_size])\n",
    "        maxHei_virt = np.nanmax(Angle_virt[i: i + window_size])\n",
    "        mH_real.append(maxHei_real)\n",
    "        mH_virt.append(maxHei_virt)\n",
    "\n",
    "    #         print('Max Height: ', maxHei)\n",
    "\n",
    "    meanMaxH_real = np.nanmean(mH_real)\n",
    "    meanMaxH_virt = np.nanmean(mH_virt)\n",
    "\n",
    "    pks_real, _ = find_peaks(Angle_real, height=meanMaxH_real*0.6, distance=50)\n",
    "#     plt.plot(pks_real, Angle_real[pks_real], 'rx', ms=14)\n",
    "\n",
    "    pks_virt, _ = find_peaks(Angle_virt, height=meanMaxH_virt*0.6, distance=50)\n",
    "#     plt.plot(pks_virt, Angle_virt[pks_virt], 'rx', ms=14)\n",
    "\n",
    "    plt.plot(Angle_real,'r')\n",
    "    plt.plot(Angle_virt,'g')\n",
    "    plt.plot(pks_real, Angle_real[pks_real], 'rx', ms=14)\n",
    "    plt.plot(pks_virt, Angle_virt[pks_virt], 'rx', ms=14)\n",
    "\n",
    "    # Lag based on difference between angular peaks \n",
    "    sampleTime = 1000 * ( np.round( (1.0 / (len(times) / trialDuration)), 3 ))\n",
    "    lag = []\n",
    "    for c, p in enumerate(pks_real):\n",
    "        try:\n",
    "            if p-4 <= pks_virt[c] <= p+4:\n",
    "                lag.append(sampleTime * (pks_virt[c] - p))\n",
    "        except Exception as e:\n",
    "            print('Err: ', e)\n",
    "\n",
    "    averageLag = np.round(np.nanmean(lag),3)\n",
    "    #     print('Lag: ', int(np.round(averageLag)))\n",
    "    \n",
    "    return averageLag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ComputeErrors(df_in):\n",
    "    masktemp = (df_in['gameObjectName'] == 'realFingerTip')\n",
    "    adaptationTrialNumbers = pd.unique(df_in[masktemp].trialNumber)\n",
    "    # np.random.shuffle(adaptationTrialNumbers)\n",
    "\n",
    "    df_out = None # If arrays are to be saved use this \n",
    "    dat_List = []\n",
    "\n",
    "    for i in range(len(adaptationTrialNumbers)):\n",
    "        realFingerMask = (df_in['gameObjectName'] == 'realFingerTip') & (df_in['trialNumber'] == adaptationTrialNumbers[i])\n",
    "        virtualFingerMask = (df_in['gameObjectName'] == 'r_index_fingernail_marker') & (df_in['trialNumber'] == adaptationTrialNumbers[i])\n",
    "        ptxMask = (df_in['gameObjectName'] == 'r_index_fingernail_marker') & (df_in['trialNumber'] == adaptationTrialNumbers[i])\n",
    "\n",
    "        # timeMask = df.loc[(df['gameObjectName'] == 'realFingerTip') & (df['trialNumber'] == adaptationTrialNumbers[i]), ['time']]\n",
    "        timeMask = df_in['trialNumber'] == adaptationTrialNumbers[i]\n",
    "        \n",
    "        try:\n",
    "            ptx = df_in[ptxMask]['PtxID'].values[0] # This only results in one participant from the cohort \n",
    "\n",
    "            plt.figure(1) # All positions\n",
    "            plt.plot(df_in[realFingerMask].xPos, df_in[realFingerMask].zPos,'r')\n",
    "            plt.plot(df_in[virtualFingerMask].xPos, df_in[virtualFingerMask].zPos,'g')\n",
    "            plt.title('X-Z Position / m')\n",
    "            plt.gca().set_aspect('equal', adjustable='box')\n",
    "            plt.gca().set_aspect('equal', adjustable='box')\n",
    "            plt.legend(['Real','Virtual'])\n",
    "\n",
    "            ax = plt.figure(2)\n",
    "            tangXZ_Real = np.sqrt(np.power(df_in[realFingerMask].xPos,2) + np.power(df_in[realFingerMask].zPos,2))\n",
    "            tangXZ_Virt = np.sqrt(np.power(df_in[virtualFingerMask].xPos,2) + np.power(df_in[virtualFingerMask].zPos,2))\n",
    "\n",
    "            print('Trial: ', i)\n",
    "\n",
    "            try:\n",
    "                tangXZ_Real_Vel = np.abs(np.diff(savgol_filter(tangXZ_Real, 75, 4)))\n",
    "                tangXZ_Virt_Vel = np.abs(np.diff(savgol_filter(tangXZ_Virt, 75, 4)))\n",
    "\n",
    "\n",
    "                print('Past issue: ', i)\n",
    "\n",
    "                plt.plot(tangXZ_Real_Vel,'r')\n",
    "                plt.plot(tangXZ_Virt_Vel,'g')\n",
    "                plt.title('Lateral Velocity (x-z axis) $\\mathregular{ms^{-1}}$')\n",
    "\n",
    "                # Sampling Frequency and Time\n",
    "                times = df_in[timeMask].time.tolist()\n",
    "                val = np.where(times == numpy.amin(times))\n",
    "                startTimeIdx = val[0][0]\n",
    "                # print('min: ', startTimeIdx)\n",
    "\n",
    "                print('Past 2nd issue: ', i)\n",
    "\n",
    "                plt.figure(3) \n",
    "                plt.plot(times)\n",
    "                plt.plot(startTimeIdx, times[startTimeIdx], 'rx')\n",
    "                # print('Movement duration: ', times[-1], 's')\n",
    "                startMovIdx = int(np.round((startTimeIdx/10))) # Convert between time series and movement array by dividing by 10? \n",
    "                sampleFreq = np.round(len(tangXZ_Real_Vel[startMovIdx:])/times[-1])\n",
    "                # print('Sampling Freq: ', sampleFreq)\n",
    "\n",
    "                print('Past 3rd issue: ', i)\n",
    "\n",
    "                #--------------- Cross-Correlation ---------------------------------------------\n",
    "                corr = np.correlate(tangXZ_Real_Vel - np.mean(tangXZ_Real_Vel), \n",
    "                                  tangXZ_Virt_Vel - np.mean(tangXZ_Virt_Vel),\n",
    "                                  mode='full')\n",
    "                sampleDifference = np.argmax(tangXZ_Virt_Vel[20:]) - np.argmax(tangXZ_Real_Vel[20:])\n",
    "\n",
    "                print('Past correlation issue: ', i)\n",
    "\n",
    "                if sampleDifference > 50:\n",
    "                    lag = (sampleDifference  * (1/sampleFreq)) * 1000\n",
    "                    print('Lag is too large: ' , lag)\n",
    "                else: \n",
    "                    lag = (sampleDifference  * (1/sampleFreq)) * 1000\n",
    "                    # print('Lag: ', np.round(lag), 'ms')\n",
    "\n",
    "                    #--------------- Positional-Error ---------------------------------------------\n",
    "                    # MSE = np.sum(np.power(np.abs((tangXZ_Real.values[20:] - tangXZ_Virt.values[20:]),2))) / len(tangXZ_Real.values[20:])\n",
    "\n",
    "                    print('Past if statement: ', i)\n",
    "\n",
    "                    MSE = np.round(np.sum(np.power(np.abs(tangXZ_Real.values[20:] - tangXZ_Virt.values[20:]),2)) / len(tangXZ_Real.values[20:]),4) * 100 # Convert to cm\n",
    "                    endPosError = (np.round(np.nanmean(np.abs(tangXZ_Virt.values[-1-30:-1] - tangXZ_Real.values[-1-30:-1])),3)/30) * 100 # Convert to cm\n",
    "                    posError = np.round(np.nanmean(np.abs(tangXZ_Virt.values[20:] - tangXZ_Real.values[20:])),3) * 100 # Convert to cm\n",
    "                    velError = np.round(np.nanmean(np.abs(tangXZ_Virt_Vel[20:] - tangXZ_Real_Vel[20:])),3) # Convert to cm\n",
    "\n",
    "                    # print('Mean Square Error: ', MSE, 'cm')\n",
    "                    # print('Target Hit Error: ', endPosError, 'cm')\n",
    "                    # print('Average Positional Error: ', posError, 'cm')\n",
    "                    # print('Velocity Error: ', velError, 'ms-1')\n",
    "\n",
    "                    dat_List.append([lag, MSE, endPosError, posError, velError, times[-1]])\n",
    "\n",
    "\n",
    "\n",
    "            except Exception as e:\n",
    "                print('MY_ERROR: Size of array: ', len(tangXZ_Real))\n",
    "                zeroArray = np.tile(np.zeros, (1, 100))\n",
    "                tangXZ_Real_Vel = zeroArray\n",
    "                tangXZ_Virt_Vel = zeroArray\n",
    "                \n",
    "        except Exception as ex:\n",
    "            print('My_Error_2: ', ex)\n",
    "    \n",
    "\n",
    "    df_out = pd.DataFrame(dat_List, columns =['Lag' ,'MSE_Error', 'Hit_Error', 'AvPos_Error', 'Vel_Error', 'Time'])\n",
    "    # times = np.arange(0,)\n",
    "    df_out.insert(0, 'PtxID', ptx, True)\n",
    "\n",
    "    return df_out "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1635956245032,
     "user": {
      "displayName": "Diar Karim",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "07169669839868301821"
     },
     "user_tz": 0
    },
    "id": "pPKU9F64ea2d"
   },
   "outputs": [],
   "source": [
    "def ReadAllData(path, ptxID):\n",
    "    \n",
    "    df = None\n",
    "    \n",
    "    # Get all files in the folder\n",
    "    files = os.listdir(path) \n",
    "    print('Num of files: ', len(files))\n",
    "    \n",
    "    # Load each file into the data frame \n",
    "    for i in range(len(files)): \n",
    "        \n",
    "#         print('File name: ', files[i])\n",
    "#         try:\n",
    "        # Extract file name info and add to the dataframe \n",
    "        fileWords = files[i].split(\"_\") \n",
    "\n",
    "        tidx = fileWords.index(\"Trial\")\n",
    "        trialNum = fileWords[tidx + 1]\n",
    "\n",
    "        # Extract user ID \n",
    "        # idx = fileWords.index(\"163*\")\n",
    "        userID = fileWords[0]\n",
    "\n",
    "\n",
    "        #------------ Add to data frame ----------------\n",
    "        path_file = path + \"/\" + files[i]\n",
    "        tmpDF = pd.read_json(path_file)\n",
    "\n",
    "        # Add user ID to data frame \n",
    "        tmpDF.insert(0, \"UserID\", userID, True)\n",
    "\n",
    "        # Add Trial Number to data frame \n",
    "        tmpDF.insert(0, \"TrialNum\", trialNum, True)\n",
    "        \n",
    "        # Figure out target panel height mathematically before assigning it\n",
    "        height_val = np.nanmean(tmpDF['yTPos'].values)\n",
    "        \n",
    "        # Extract height \n",
    "        if height_val < 0.4:\n",
    "            height = \"low\"\n",
    "        elif height_val < 0.5:\n",
    "            height = \"mid\"\n",
    "        elif height_val < 0.7:\n",
    "            height = \"high\"\n",
    "        else:\n",
    "            height = \"noHeight\"\n",
    "            \n",
    "#         # Compute actual sampling rate\n",
    "#         timetaken = tmpDF['time'].values\n",
    "#         indexOfStart = np.where(timetaken == 0.0)\n",
    "        \n",
    "        # Compute actual sampling rate\n",
    "        timetaken = tmpDF['time'].values\n",
    "#         timetaken2 = ResizeArray(timetaken, newArrLength)\n",
    "        timetaken3 = np.round(timetaken,1)\n",
    "        timetaken3 = timetaken3.tolist()\n",
    "\n",
    "        # If 0.0 time isn't present, then use the smallest time value as the start of the trial\n",
    "        try:\n",
    "            indexOfStart = timetaken3.index(0.0) # indexOfStart = np.where(timetaken == 0.0)\n",
    "        except Exception as e:\n",
    "            print('IndexErr: ', e)\n",
    "            minTimeVal = np.nanmin(timetaken3)\n",
    "            indexOfStart = timetaken3.index(minTimeVal)\n",
    "        \n",
    "        # If 0.0 time isn't present, then use the smallest time value as the start of the trial\n",
    "#         samplingRate = np.round(1.0 / ((len(timetaken)-indexOfStart[0][0]) / timetaken[-1]), 4)\n",
    "        try:\n",
    "            samplingRate = np.round(1.0 / ((len(timetaken3)-indexOfStart) / timetaken3[-1]), 4)\n",
    "        except Exception as e:\n",
    "            print('SampleErr: ', e)\n",
    "            lastMaxTimeVal = np.nanmax(timetaken3[-1-10:-1])\n",
    "            samplingRate = np.round(1.0 / ((len(timetaken3)-indexOfStart) / lastMaxTimeVal), 4)\n",
    "        \n",
    "        \n",
    "        # Get individual velocities \n",
    "        pos_x = tmpDF['xPos'].values\n",
    "        pos_xf = savgol_filter(pos_x, 21, 9)\n",
    "        vel_x = np.gradient(pos_xf / samplingRate)\n",
    "        pos_y = tmpDF['yPos'].values\n",
    "        pos_yf = savgol_filter(pos_y, 21, 9)\n",
    "        vel_y = np.gradient(pos_yf / samplingRate)\n",
    "        pos_z = tmpDF['zPos'].values\n",
    "        pos_zf = savgol_filter(pos_z, 21, 9)\n",
    "        vel_z = np.gradient(pos_zf / samplingRate)\n",
    "        vel_type_1 = np.sqrt(np.power(vel_x,2) + np.power(vel_y,2) + np.power(vel_z,2))\n",
    "        vel_type_1f = savgol_filter(vel_type_1, 21, 9)\n",
    "\n",
    "        # Extract tempot \n",
    "        if \"80\" in fileWords or \"slow\" in fileWords:\n",
    "            tempo = \"80\"\n",
    "        elif \"120\" in fileWords or \"medium\" in fileWords:\n",
    "            tempo = \"120\"\n",
    "        elif \"160\" in fileWords or \"fast\" in fileWords:\n",
    "            tempo = \"160\"\n",
    "        else: \n",
    "            tempo = \"noTempo\"\n",
    "                \n",
    "        # Add height to data frame\n",
    "        tmpDF.insert(0, \"height\", height, True)\n",
    "\n",
    "        # Add tempo to data frame \n",
    "        tmpDF.insert(0, \"tempos\", tempo, True)        \n",
    "        \n",
    "        if df is None:\n",
    "            df = tmpDF\n",
    "        else:\n",
    "            df = pd.concat((df, tmpDF))\n",
    "\n",
    "#         except Exception as e:\n",
    "#             print('My err: ', e)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ReadAllDataBend(path):\n",
    "    # # Define data frame variable\n",
    "    df = None \n",
    "    df_endpoints = None\n",
    "    \n",
    "    folders = os.listdir(path) \n",
    "    \n",
    "    for p in range(len(folders)):\n",
    "\n",
    "        # Extract folder names info and add to the dataframe \n",
    "        folderWords = folders[p].split(\"_\")\n",
    "\n",
    "        if \"2021\" in folders[p]: # os.path.isdir(path + folders[p]) and \n",
    "\n",
    "            # Extract participant name \n",
    "            idx = folderWords.index(\"2021\")\n",
    "            ptxID = folderWords[idx - 3]\n",
    "            print(ptxID)\n",
    "#             print(folders[p])\n",
    "\n",
    "            # Get all files in the folder\n",
    "            files = os.listdir(path + folders[p]) \n",
    "\n",
    "#             print(p, ' 1: Inside first loop...')\n",
    "\n",
    "            # Load each file into the data frame \n",
    "            for i in range(len(files)): \n",
    "\n",
    "#                 print('2: Inside second loop...')\n",
    "\n",
    "                # if \".json\" in files[i] and height in files[i] and tempo in files[i]:  \n",
    "\n",
    "                try:\n",
    "                    # Extract file name info and add to the dataframe \n",
    "                    fileWords = files[i].split(\"_\") \n",
    "#                     print (fileWords)\n",
    "                    \n",
    "                    tidx = fileWords.index(\"Trial\")\n",
    "                    trialNum = fileWords[tidx + 1]\n",
    "                    \n",
    "                    # Extract height \n",
    "                    if \"low\" in fileWords:\n",
    "                        height = \"low\"\n",
    "                    elif \"mid\" in fileWords:\n",
    "                        height = \"mid\"\n",
    "                    elif \"high\" in fileWords:\n",
    "                        height = \"high\"\n",
    "                    else:\n",
    "                        height = \"noHeight\"\n",
    "\n",
    "                    # Extract tempot \n",
    "                    if \"80\" in fileWords or \"slow\" in fileWords:\n",
    "                        tempo = \"80\"\n",
    "                    elif \"120\" in fileWords or \"medium\" in fileWords:\n",
    "                        tempo = \"120\"\n",
    "                    elif \"160\" in fileWords or \"fast\" in fileWords:\n",
    "                        tempo = \"160\"\n",
    "                    else: \n",
    "                        tempo = \"noTempo\"\n",
    "\n",
    "                    # Extract user ID \n",
    "                    # idx = fileWords.index(\"163*\")\n",
    "                    userID = fileWords[0]\n",
    "\n",
    "\n",
    "                    #------------ Add to data frame ----------------\n",
    "\n",
    "                    tmpDF = pd.read_json(path + folders[p] + \"/\" + files[i])\n",
    "\n",
    "                    # Add height to data frame\n",
    "                    tmpDF.insert(0, \"height\", height, True)\n",
    "\n",
    "                    # Add tempo to data frame \n",
    "                    tmpDF.insert(0, \"tempo\", tempo, True)\n",
    "\n",
    "                    # Add user ID to data frame \n",
    "                    tmpDF.insert(0, \"UserID\", userID, True)\n",
    "\n",
    "                    # Add Trial Number to data frame \n",
    "                    tmpDF.insert(0, \"TrialNum\", trialNum, True)\n",
    "                    \n",
    "                    # Add participant ID to data frame \n",
    "                    tmpDF.insert(0, \"PtxID\", ptxID, True)\n",
    "\n",
    "                    if df is None:\n",
    "                        df = tmpDF\n",
    "                    else:\n",
    "                        df = pd.concat((df, tmpDF))\n",
    "#                         print('3: At the end of the loop...')\n",
    "                    \n",
    "    #-------------------------------------------------------------------------------------------------\n",
    "    \n",
    "#                     # Virtual finger tip terminal position\n",
    "#                 end_points_x = df_all[mask_2_virt & (df_all['gameObjectName'] == 'r_index_fingernail_marker')]['xPos'].values[-1-10:-1]\n",
    "#                 end_points_z = df_all[mask_2_virt & (df_all['gameObjectName'] == 'r_index_fingernail_marker')]['zPos'].values[-1-10:-1]\n",
    "#                 vRow_A_point_x.append(np.round(end_points_x,5))\n",
    "#                 vste_x.append(np.round(np.nanstd(end_points_x) / np.sqrt(len(end_points_x)), 5))\n",
    "#                 vRow_A_point_z.append(np.round(end_points_z,5))\n",
    "#                 vste_z.append(np.round(np.nanstd(end_points_z) / np.sqrt(len(end_points_z)), 5))\n",
    "                \n",
    "#                     Row_A_point_x = np.nanmean(tmpDF[(df_all['gameObjectName'] == 'realFingerTip')]['xPos'].values[-1-30:-1], axis=0)\n",
    "#                     Row_A_point_z = np.nanmean(tmpDF[(df_all['gameObjectName'] == 'realFingerTip')]['zPos'].values[-1-30:-1], axis=0)\n",
    "#                     vRow_A_point_x = np.nanmean(tmpDF[(df_all['gameObjectName'] == 'r_index_fingernail_marker')]['xPos'].values[-1-30:-1], axis=0)\n",
    "#                     vRow_A_point_z = np.nanmean(tmpDF[(df_all['gameObjectName'] == 'r_index_fingernail_marker')]['zPos'].values[-1-30:-1], axis=0)\n",
    "\n",
    "#                     ste_x.append(np.round(np.nanstd(end_points_x) / np.sqrt(len(end_points_x)), 5))\n",
    "\n",
    "                    \n",
    "#                     dataEndP = zip(Row_A_point_x, Row_A_point_z, ste_x, ste_z, vRow_A_point_x , vRow_A_point_z, vste_x, vste_z, targets)\n",
    "#                     tmpDF = pd.DataFrame(dataEndP, columns=['Real_X_Pos','Real_Z_Pos', 'Real_SE_X',  'Real_SE_Z', 'Virtual_X_Pos','Virtual_Z_Pos', 'Virtual_SE_X',  'Virtual_SE_Z', 'targetID'])    \n",
    "\n",
    "#                     tmpDF_endpoint = \n",
    "        \n",
    "#                     if df_endpoints is None:\n",
    "#                         df_endpoints = tmpDF_endpoint\n",
    "#                     else:\n",
    "#                         df_endpoints = pd.concat((df_endpoints, tmpDF_endpoint))\n",
    "    \n",
    "    \n",
    "                except Exception as e:\n",
    "                    print('My err: ', e)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CrossCorr2(vel_1, vel_2, sampleFreq):\n",
    "    import operator\n",
    "    \n",
    "    # Detrend data first: \n",
    "    response = mlb.detrend(vel_1)\n",
    "    signal = mlb.detrend(vel_2)\n",
    "    max_corr = np.zeros(6)\n",
    "    max_lag = np.zeros(6)\n",
    "    \n",
    "    # Compute lag at max lag\n",
    "    try:\n",
    "        lags,c,line,b = plt.xcorr(response, signal,usevlines = False,normed=True)\n",
    "        \n",
    "#         plt.figure()\n",
    "#         plt.subplot(1,1,1)\n",
    "#         corr_6 = plt.plot(lags,c,color='g')\n",
    "\n",
    "        #         c = np.sqrt(c*c)\n",
    "        index, value = max(enumerate(c), key=operator.itemgetter(1))\n",
    "        max_corr[5] = value\n",
    "        max_lag[5] = lags[index]\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        max_corr[5] = np.nan\n",
    "        max_lag[5] = np.nan\n",
    "        \n",
    "    return max_lag[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SamplingRate(timeValues):\n",
    "    # Compute actual sampling rate\n",
    "    timetaken = timeValues\n",
    "#     timetaken2 = ResizeArray(timetaken, newArrLength)\n",
    "    timetaken3 = np.round(timetaken,1)\n",
    "    timetaken3 = timetaken3.tolist()\n",
    "\n",
    "    # If 0.0 time isn't present, then use the smallest time value as the start of the trial\n",
    "    try:\n",
    "        indexOfStart = timetaken3.index(0.0) # indexOfStart = np.where(timetaken == 0.0)\n",
    "    except Exception as e:\n",
    "        print('IndexErr: ', e)\n",
    "        minTimeVal = np.nanmin(timetaken3)\n",
    "        indexOfStart = timetaken3.index(minTimeVal)\n",
    "\n",
    "    # Check if devision by zero, because the last value happens to be zero and use last largest value instead\n",
    "    try:\n",
    "        samplingRate = np.round(1.0 / ((len(timetaken3)-indexOfStart) / timetaken3[-1]), 4)\n",
    "    except Exception as e:\n",
    "        print('SampleErr: ', e)\n",
    "        lastMaxTimeVal = np.nanmax(timetaken3[-1-10:-1])\n",
    "        samplingRate = np.round(1.0 / ((len(timetaken3)-indexOfStart) / lastMaxTimeVal), 4)\n",
    "\n",
    "#     print('Sampling Rate: ', np.round(1.0/samplingRate), ' Hz')\n",
    "    \n",
    "    return np.round(1.0/samplingRate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pSpZUvzN72F-"
   },
   "source": [
    "# New Section"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g0KCredpZdBy"
   },
   "source": [
    "Load all the files into a single data frame. This should be updated to create 5 separate data frames: (1) Base line, (2) Adaptation, (3) Washout, (4) Catch and (5) Questions "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DLOQPFnPd_Y2"
   },
   "source": [
    "# Real all files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pathBend = 'E:/Projects/QuestAccuracyAnalysis/Quest Accuracy Data-20211103T165650Z-001/Quest Accuracy Data/AAA_Diar_Strange/benddata/'\n",
    "pathBend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "folders = os.listdir(pathBend)\n",
    "print(len(folders))\n",
    "print(folders)\n",
    "\n",
    "startTime = time.time()\n",
    "\n",
    "df_allBend = ReadAllDataBend(pathBend)\n",
    "# df_all = pd.read_json(path + \"latest_df_all.json\")\n",
    "\n",
    "endTime = time.time()\n",
    "computeDuration = endTime - startTime\n",
    "print('time: ', computeDuration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pd.unique(df_allBend['gameObjectName']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "realMask1 = (df_allBend['UserID'] == '1634123474') & (df_allBend['gameObjectName'] == 'realFingerTip')\n",
    "realMask2 = (df_allBend['UserID'] == '1634123474') & (df_allBend['gameObjectName'] == 'realFingerTip (1)')\n",
    "realMask3 = (df_allBend['UserID'] == '1634123474') & (df_allBend['gameObjectName'] == 'realFingerTip (2)')\n",
    "realMask4 = (df_allBend['UserID'] == '1634123474') & (df_allBend['gameObjectName'] == 'realFingerTip (3)')\n",
    "\n",
    "virtMask1 = (df_allBend['UserID'] == '1634123474') & (df_allBend['gameObjectName'] == 'r_index_fingernail_marker')\n",
    "virtMask2 = (df_allBend['UserID'] == '1634123474') & (df_allBend['gameObjectName'] == 'b_r_index1')\n",
    "virtMask3 = (df_allBend['UserID'] == '1634123474') & (df_allBend['gameObjectName'] == 'b_r_index2')\n",
    "virtMask4 = (df_allBend['UserID'] == '1634123474') & (df_allBend['gameObjectName'] == 'b_r_index3')\n",
    "\n",
    "timeVals = df_allBend[virtMask1]['time']  \n",
    "sR = SamplingRate(timeVals)\n",
    "interSampleInterval = 1/sR #0.01\n",
    "\n",
    "startIdx = 450\n",
    "endIdx = 500\n",
    "\n",
    "# Marker 1\n",
    "pos_x_m1 = df_allBend[realMask1]['xPos'][startIdx:endIdx]\n",
    "pos_y_m1 = df_allBend[realMask1]['yPos'][startIdx:endIdx]\n",
    "pos_xf = savgol_filter(pos_x_m1, 21, 9)\n",
    "pos_yf = savgol_filter(pos_y_m1, 21, 9)\n",
    "pos_xv_m1 = df_allBend[virtMask1]['xPos'][startIdx:endIdx]\n",
    "pos_yv_m1 = df_allBend[virtMask1]['yPos'][startIdx:endIdx]\n",
    "pos_xvf = savgol_filter(pos_xv_m1, 21, 9)\n",
    "pos_yvf = savgol_filter(pos_yv_m1, 21, 9)\n",
    "\n",
    "# Position\n",
    "plt.figure\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(pos_x_m1, pos_y_m1,'k-o')\n",
    "plt.plot(pos_xv_m1,pos_yv_m1,'m-o')\n",
    "plt.legend(['Real','Virtual'])\n",
    "plt.plot([pos_x_m1, pos_xv_m1], [pos_y_m1, pos_yv_m1],'b-',alpha=0.5)\n",
    "plt.ylim([0.5, 0.9])\n",
    "plt.axis('equal')\n",
    "\n",
    "# Velocity Real\n",
    "vel_x = np.gradient(pos_xf / interSampleInterval)\n",
    "vel_y = np.gradient(pos_yf / interSampleInterval)\n",
    "vel_type_1 = np.sqrt(np.power(vel_x,2) + np.power(vel_y,2))\n",
    "vel_real = savgol_filter(vel_type_1, 21, 9)\n",
    "\n",
    "# Velocity Virtual\n",
    "vel_xv = np.gradient(pos_xvf / interSampleInterval)\n",
    "vel_yv = np.gradient(pos_yvf / interSampleInterval)\n",
    "vel_type_1v = np.sqrt(np.power(vel_xv,2) + np.power(vel_yv,2))\n",
    "vel_virt = savgol_filter(vel_type_1v, 21, 9)\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(vel_real,'k')\n",
    "plt.plot(vel_virt,'m')\n",
    "plt.ylim([0, 1.5])\n",
    "\n",
    "lag = CrossCorr(vel_real, vel_virt, interSampleInterval)\n",
    "print('Lag: ', lag, ' ms')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6IiV2WhafZrA",
    "outputId": "af251985-ed33-4345-8e9e-7ee18dede981"
   },
   "outputs": [],
   "source": [
    "# path = \"/content/drive/MyDrive/Colab Notebooks/Quest Accuracy Data/QuestTrackingData/Pete_5th_Oct_2021/\"\n",
    "path = \"E:/Projects/QuestAccuracyAnalysis/Quest Accuracy Data-20211103T165650Z-001/Quest Accuracy Data/QuestTrackingData/\"\n",
    "print(path)\n",
    "\n",
    "# # Define data frame variable\n",
    "df_all = None\n",
    "df_tmp = None \n",
    "\n",
    "df_Max = None\n",
    "df_Pet = None\n",
    "df_Kat = None\n",
    "\n",
    "folders = os.listdir(path)\n",
    "\n",
    "for i, p in enumerate(folders):\n",
    "    \n",
    "    # Start timer \n",
    "    startTime = time.time()\n",
    "    \n",
    "    print('idx: ', i ,' Folder: ', p)\n",
    "    \n",
    "    # Extract folder names info and add to the dataframe \n",
    "    folderWords = folders[i].split(\"_\")\n",
    "    idx = folderWords.index(\"2021\")\n",
    "    ptxID = folderWords[idx - 3]\n",
    "    print('Ptx ID: ', ptxID, '**************************************')\n",
    "    \n",
    "    if \"Max\" in ptxID: \n",
    "        df_tmmp = ReadAllData(path_folder, ptxID)     # df_all = pd.read_json(path + \"latest_df_all.json\")\n",
    "        df_tmmp.insert(0, \"PtxID\", ptxID, True) # Add participant ID to data frame \n",
    "        df_tmmp = df_tmmp[['PtxID','TrialNum','UserID', 'tempos', 'height', 'frameNum', 'gameObjectName', 'xPos', 'yPos','zPos','xRot', 'yRot', 'zRot', 'targetID', 'xTPos', 'yTPos', 'zTPos', 'time']]\n",
    "        df_Max = df_tmmp\n",
    "#         continue\n",
    "    if \"Pete\" in ptxID: \n",
    "        df_tmmp = ReadAllData(path_folder, ptxID)     # df_all = pd.read_json(path + \"latest_df_all.json\")\n",
    "        df_tmmp.insert(0, \"PtxID\", ptxID, True) # Add participant ID to data frame \n",
    "        df_tmmp = df_tmmp[['PtxID','TrialNum','UserID', 'tempos', 'height', 'frameNum', 'gameObjectName', 'xPos', 'yPos','zPos','xRot', 'yRot', 'zRot', 'targetID', 'xTPos', 'yTPos', 'zTPos', 'time']]\n",
    "        df_Pet = df_tmmp\n",
    "#         continue\n",
    "    if \"Katrina\" in ptxID: \n",
    "        df_tmmp = ReadAllData(path_folder, ptxID)     # df_all = pd.read_json(path + \"latest_df_all.json\")\n",
    "        df_tmmp.insert(0, \"PtxID\", ptxID, True) # Add participant ID to data frame \n",
    "        df_tmmp = df_tmmp[['PtxID','TrialNum','UserID', 'tempos', 'height', 'frameNum', 'gameObjectName', 'xPos', 'yPos','zPos','xRot', 'yRot', 'zRot', 'targetID', 'xTPos', 'yTPos', 'zTPos', 'time']]\n",
    "        df_Kat = df_tmmp\n",
    "#         continue\n",
    "    else:     \n",
    "        try:\n",
    "            path_folder = path + p + \"/\"\n",
    "            df_tmp = ReadAllData(path_folder, ptxID)     # df_all = pd.read_json(path + \"latest_df_all.json\")\n",
    "            df_tmp.insert(0, \"PtxID\", ptxID, True) # Add participant ID to data frame \n",
    "\n",
    "            df_tmp2 = df_tmp[['PtxID','TrialNum','UserID', 'tempos', 'height', 'frameNum', 'gameObjectName', 'xPos', 'yPos','zPos','xRot', 'yRot', 'zRot', 'targetID', 'xTPos', 'yTPos', 'zTPos', 'time']]\n",
    "\n",
    "            if df_all is None:\n",
    "                df_all = df_tmp2\n",
    "            else:\n",
    "                df_all = pd.concat((df_all, df_tmp2))\n",
    "\n",
    "        except Exception as e: \n",
    "            print('Err: ', e)\n",
    "            continue\n",
    "\n",
    "    endTime = time.time()\n",
    "    computeDuration = endTime - startTime\n",
    "    print('Processing time: ', np.round(computeDuration/60.0), ' minutes')    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fix issues with Joe's and Max's data sets\n",
    "##### These are data frames with duplicate columns, for some unexplainable reason. As a result the duplicate columns have to be removed first before we can add them to the main data frame \n",
    "\n",
    "###### This is to correct the following error: InvalidIndexError: Reindexing only valid with uniquely valued Index objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_Kat =df_Kat.loc[:,~df_Kat.columns.duplicated()]\n",
    "df_Max =df_Max.loc[:,~df_Max.columns.duplicated()]\n",
    "df_Pet =df_Pet.loc[:,~df_Pet.columns.duplicated()]\n",
    "\n",
    "df_all = pd.concat((df_all, df_Kat))\n",
    "df_all = pd.concat((df_all, df_Max))\n",
    "df_all = pd.concat((df_all, df_Pet))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.unique(df_all['PtxID'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ptxNum = 6\n",
    "ptxIds = pd.unique(df_all['PtxID'])\n",
    "print('Ptx: ', ptxIds[ptxNum])\n",
    "\n",
    "temps = pd.unique(df_all['tempos'])\n",
    "print('Tempos: ', temps)\n",
    "\n",
    "\n",
    "ptx_tempo_Mask_80 = (df_all['PtxID'] == ptxIds[ptxNum]) & (df_all['tempos'] == temps[2])\n",
    "\n",
    "tempores1 = pd.unique(df_all[ptx_tempo_Mask_80]['tempos'])\n",
    "ptx_tempo_Mask_80 = (df_all['PtxID'] == ptxIds[ptxNum]) & (df_all['tempos'] == temps[1])\n",
    "tempores2 = pd.unique(df_all[ptx_tempo_Mask_80]['tempos'])\n",
    "ptx_tempo_Mask_80 = (df_all['PtxID'] == ptxIds[ptxNum]) & (df_all['tempos'] == temps[0])\n",
    "tempores3 = pd.unique(df_all[ptx_tempo_Mask_80]['tempos'])\n",
    "\n",
    "print(temps[2], ': ', tempores1, '\\n', temps[1], ': ', tempores2,'\\n', temps[0], ': ', tempores3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot data from individual participants "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = (df_all['PtxID'] == 'Susan') & (df_all['tempos'] == '160') & (df_all['gameObjectName'] == 'realFingerTip') & (df_all['height'] == 'low')\n",
    "targs = pd.unique(df_all[mask]['targetID'])\n",
    "np.sort(targs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ptxIds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # print('Trial: ', 23)    \n",
    "newArrLength = 100\n",
    "height = 'low'\n",
    "temp = '160'\n",
    "targID = 'row_C4'\n",
    "\n",
    "mask = (df_all['tempos'] == temp) & (df_all['height'] == height) & (df_all['targetID'] == targID) & (df_all['PtxID'] == ptxIds[0]) & (df_all['gameObjectName'] == 'realFingerTip') \n",
    "mask_virt = (df_all['tempos'] == temp) & (df_all['height'] == height) & (df_all['targetID'] == targID) & (df_all['PtxID'] == ptxIds[0]) & (df_all['gameObjectName'] == 'r_index_fingernail_marker') \n",
    "VisualizeTrajectories(mask, mask_virt, newArrLength)\n",
    "\n",
    "suspectTrials = pd.unique(df_all[mask]['TrialNum'])\n",
    "print(suspectTrials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "suspectTrials = pd.unique(df_all[mask]['TrialNum'])\n",
    "print(suspectTrials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Ptx:  Susan  Trial:  21  TargetID:  row_C3  tempo:  160  height:  low\n",
    "%matplotlib inline\n",
    "plt.rcParams[\"figure.figsize\"] = (4,5)\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "\n",
    "trialNumbs = '70' #'21'\n",
    "\n",
    "mask = (df_all['TrialNum'] == trialNumbs) & (df_all['tempos'] == temp) & (df_all['height'] == height) & (df_all['targetID'] == targID) & (df_all['PtxID'] == ptxIds[0]) & (df_all['gameObjectName'] == 'realFingerTip') \n",
    "mask_virt = (df_all['TrialNum'] == trialNumbs) & (df_all['tempos'] == temp) & (df_all['height'] == height) & (df_all['targetID'] == targID) & (df_all['PtxID'] == ptxIds[0]) & (df_all['gameObjectName'] == 'r_index_fingernail_marker') \n",
    "\n",
    "saveFig = True\n",
    "VisualizeTrajectories(mask, mask_virt, newArrLength, saveFig) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# realMask1 = (df_all['UserID'] == '1634123474') & (df_all['gameObjectName'] == 'realFingerTip')\n",
    "# realMask2 = (df_all['UserID'] == '1634123474') & (df_all['gameObjectName'] == 'realFingerTip (1)')\n",
    "# realMask3 = (df_all['UserID'] == '1634123474') & (df_all['gameObjectName'] == 'realFingerTip (2)')\n",
    "# realMask4 = (df_all['UserID'] == '1634123474') & (df_all['gameObjectName'] == 'realFingerTip (3)')\n",
    "\n",
    "# virtMask1 = (df_all['UserID'] == '1634123474') & (df_all['gameObjectName'] == 'r_index_fingernail_marker')\n",
    "# virtMask2 = (df_all['UserID'] == '1634123474') & (df_all['gameObjectName'] == 'b_r_index1')\n",
    "# virtMask3 = (df_all['UserID'] == '1634123474') & (df_all['gameObjectName'] == 'b_r_index2')\n",
    "# virtMask4 = (df_all['UserID'] == '1634123474') & (df_all['gameObjectName'] == 'b_r_index3')\n",
    "\n",
    "# timeVals = df_all[virtMask1]['time']  \n",
    "# sR = SamplingRate(timeVals)\n",
    "# interSampleInterval = 1/sR #0.01\n",
    "\n",
    "# startIdx = 450\n",
    "# endIdx = 500\n",
    "\n",
    "# # Marker 1\n",
    "# pos_x_m1 = df_all[realMask1]['xPos'][startIdx:endIdx]\n",
    "# pos_y_m1 = df_all[realMask1]['yPos'][startIdx:endIdx]\n",
    "# pos_xf = savgol_filter(pos_x_m1, 21, 9)\n",
    "# pos_yf = savgol_filter(pos_y_m1, 21, 9)\n",
    "# pos_xv_m1 = df_all[virtMask1]['xPos'][startIdx:endIdx]\n",
    "# pos_yv_m1 = df_all[virtMask1]['yPos'][startIdx:endIdx]\n",
    "# pos_xvf = savgol_filter(pos_xv_m1, 21, 9)\n",
    "# pos_yvf = savgol_filter(pos_yv_m1, 21, 9)\n",
    "\n",
    "# # Position\n",
    "# plt.figure\n",
    "# plt.subplot(1,2,1)\n",
    "# plt.plot(pos_x_m1, pos_y_m1,'k-o')\n",
    "# plt.plot(pos_xv_m1,pos_yv_m1,'m-o')\n",
    "# plt.legend(['Real','Virtual'])\n",
    "# plt.plot([pos_x_m1, pos_xv_m1], [pos_y_m1, pos_yv_m1],'b-',alpha=0.5)\n",
    "# plt.ylim([0.5, 0.9])\n",
    "# plt.axis('equal')\n",
    "\n",
    "# # Velocity Real\n",
    "# vel_x = np.gradient(pos_xf / interSampleInterval)\n",
    "# vel_y = np.gradient(pos_yf / interSampleInterval)\n",
    "# vel_type_1 = np.sqrt(np.power(vel_x,2) + np.power(vel_y,2))\n",
    "# vel_real = savgol_filter(vel_type_1, 21, 9)\n",
    "\n",
    "# # Velocity Virtual\n",
    "# vel_xv = np.gradient(pos_xvf / interSampleInterval)\n",
    "# vel_yv = np.gradient(pos_yvf / interSampleInterval)\n",
    "# vel_type_1v = np.sqrt(np.power(vel_xv,2) + np.power(vel_yv,2))\n",
    "# vel_virt = savgol_filter(vel_type_1v, 21, 9)\n",
    "\n",
    "# plt.subplot(1,2,2)\n",
    "# plt.plot(vel_real,'k')\n",
    "# plt.plot(vel_virt,'m')\n",
    "# plt.ylim([0, 1.5])\n",
    "\n",
    "# lag = CrossCorr(vel_real, vel_virt, interSampleInterval)\n",
    "# print('Lag: ', lag, ' ms')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "trialNumArr = np.arange(0,72)\n",
    "height = 'mid'\n",
    "temp = '160'\n",
    "\n",
    "for trialNum in trialNumArr:\n",
    "    \n",
    "#     print('Participant: ', ptxIds[0], 'Trial: ', trialNum)\n",
    "#     print('Participant: ', ptxIds[0], 'Trial: ', trialNum)\n",
    "    try:\n",
    "        print('Trial: ', trialNum)    \n",
    "        mask = (df_all['targetID'] == 'row_C6') & (df_all['tempos'] == temp) & (df_all['gameObjectName'] == 'realFingerTip') & (df_all['height'] == height)\n",
    "        mask_virt = (df_all['targetID'] == 'row_C6') & (df_all['tempos'] == temp) & (df_all['gameObjectName'] == 'r_index_fingernail_marker') & (df_all['height'] == height)\n",
    "        VisualizeTrajectories(mask, mask_virt)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        \n",
    "#     mask = (df_all['TrialNum'] == str(trialNum)) & (df_all['PtxID'] == ptxIds[0]) & (df_all['tempos'] == temp) & (df_all['gameObjectName'] == 'realFingerTip') & (df_all['height'] == height)\n",
    "#     mask_virt = (df_all['TrialNum'] == str(trialNum)) & (df_all['PtxID'] == ptxIds[0]) & (df_all['tempos'] == temp) & (df_all['gameObjectName'] == 'r_index_fingernail_marker') & (df_all['height'] == height)\n",
    "#     VisualizeTrajectories(mask, mask_virt)\n",
    "\n",
    "#     print('Participant: ', ptxIds[1], 'Trial: ', trialNum)\n",
    "#     mask = (df_all['TrialNum'] == str(trialNum)) & (df_all['PtxID'] == ptxIds[1]) & (df_all['tempos'] == temp) & (df_all['gameObjectName'] == 'realFingerTip') & (df_all['height'] == height)\n",
    "#     mask_virt = (df_all['TrialNum'] == str(trialNum)) & (df_all['PtxID'] == ptxIds[0]) & (df_all['tempos'] == temp) & (df_all['gameObjectName'] == 'r_index_fingernail_marker') & (df_all['height'] == height)\n",
    "#     VisualizeTrajectories(mask, mask_virt)\n",
    "\n",
    "#     print('Participant: ', ptxIds[2], 'Trial: ', trialNum)\n",
    "#     mask = (df_all['TrialNum'] == str(trialNum)) & (df_all['PtxID'] == ptxIds[2]) & (df_all['tempos'] == temp) & (df_all['gameObjectName'] == 'realFingerTip') & (df_all['height'] == height)\n",
    "#     mask_virt = (df_all['TrialNum'] == str(trialNum)) & (df_all['PtxID'] == ptxIds[0]) & (df_all['tempos'] == temp) & (df_all['gameObjectName'] == 'r_index_fingernail_marker') & (df_all['height'] == height)\n",
    "#     VisualizeTrajectories(mask, mask_virt)\n",
    "    \n",
    "#     print('Participant: ', ptxIds[3], 'Trial: ', trialNum)\n",
    "#     mask = (df_all['TrialNum'] == str(trialNum)) & (df_all['PtxID'] == ptxIds[3]) & (df_all['tempos'] == temp) & (df_all['gameObjectName'] == 'realFingerTip') & (df_all['height'] == height)\n",
    "#     mask_virt = (df_all['TrialNum'] == str(trialNum)) & (df_all['PtxID'] == ptxIds[0]) & (df_all['tempos'] == temp) & (df_all['gameObjectName'] == 'r_index_fingernail_marker') & (df_all['height'] == height)\n",
    "#     VisualizeTrajectories(mask, mask_virt)\n",
    "\n",
    "#     print('Participant: ', ptxIds[4], 'Trial: ', trialNum)\n",
    "#     mask = (df_all['TrialNum'] == str(trialNum)) & (df_all['PtxID'] == ptxIds[4]) & (df_all['tempos'] == temp) & (df_all['gameObjectName'] == 'realFingerTip') & (df_all['height'] == height)\n",
    "#     mask_virt = (df_all['TrialNum'] == str(trialNum)) & (df_all['PtxID'] == ptxIds[0]) & (df_all['tempos'] == temp) & (df_all['gameObjectName'] == 'r_index_fingernail_marker') & (df_all['height'] == height)\n",
    "#     VisualizeTrajectories(mask, mask_virt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_metrics_80_low = ExtractMetrics(df_all, '80', 'low')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_metrics_120_low = ExtractMetrics(df_all, '120', 'low')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_metrics_160_low = ExtractMetrics(df_all, '160', 'low')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_metrics_80_mid = ExtractMetrics(df_all, '80', 'mid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_metrics_120_mid = ExtractMetrics(df_all, '120', 'mid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_metrics_160_mid = ExtractMetrics(df_all, '160', 'mid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_metrics_80_high = ExtractMetrics(df_all, '80', 'high')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_metrics_120_high = ExtractMetrics(df_all, '120', 'high')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_metrics_160_high = ExtractMetrics(df_all, '160', 'high')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pd.unique(df_metrics_80_low['TargetID'])\n",
    "#pd.unique(df_metrics_120_low['TargetID']) # Got extra weird targets\n",
    "#pd.unique(df_metrics_160_low['TargetID'])\n",
    "\n",
    "#pd.unique(df_metrics_80_mid['TargetID'])\n",
    "#pd.unique(df_metrics_120_mid['TargetID']) # Got extra weird targets\n",
    "#pd.unique(df_metrics_160_mid['TargetID'])\n",
    "\n",
    "#pd.unique(df_metrics_80_high['TargetID'])\n",
    "#pd.unique(df_metrics_120_high['TargetID'])\n",
    "#pd.unique(df_metrics_160_high['TargetID'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_metrics_80_low.insert(2, 'tempos', '80')\n",
    "df_metrics_80_low.insert(2, 'height', 'low')\n",
    "\n",
    "df_metrics_120_low.insert(2, 'tempos', '120')\n",
    "df_metrics_120_low.insert(2, 'height', 'low')\n",
    "\n",
    "df_metrics_160_low.insert(2, 'tempos', '160')\n",
    "df_metrics_160_low.insert(2, 'height', 'low')\n",
    "\n",
    "df_metrics_80_mid.insert(2, 'tempos', '80')\n",
    "df_metrics_80_mid.insert(2, 'height', 'mid')\n",
    "\n",
    "df_metrics_120_mid.insert(2, 'tempos', '120')\n",
    "df_metrics_120_mid.insert(2, 'height', 'mid')\n",
    "\n",
    "df_metrics_160_mid.insert(2, 'tempos', '160')\n",
    "df_metrics_160_mid.insert(2, 'height', 'mid')\n",
    "\n",
    "df_metrics_80_high.insert(2, 'tempos', '80')\n",
    "df_metrics_80_high.insert(2, 'height', 'high')\n",
    "\n",
    "df_metrics_120_high.insert(2, 'tempos', '120')\n",
    "df_metrics_120_high.insert(2, 'height', 'high')\n",
    "\n",
    "df_metrics_160_high.insert(2, 'tempos', '160')\n",
    "df_metrics_160_high.insert(2, 'height', 'high')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_metrics_all = df_metrics_80_low\n",
    "df_metrics_all = pd.concat((df_metrics_all, df_metrics_120_low))\n",
    "df_metrics_all = pd.concat((df_metrics_all, df_metrics_160_low))\n",
    "\n",
    "df_metrics_all = pd.concat((df_metrics_all, df_metrics_80_mid))\n",
    "df_metrics_all = pd.concat((df_metrics_all, df_metrics_120_mid))\n",
    "df_metrics_all = pd.concat((df_metrics_all, df_metrics_160_mid))\n",
    "\n",
    "df_metrics_all = pd.concat((df_metrics_all, df_metrics_80_high))\n",
    "df_metrics_all = pd.concat((df_metrics_all, df_metrics_120_high))\n",
    "df_metrics_all = pd.concat((df_metrics_all, df_metrics_160_high))\n",
    "\n",
    "df_metrics_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save data frames to file\n",
    "\n",
    "df_metrics_all.to_csv('df_metrics_all.csv', index=False)\n",
    "compression_opts = dict(method='zip', archive_name='df_all.csv')  \n",
    "df_all.to_csv('df_all.zip', index=False, compression=compression_opts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start from here by loading previously saved dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_metrics_all = pd.read_csv (r'df_metrics_all.csv')\n",
    "df_metrics_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all = pd.read_csv('df_all.zip', compression='zip')\n",
    "df_all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sort out weird targets in the df_metrics_all data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weirdTargetIDs = ['0    row_C4\\n0    row_B2\\nName: targetID, dtype: object',\n",
    "                  '0    row_B3\\n0    row_A3\\nName: targetID, dtype: object',\n",
    "                  '0    row_C6\\n0    row_A2\\nName: targetID, dtype: object',\n",
    "                  '0    row_C2\\n0    row_C2\\nName: targetID, dtype: object',\n",
    "                  '0    row_B5\\n0    row_C1\\nName: targetID, dtype: object',\n",
    "                  '0    row_B2\\n0    row_B5\\nName: targetID, dtype: object',\n",
    "                  '0    row_C3\\n0    row_C6\\nName: targetID, dtype: object',\n",
    "                  '0    row_B2\\n0    row_C3\\nName: targetID, dtype: object',\n",
    "                  '0    row_A1\\n0    row_C2\\nName: targetID, dtype: object',\n",
    "                  '0    row_C4\\n0    row_C4\\nName: targetID, dtype: object',\n",
    "                  '0    row_C1\\n0    row_A2\\nName: targetID, dtype: object',\n",
    "                  '0    row_B3\\n0    row_A4\\nName: targetID, dtype: object',\n",
    "                  '0    row_A3\\n0    row_A6\\nName: targetID, dtype: object',\n",
    "                  '0    row_C5\\n0    row_B4\\nName: targetID, dtype: object',\n",
    "                  '0    row_C3\\n0    row_A1\\nName: targetID, dtype: object',\n",
    "                  '0    row_A6\\n0    row_B5\\nName: targetID, dtype: object',\n",
    "                  '0    row_B5\\n0    row_B1\\nName: targetID, dtype: object',\n",
    "                  '0    row_A2\\n0    row_B6\\nName: targetID, dtype: object',\n",
    "                  '0    row_B4\\n0    row_C1\\nName: targetID, dtype: object',\n",
    "                  '0    row_B6\\n0    row_A5\\nName: targetID, dtype: object',\n",
    "                  '0    row_B1\\n0    row_C5\\nName: targetID, dtype: object',\n",
    "                  '0    row_A5\\n0    row_A3\\nName: targetID, dtype: object',\n",
    "                  '0    row_C6\\n0    row_C6\\nName: targetID, dtype: object',\n",
    "                  '0    row_C2\\n0    row_B3\\nName: targetID, dtype: object',\n",
    "                  '0    row_B2\\n0    row_A4\\nName: targetID, dtype: object',\n",
    "                  '0    row_C3\\n0    row_B2\\nName: targetID, dtype: object',\n",
    "                  '0    row_C2\\n0    row_A1\\nName: targetID, dtype: object',\n",
    "                  '0    row_A2\\n0    row_C1\\nName: targetID, dtype: object',\n",
    "                  '0    row_A4\\n0    row_B3\\nName: targetID, dtype: object',\n",
    "                  '0    row_A6\\n0    row_A3\\nName: targetID, dtype: object',\n",
    "                  '0    row_B4\\n0    row_C5\\nName: targetID, dtype: object',\n",
    "                  '0    row_A1\\n0    row_C3\\nName: targetID, dtype: object',\n",
    "                  '0    row_B5\\n0    row_A6\\nName: targetID, dtype: object',\n",
    "                  '0    row_B1\\n0    row_B5\\nName: targetID, dtype: object',\n",
    "                  '0    row_B6\\n0    row_A2\\nName: targetID, dtype: object',\n",
    "                  '0    row_C1\\n0    row_B4\\nName: targetID, dtype: object',\n",
    "                  '0    row_A5\\n0    row_B6\\nName: targetID, dtype: object',\n",
    "                  '0    row_C5\\n0    row_B1\\nName: targetID, dtype: object',\n",
    "                  '0    row_A3\\n0    row_A5\\nName: targetID, dtype: object',\n",
    "                  '0    row_B3\\n0    row_C2\\nName: targetID, dtype: object']\n",
    "\n",
    "\n",
    "print('Length of weird TargetIDs: ', len(weirdTargetIDs))\n",
    "for i in weirdTargetIDs:\n",
    "    mask = (df_metrics_all['TargetID'] == i)\n",
    "    df_metrics_all.drop(df_metrics_all[mask].index, inplace=True)\n",
    "\n",
    "df_metrics_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pd.unique(df_metrics_all['TargetID'])\n",
    "pd.unique(df_metrics_all['TargetID'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean up data\n",
    "- [ ] Outlier detection and removal \n",
    "- [ ] using 3x std\n",
    "- [ ] using interquartile\n",
    "- [ ] map to normal distribution "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric1 = 'Lag'\n",
    "lag_vals = df_metrics_all[df_metrics_all[metric1] < 200.0]\n",
    "\n",
    "print('Mean Lag: ', lag_vals[metric1].mean())\n",
    "print('SD Lag: ', lag_vals[metric1].std())\n",
    "print('Max Lag: ', lag_vals[metric1].max())\n",
    "print('Min Lag: ', lag_vals[metric1].min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# metric1 = 'Lag'\n",
    "# # Remove outliers and map to normal distribution \n",
    "# data_norm = CleanUpnNorm(df_metrics_all, metric1, 1.0)\n",
    "# # Re-add the corrected metrics to the dataframe\n",
    "# df_metrics_all[metric1 + '_cleaned'] = data_norm\n",
    "# plt.ylabel(metric1)\n",
    "\n",
    "metric2 = 'EndError'\n",
    "# Remove outliers and map to normal distribution \n",
    "data_norm = CleanUpnNorm(df_metrics_all, metric2, 0.1)\n",
    "# Re-add the corrected metrics to the dataframe\n",
    "df_metrics_all[metric2 + '_cleaned'] = data_norm\n",
    "plt.ylabel(metric2)\n",
    "\n",
    "metric3 = 'PathOffset'\n",
    "# Remove outliers and map to normal distribution \n",
    "data_norm = CleanUpnNorm(df_metrics_all, metric3, 0.1)\n",
    "# Re-add the corrected metrics to the dataframe\n",
    "df_metrics_all[metric3 + '_cleaned'] = data_norm\n",
    "plt.ylabel(metric3)\n",
    "\n",
    "metric4 = 'PathOffsetNoLag'\n",
    "# Remove outliers and map to normal distribution \n",
    "data_norm = CleanUpnNorm(df_metrics_all, metric4, 0.75)\n",
    "# Re-add the corrected metrics to the dataframe\n",
    "df_metrics_all[metric4 + '_cleaned'] = data_norm\n",
    "plt.ylabel(metric4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sphericity Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spher, _, chisq, dof, pval = pg.sphericity(df_metrics_all, dv='PathOffset_cleaned',\n",
    "                                           subject='PtxID',\n",
    "                                           within=['tempos'])\n",
    "print('Spherical: ', spher, '\\nChiSq: ', round(chisq, 3), '\\nDof:', dof, '\\np-val: ', round(pval, 3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Tempo and height \n",
    "sns.set()\n",
    "\n",
    "# from statsmodels.graphics.factorplots import interaction_plot\n",
    "# fig = interaction_plot(df_metrics_all[\"tempos\"], df_metrics_all[\"height\"], df_metrics_all[\"EndError_cleaned\"], \n",
    "#                        colors=['red','green', \"blue\"], markers=['D','^', 'o'], ylabel='Positional Error (cm)', xlabel='Tempo')\n",
    "\n",
    "# sns.pointplot(data=data, x='height', y='EndError_cleaned',hue='TargetLateral', dodge=True,\n",
    "#               capsize=.1, errwidth=1, palette='colorblind')\n",
    "\n",
    "plt.figure()\n",
    "fig1 = sns.pointplot(x=\"tempos\", y=\"EndError_cleaned\", hue=\"height\",dodge=True, data=df_metrics_all,\n",
    "                     capsize=.1, errwidth=1, palette='colorblind')\n",
    "plt.figure()\n",
    "fig2 = sns.pointplot(x=\"tempos\", y=\"PathOffsetNoLag_cleaned\", hue=\"height\",dodge=True, data=df_metrics_all,\n",
    "                     capsize=.1, errwidth=1, palette='colorblind')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "fig1 = sns.pointplot(x=\"TargetID\", y=\"tempos\", hue=\"height\",dodge=True, data=df_metrics_all,\n",
    "                     capsize=.1, errwidth=1, palette='colorblind')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Target ID\n",
    "plt.figure()\n",
    "fig1 = sns.pointplot(x=\"TargetID\", y=\"EndError_cleaned\", hue=\"height\", data=df_metrics_all, palette='Reds')\n",
    "\n",
    "plt.figure()\n",
    "fig2 = sns.pointplot(x=\"TargetID\", y=\"PathOffsetNoLag_cleaned\", hue=\"height\", data=df_metrics_all, palette='Greens')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = (4,4)\n",
    "plt.tight_layout(pad=5.0)\n",
    "titleFontSize = 16\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "plt.figure()\n",
    "ax1 = sns.barplot(\n",
    "    data=df_metrics_all,\n",
    "    x='tempos', \n",
    "    y='PathOffsetNoLag_cleaned',\n",
    "    hue='height',\n",
    "    palette = 'Blues')\n",
    "\n",
    "# plt.title('Angular Error \\n Between Quest and MoCap \\n \\n \\n \\n \\n \\n \\n \\n')\n",
    "plt.ylabel('Offset (cm)')\n",
    "plt.xlabel('Tempos')\n",
    "plt.xticks([0,1,2],['80','120','160'])\n",
    "\n",
    "statannot.add_stat_annotation(\n",
    "    ax1,\n",
    "    data=df_metrics_all,\n",
    "    x='tempos',\n",
    "    y='PathOffsetNoLag_cleaned',\n",
    "    box_pairs=[(80, 120), (80,160), (120, 160)],\n",
    "    test=\"t-test_ind\",\n",
    "    text_format=\"star\",\n",
    "    loc=\"outside\",\n",
    ")\n",
    "\n",
    "# plt.rcParams[\"figure.figsize\"] = (3,3)\n",
    "plt.title('Pathoffset Across Tempos \\n\\n\\n\\n',fontsize=titleFontSize)\n",
    "plt.savefig(str(np.round(time.time())) + '_Pathoffset Across Tempos.png', dpi=600, bbox_inches='tight')\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "plt.figure()\n",
    "ax1 = sns.barplot(\n",
    "    data=df_metrics_all,\n",
    "    x='height', \n",
    "    y='PathOffsetNoLag_cleaned',\n",
    "    hue='tempos',\n",
    "    palette = 'Blues')\n",
    "\n",
    "# plt.title('Angular Error \\n Between Quest and MoCap \\n \\n \\n \\n \\n \\n \\n \\n')\n",
    "plt.ylabel('Offset (cm)')\n",
    "plt.xlabel('Heights')\n",
    "plt.xticks([0,1,2],['Low','Mid','High'])\n",
    "\n",
    "statannot.add_stat_annotation(\n",
    "    ax1,\n",
    "    data=df_metrics_all,\n",
    "    x='height',\n",
    "    y='PathOffsetNoLag_cleaned',\n",
    "    box_pairs=[('low', 'mid'), ('low','high'), ('mid', 'high')],\n",
    "    test=\"t-test_ind\",\n",
    "    text_format=\"star\",\n",
    "    loc=\"outside\",\n",
    ")\n",
    "\n",
    "# plt.rcParams[\"figure.figsize\"] = (3,3)\n",
    "plt.title('Pathoffset Across Heights \\n\\n\\n\\n',fontsize=titleFontSize)\n",
    "plt.savefig(str(np.round(time.time())) + '_Pathoffset Across Heights.png', dpi=600, bbox_inches='tight')\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "plt.figure()\n",
    "ax2 = sns.barplot(\n",
    "    data=df_metrics_all,\n",
    "    x='TargetLateral', \n",
    "    y='PathOffsetNoLag_cleaned',\n",
    "    hue='TargetVertical',\n",
    "    palette = 'Blues')\n",
    "\n",
    "# plt.title('Angular Error \\n Between Quest and MoCap \\n \\n \\n \\n \\n \\n \\n \\n')\n",
    "plt.ylabel('Offset (cm)')\n",
    "plt.xlabel('Target Location')\n",
    "plt.xticks([0,1,2],['left','center','right'])\n",
    "\n",
    "statannot.add_stat_annotation(\n",
    "    ax2,\n",
    "    data=df_metrics_all,\n",
    "    x='TargetLateral',\n",
    "    y='PathOffsetNoLag_cleaned',\n",
    "    box_pairs=[('left', 'center'), ('left','right'), ('center', 'right')],\n",
    "    test=\"t-test_ind\",\n",
    "    text_format=\"star\",\n",
    "    loc=\"outside\",\n",
    ")\n",
    "\n",
    "# plt.rcParams[\"figure.figsize\"] = (3,3)\n",
    "plt.title('Pathoffset Across Target Locations\\n\\n\\n\\n',fontsize=titleFontSize)\n",
    "plt.savefig(str(np.round(time.time())) + '_Pathoffset Across Target Locations.png', dpi=600, bbox_inches='tight')\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "plt.figure()\n",
    "ax3 = sns.barplot(\n",
    "    data=df_metrics_all,\n",
    "    x='TargetLateral', \n",
    "    y='EndError_cleaned',\n",
    "    hue='TargetVertical',\n",
    "    palette = 'Blues')\n",
    "\n",
    "# plt.title('Angular Error \\n Between Quest and MoCap \\n \\n \\n \\n \\n \\n \\n \\n')\n",
    "plt.ylabel('Error (cm)')\n",
    "plt.xlabel('Target Location')\n",
    "plt.xticks([0,1,2],['left','center','right'])\n",
    "\n",
    "statannot.add_stat_annotation(\n",
    "    ax3,\n",
    "    data=df_metrics_all,\n",
    "    x='TargetLateral',\n",
    "    y='EndError_cleaned',\n",
    "    box_pairs=[('left', 'center'), ('left','right'), ('center', 'right')],\n",
    "    test=\"t-test_ind\",\n",
    "    text_format=\"star\",\n",
    "    loc=\"outside\",\n",
    ")\n",
    "\n",
    "# plt.rcParams[\"figure.figsize\"] = (3,3)\n",
    "plt.title('Positional Error Across Target Location \\n\\n\\n\\n',fontsize=titleFontSize)\n",
    "plt.savefig(str(np.round(time.time())) + '_Positional Error Across Target Location.png', dpi=600, bbox_inches='tight')\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "plt.figure()\n",
    "ax4 = sns.barplot(\n",
    "    data=df_metrics_all,\n",
    "    x='height', \n",
    "    y='EndError_cleaned',\n",
    "    hue='tempos',\n",
    "    palette = 'Blues')\n",
    "\n",
    "# plt.title('Angular Error \\n Between Quest and MoCap \\n \\n \\n \\n \\n \\n \\n \\n')\n",
    "plt.ylabel('Error (cm)')\n",
    "plt.xlabel('Heights')\n",
    "plt.xticks([0,1,2],['low','mid','high'])\n",
    "\n",
    "# statannot.add_stat_annotation(\n",
    "#     ax4,\n",
    "#     data=df_metrics_all,\n",
    "#     x='height',\n",
    "#     y='EndError_cleaned',\n",
    "#     box_pairs=[('low', 'mid'), ('low','high'), ('mid', 'high')],\n",
    "#     test=\"t-test_ind\",\n",
    "#     text_format=\"star\",\n",
    "#     loc=\"outside\",\n",
    "# )\n",
    "\n",
    "plt.title('Positional Error Across Heights \\n\\n\\n\\n',fontsize=titleFontSize)\n",
    "plt.savefig(str(np.round(time.time())) + '_Positional Error Across Heights.png', dpi=600, bbox_inches='tight')\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "plt.figure()\n",
    "ax4 = sns.barplot(\n",
    "    data=df_metrics_all,\n",
    "    x='tempos', \n",
    "    y='EndError_cleaned',\n",
    "    hue='height',\n",
    "    palette = 'Blues')\n",
    "\n",
    "# plt.title('Angular Error \\n Between Quest and MoCap \\n \\n \\n \\n \\n \\n \\n \\n')\n",
    "plt.ylabel('Error (cm)')\n",
    "plt.xlabel('Tempos')\n",
    "plt.xticks([0,1,2],['80','120','160'])\n",
    "\n",
    "# statannot.add_stat_annotation(\n",
    "#     ax4,\n",
    "#     data=df_metrics_all,\n",
    "#     x='tempos',\n",
    "#     y='EndError_cleaned',\n",
    "#     box_pairs=[('80', '120'), ('80','160'), ('120', '160')],\n",
    "#     test=\"t-test_ind\",\n",
    "#     text_format=\"star\",\n",
    "#     loc=\"outside\",\n",
    "# )\n",
    "\n",
    "plt.title('Positional Error Across Tempos \\n\\n\\n\\n',fontsize=titleFontSize)\n",
    "plt.savefig(str(np.round(time.time())) + '_Positional Error Across Tempos.png', dpi=600, bbox_inches='tight')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "plt.rcParams[\"figure.figsize\"] = (4,4)\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "plt.tight_layout(pad=5.0)\n",
    "titleFontSize = 16\n",
    "\n",
    "plt.figure()\n",
    "sns.boxplot(x='TargetLateral',y='EndError_cleaned', order=[\"left\", \"center\", \"right\"],hue='TargetVertical', data=df_metrics_all, palette='Reds')\n",
    "plt.title('Positional Error Across Target Locations', fontsize=titleFontSize)\n",
    "plt.xlabel('Lateral Target Location')\n",
    "plt.ylabel('Error (cm)')\n",
    "plt.grid(False)\n",
    "plt.legend(bbox_to_anchor=(1.05, 1.0), loc='upper left')\n",
    "plt.savefig(str(np.round(time.time())) + '_EndError_TargetLoc.png', dpi=600, bbox_inches='tight')\n",
    "\n",
    "plt.figure()\n",
    "sns.boxplot(x='height',y='EndError_cleaned',hue='tempos', data=df_metrics_all, palette='Greens')\n",
    "plt.title('Positional Error Across Heights and Tempos', fontsize=titleFontSize)\n",
    "plt.xlabel('Heights')\n",
    "plt.ylabel('Error (cm)')\n",
    "plt.grid(False)\n",
    "plt.legend(bbox_to_anchor=(1.05, 1.0), loc='upper left')\n",
    "plt.savefig(str(np.round(time.time())) + '_EndError_Tempos.png', dpi=600, bbox_inches='tight')\n",
    "\n",
    "plt.figure()\n",
    "sns.boxplot(x='TargetLateral',y='PathOffsetNoLag_cleaned', order=[\"left\", \"center\", \"right\"],hue='TargetVertical', data=df_metrics_all, palette='Reds')\n",
    "plt.title('Path Offset Across Target Locations', fontsize=titleFontSize)\n",
    "plt.xlabel('Lateral Target Location')\n",
    "plt.ylabel('Offset (cm)')\n",
    "plt.grid(False)\n",
    "plt.legend(bbox_to_anchor=(1.05, 1.0), loc='upper left')\n",
    "plt.savefig(str(np.round(time.time())) + '_PathOffset_TargetLoc.png', dpi=600, bbox_inches='tight')\n",
    "\n",
    "plt.figure()\n",
    "sns.boxplot(x='height',y='PathOffsetNoLag_cleaned',hue='tempos', data=df_metrics_all, palette='Greens')\n",
    "plt.title('Path Offset Across Heights and Tempos', fontsize=titleFontSize)\n",
    "plt.xlabel('Heights')\n",
    "plt.ylabel('Offset (cm)')\n",
    "plt.grid(False)\n",
    "plt.legend(bbox_to_anchor=(1.05, 1.0), loc='upper left')\n",
    "plt.savefig(str(np.round(time.time())) + '_PathOffset_Heights.png', dpi=600, bbox_inches='tight')\n",
    "\n",
    "\n",
    "plt.figure()\n",
    "sns.boxplot(x='tempos',y='Lag',hue='height', data=df_metrics_all, palette='Blues')\n",
    "plt.title('Delay Across Tempos', fontsize=titleFontSize)\n",
    "plt.xlabel('Tempos')\n",
    "plt.ylabel('Delay (ms)')\n",
    "plt.ylim([-10,200])\n",
    "plt.grid(False)\n",
    "plt.legend(bbox_to_anchor=(1.05, 1.0), loc='upper left')\n",
    "plt.savefig(str(np.round(time.time())) + '_Delay_Tempos.png', dpi=600, bbox_inches='tight')\n",
    "\n",
    "\n",
    "# plt.figure()\n",
    "# sns.boxplot(x='TargetLateral',y='Lag',hue='height', data=df_metrics_all, palette='Blues')\n",
    "# plt.title('Lag', fontsize=titleFontSize)\n",
    "# plt.xlabel('Target Group')\n",
    "# plt.ylabel('Lag (ms)')\n",
    "# # plt.grid(False)\n",
    "# plt.savefg(str(np.round(time.time())) + '_Lag.png', dpi=600, bbox_inches='tight')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Two-Way Repeated Measures ANOVA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "detailing = False\n",
    "\n",
    "# print('\\n>> **Lateral has sig. effect \\n>> Frontal no effect and \\n>> **and there is a sig. interaction between lateral and frontal.\\n')\n",
    "\n",
    "res5 = pg.rm_anova(dv='EndError_cleaned', within=['TargetLateral', 'TargetVertical'], \n",
    "                     subject='PtxID', data=df_metrics_all, detailed=detailing)\n",
    "print('\\n EndError_cleaned LT\\n', res5.round(3))\n",
    "\n",
    "#-------------------------------------------------------------------------------------------\n",
    "print('\\nEndError on Height and Tempo')\n",
    "# print('\\n>> **Heigh has sig. effect \\n>> Tempo no effect and \\n>> **and there is a sig. interaction between lateral and frontal.\\n')\n",
    "\n",
    "res5 = pg.rm_anova(dv='EndError_cleaned', within=['height', 'tempos'], \n",
    "                     subject='PtxID', data=df_metrics_all, detailed=detailing)\n",
    "print('\\n EndError_cleaned LT\\n', res5.round(3))\n",
    "\n",
    "#-------------------------------------------------------------------------------------------\n",
    "print('\\nPathoffset on Target Location')\n",
    "# print('\\n>> Lateral has no effect \\n>> Frontal no effect and \\n>> **but there is a sig. interaction between lateral and frontal.\\n')\n",
    "\n",
    "res5 = pg.rm_anova(dv='PathOffsetNoLag_cleaned', within=['TargetLateral', 'TargetVertical'], \n",
    "                     subject='PtxID', data=df_metrics_all, detailed=detailing)\n",
    "print('\\n PathOffsetNoLag_cleaned LT\\n', res5.round(3))\n",
    "\n",
    "#-------------------------------------------------------------------------------------------\n",
    "print('\\nPathoffset on Height and Tempo')\n",
    "# print('\\n>> Height has no effect \\n>> **Tempos have a sig. effect and \\n>> **interaction between height and tempo.\\n')\n",
    "\n",
    "res5 = pg.rm_anova(dv='PathOffsetNoLag_cleaned', within=['height', 'tempos'], \n",
    "                     subject='PtxID', data=df_metrics_all, detailed=detailing)\n",
    "print('\\n PathOffsetNoLag_cleaned LT\\n', res5.round(3))\n",
    "\n",
    "#-------------------------------------------------------------------------------------------\n",
    "# print('\\nDelay on Height and Tempo')\n",
    "# # print('\\n>> Lag no effect \\n>> Tempo no effect and \\n>> No interaction between lateral and frontal.\\n')\n",
    "\n",
    "res6 = pg.rm_anova(dv='Lag', within=['tempos', 'height'], \n",
    "                  subject='PtxID', data=df_metrics_all,  detailed=detailing)\n",
    "print('\\n Lag HT\\n', res6.round(3))\n",
    "\n",
    "res7 = pg.rm_anova(dv='Lag', within=['TargetLateral', 'height'], \n",
    "                  subject='PtxID', data=df_metrics_all,  detailed=detailing)\n",
    "print('\\n Lag HL\\n', res6.round(3))\n",
    "\n",
    "# print('\\nDelay on Lateral and Vertical Target Loc')\n",
    "# # print('\\n>> Lag no effect \\n>> Tempo no effect and \\n>> No interaction between lateral and frontal.\\n')\n",
    "\n",
    "# res6 = pg.rm_anova(dv='Lag', within=['TargetLateral', 'TargetVertical'], \n",
    "#                   subject='PtxID', data=df_metrics_all,  detailed=detailing)\n",
    "# print('\\n Lag HT\\n', res6.round(3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import researchpy as rp\n",
    "import statsmodels.api as sm\n",
    "import scipy.stats as stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "rp.summary_cont(df_metrics_all.groupby([\"height\", \"tempos\", \"TargetLateral\", \"TargetVertical\"])[\"EndError_cleaned\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "boxplot = df_metrics_all.boxplot([\"EndError_cleaned\"], by = [\"height\", \"tempos\", \"TargetLateral\", \"TargetVertical\"],\n",
    "                     figsize = (16, 9),\n",
    "                     showmeans = True,\n",
    "                     notch = True)\n",
    "\n",
    "boxplot.set_xlabel(\"Conditions\")\n",
    "boxplot.set_ylabel(\"Error (m)\")\n",
    "\n",
    "plt.savefig(\"boxplot.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('PtxID shape: ', df_metrics_all[\"PtxID\"].shape)\n",
    "print('TargetLateral shape', df_metrics_all[\"TargetLateral\"].shape)\n",
    "print('TargetVertical shape: ', df_metrics_all[\"TargetVertical\"].shape)\n",
    "print('tempos shape: ', df_metrics_all[\"tempos\"].shape)\n",
    "print('EndError_cleaned shape: ', df_metrics_all[\"EndError_cleaned\"].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.formula.api as smf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model = smf.mixedlm(\"EndError ~ C(height) + tempos + C(TargetID)\",\n",
    "                    data = df_metrics_all, \n",
    "                    groups= df_metrics_all[\"PtxID\"])\n",
    "\n",
    "\n",
    "model_fit = model.fit()\n",
    "print(model_fit.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\nEndError on Target Location')\n",
    "mask = df_metrics_all['TargetLateral'] == 'center'\n",
    "print('Average Error Center: ', np.round(df_metrics_all[mask]['EndError_cleaned'].mean(),2), ' cm', ' SD: ', np.round(df_metrics_all[mask]['EndError_cleaned'].std(),2))\n",
    "mask = df_metrics_all['TargetLateral'] == 'left'\n",
    "print('Average Error Left: ', np.round(df_metrics_all[mask]['EndError_cleaned'].mean(),2), ' cm', ' SD: ', np.round(df_metrics_all[mask]['EndError_cleaned'].std(),2))\n",
    "mask = df_metrics_all['TargetLateral'] == 'right'\n",
    "print('Average Error Right: ', np.round(df_metrics_all[mask]['EndError_cleaned'].mean(),2), ' cm', ' SD: ', np.round(df_metrics_all[mask]['EndError_cleaned'].std(),2))\n",
    "\n",
    "print('\\nEndError on Target Vertical')\n",
    "mask = df_metrics_all['TargetVertical'] == 'middle'\n",
    "print('Average Error Middle: ', np.round(df_metrics_all[mask]['EndError_cleaned'].mean(),2), ' cm', ' SD: ', np.round(df_metrics_all[mask]['EndError_cleaned'].std(),2))\n",
    "mask = df_metrics_all['TargetVertical'] == 'far'\n",
    "print('Average Error Far: ', np.round(df_metrics_all[mask]['EndError_cleaned'].mean(),2), ' cm', ' SD: ', np.round(df_metrics_all[mask]['EndError_cleaned'].std(),2))\n",
    "mask = df_metrics_all['TargetVertical'] == 'close'\n",
    "print('Average Error Close: ', np.round(df_metrics_all[mask]['EndError_cleaned'].mean(),2), ' cm', ' SD: ', np.round(df_metrics_all[mask]['EndError_cleaned'].std(),2))\n",
    "\n",
    "print('\\nError on Tempo')\n",
    "# mask = df_metrics_all['tempos'] == '80'\n",
    "print('Average Error on Tempo: ', np.round(df_metrics_all['EndError_cleaned'].mean(),2), ' cm', ' SD: ', np.round(df_metrics_all['EndError_cleaned'].std(),2))\n",
    "# mask = df_metrics_all['tempos'] == '120'\n",
    "# print('Average Error 120bmp: ', np.round(df_metrics_all[mask]['EndError_cleaned'].mean(),2), ' cm', ' SD: ', np.round(df_metrics_all[mask]['EndError_cleaned'].std(),2))\n",
    "# mask = df_metrics_all['tempos'] == '160'\n",
    "# print('Average Error 160bmp: ', np.round(df_metrics_all[mask]['EndError_cleaned'].mean(),2), ' cm', ' SD: ', np.round(df_metrics_all[mask]['EndError_cleaned'].std(),2))\n",
    "\n",
    "print('\\nError on Height')\n",
    "mask = (df_metrics_all['height'] == 'low') & (df_metrics_all['height'] == 'mid') & (df_metrics_all['height'] == 'high')\n",
    "print('Average Error on Height: ', np.round(df_metrics_all[mask]['EndError_cleaned'].mean(),2), ' cm', ' SD: ', np.round(df_metrics_all[mask]['EndError_cleaned'].std(),2))\n",
    "# mask = df_metrics_all['height'] == 'mid'\n",
    "# print('Average Error Mid: ', np.round(df_metrics_all[mask]['EndError_cleaned'].mean(),2), ' cm', ' SD: ', np.round(df_metrics_all[mask]['EndError_cleaned'].std(),2))\n",
    "# mask = df_metrics_all['height'] == 'high'\n",
    "# print('Average Error High: ', np.round(df_metrics_all[mask]['EndError_cleaned'].mean(),2), ' cm', ' SD: ', np.round(df_metrics_all[mask]['EndError_cleaned'].std(),2))\n",
    "\n",
    "\n",
    "print('\\nPathOffset on Target Location')\n",
    "mask = df_metrics_all['TargetLateral'] == 'center'\n",
    "print('Average Offset Center: ', np.round(df_metrics_all['PathOffsetNoLag_cleaned'].mean(),2), ' cm', ' SD: ', np.round(df_metrics_all[mask]['PathOffsetNoLag_cleaned'].std(),2))\n",
    "# mask = df_metrics_all['TargetLateral'] == 'left'\n",
    "# print('Average Offset Left: ', np.round(df_metrics_all[mask]['PathOffsetNoLag_cleaned'].mean(),2), ' cm', ' SD: ', np.round(df_metrics_all[mask]['PathOffsetNoLag_cleaned'].std(),2))\n",
    "# mask = df_metrics_all['TargetLateral'] == 'right'\n",
    "# print('Average Offset Right: ', np.round(df_metrics_all[mask]['PathOffsetNoLag_cleaned'].mean(),2), ' cm', ' SD: ', np.round(df_metrics_all[mask]['PathOffsetNoLag_cleaned'].std(),2))\n",
    "\n",
    "print('\\nPathOffset on Target Vertical')\n",
    "mask = df_metrics_all['TargetVertical'] == 'middle'\n",
    "print('Average Offset Middle: ', np.round(df_metrics_all['PathOffsetNoLag_cleaned'].mean(),2), ' cm', ' SD: ', np.round(df_metrics_all[mask]['PathOffsetNoLag_cleaned'].std(),2))\n",
    "# mask = df_metrics_all['TargetVertical'] == 'far'\n",
    "# print('Average Offset Far: ', np.round(df_metrics_all[mask]['PathOffsetNoLag_cleaned'].mean(),2), ' cm', ' SD: ', np.round(df_metrics_all[mask]['PathOffsetNoLag_cleaned'].std(),2))\n",
    "# mask = df_metrics_all['TargetVertical'] == 'close'\n",
    "# print('Average Offset Close: ', np.round(df_metrics_all[mask]['PathOffsetNoLag_cleaned'].mean(),2), ' cm', ' SD: ', np.round(df_metrics_all[mask]['PathOffsetNoLag_cleaned'].std(),2))\n",
    "\n",
    "print('\\nOffset on Tempo')\n",
    "mask = df_metrics_all['tempos'] == '80'\n",
    "print('Average Offset 80bmp: ', np.round(df_metrics_all[mask]['PathOffsetNoLag_cleaned'].mean(),2), ' cm', ' SD: ', np.round(df_metrics_all[mask]['PathOffsetNoLag_cleaned'].std(),2))\n",
    "mask = df_metrics_all['tempos'] == '120'\n",
    "print('Average Offset 120bmp: ', np.round(df_metrics_all[mask]['PathOffsetNoLag_cleaned'].mean(),2), ' cm', ' SD: ', np.round(df_metrics_all[mask]['PathOffsetNoLag_cleaned'].std(),2))\n",
    "mask = df_metrics_all['tempos'] == '160'\n",
    "print('Average Offset 160bmp: ', np.round(df_metrics_all[mask]['PathOffsetNoLag_cleaned'].mean(),2), ' cm', ' SD: ', np.round(df_metrics_all[mask]['PathOffsetNoLag_cleaned'].std(),2))\n",
    "\n",
    "print('\\nOffset on Height')\n",
    "mask = df_metrics_all['height'] == 'low'\n",
    "print('Average Offset Low: ', np.round(df_metrics_all[mask]['PathOffsetNoLag_cleaned'].mean(),2), ' cm', ' SD: ', np.round(df_metrics_all[mask]['PathOffsetNoLag_cleaned'].std(),2))\n",
    "mask = df_metrics_all['height'] == 'mid'\n",
    "print('Average Offset Low: ', np.round(df_metrics_all[mask]['PathOffsetNoLag_cleaned'].mean(),2), ' cm', ' SD: ', np.round(df_metrics_all[mask]['PathOffsetNoLag_cleaned'].std(),2))\n",
    "mask = df_metrics_all['height'] == 'high'\n",
    "print('Average Offset Low: ', np.round(df_metrics_all[mask]['PathOffsetNoLag_cleaned'].mean(),2), ' cm', ' SD: ', np.round(df_metrics_all[mask]['PathOffsetNoLag_cleaned'].std(),2))\n",
    "\n",
    "\n",
    "\n",
    "print('\\nLag on Tempo')\n",
    "mask = (df_metrics_all['tempos'] == '80') & (df_metrics_all['Lag'] < 200)\n",
    "print('Average Lag 80bmp: ', np.round(df_metrics_all[mask]['Lag'].mean(),2), ' ms', ' SD: ', np.round(df_metrics_all[mask]['Lag'].std(),2))\n",
    "mask = (df_metrics_all['tempos'] == '120') & (df_metrics_all['Lag'] < 200)\n",
    "print('Average Lag 120bmp: ', np.round(df_metrics_all[mask]['Lag'].mean(),2), ' ms', ' SD: ', np.round(df_metrics_all[mask]['Lag'].std(),2))\n",
    "mask = (df_metrics_all['tempos'] == '160') & (df_metrics_all['Lag'] < 200)\n",
    "print('Average Lag 160bmp: ', np.round(df_metrics_all[mask]['Lag'].mean(),2), ' ms', ' SD: ', np.round(df_metrics_all[mask]['Lag'].std(),2))\n",
    "\n",
    "mask = (df_metrics_all['Lag'] < 200)\n",
    "print('\\nAverage Overall Delay: ', np.round(df_metrics_all[mask]['Lag'].mean(),2),' ms')\n",
    "print('Delay SD: ', np.round(df_metrics_all[mask]['Lag'].std(),2),' ms')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "detailing = False\n",
    "\n",
    "res1 = pg.rm_anova(dv='EndError_cleaned', within=['height', 'tempos'], \n",
    "                  subject='PtxID', data=df_metrics_all,  detailed=detailing)\n",
    "print('\\n EndError HT\\n', res1.round(3))\n",
    "\n",
    "res2 = pg.rm_anova(dv='EndError_cleaned', within=['TargetLateral', 'TargetVertical'], \n",
    "                  subject='PtxID', data=df_metrics_all,  detailed=detailing)\n",
    "print('\\n EndError LV\\n', res2.round(3))\n",
    "\n",
    "res3 = pg.rm_anova(dv='PathOffset_cleaned', within=['height', 'tempos'], \n",
    "                  subject='PtxID', data=df_metrics_all,  detailed=detailing)\n",
    "print('\\n PathOffset_cleaned HT\\n', res3.round(3))\n",
    "\n",
    "res4 = pg.rm_anova(dv='PathOffset_cleaned', within=['TargetLateral', 'TargetVertical'], \n",
    "                  subject='PtxID', data=df_metrics_all,  detailed=detailing)\n",
    "print('\\n PathOffset_cleaned LV\\n', res4.round(3))\n",
    "\n",
    "res5 = pg.rm_anova(dv='PathOffset_cleaned', within=['TargetLateral', 'tempos'], \n",
    "                     subject='PtxID', data=df_metrics_all, detailed=detailing)\n",
    "print('\\n PathOffset_cleaned LT\\n', res5.round(3))\n",
    "\n",
    "res5 = pg.rm_anova(dv='PathOffsetNoLag_cleaned', within=['TargetLateral', 'tempos'], \n",
    "                     subject='PtxID', data=df_metrics_all, detailed=detailing)\n",
    "print('\\n PathOffsetNoLag_cleaned LT\\n', res5.round(3))\n",
    "\n",
    "res6 = pg.rm_anova(dv='Lag', within=['tempos', 'height'], \n",
    "                  subject='PtxID', data=df_metrics_all,  detailed=detailing)\n",
    "print('\\n Lag HT\\n', res6.round(3))\n",
    "\n",
    "# res7 = pg.rm_anova(dv='Lag_cleaned', within=['tempos', 'height'], \n",
    "#                   subject='PtxID', data=df_metrics_all,  detailed=True)\n",
    "# print('\\n Lag_cleaned HT Cleaned\\n', res7.round(3))\n",
    "\n",
    "# print('power: %.4f' % power_rm_anova(eta=0.861, m=2, n=5))\n",
    "\n",
    "# print('n: %.4f' % power_rm_anova(eta=0.861, m=2, power=0.8))\n",
    "\n",
    "# print('eta: %.4f' % power_rm_anova(n=5, m=2, power=0.95, alpha=0.05))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Post hoc (pairwise t-tests) w/o correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# res2 = pg.rm_anova(dv='EndError_cleaned', within=['TargetLateral', 'TargetVertical'], \n",
    "#                   subject='PtxID', data=df_metrics_all,  detailed=detailing)\n",
    "# print('\\n EndError LV\\n', res2.round(3))\n",
    "\n",
    "posthocs = pg.pairwise_ttests(dv='EndError_cleaned', within=['height', 'TargetID'], \n",
    "                              subject='PtxID', data=df_metrics_all)\n",
    "# pg.print_table(posthocs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_metrics_all.groupby(['TargetLateral', 'TargetVertical'])['EndError_cleaned'].agg(['mean', 'std']).round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "posthocs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Path offset post-hoc analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "posthocs2 = pg.pairwise_ttests(dv='PathOffsetNoLag_cleaned', within=['TargetLateral', 'TargetVertical'], \n",
    "                              subject='PtxID', data=df_metrics_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_metrics_all.groupby(['TargetLateral', 'TargetVertical'])['PathOffsetNoLag_cleaned'].agg(['mean', 'std']).round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "posthocs3 = pg.pairwise_ttests(dv='PathOffsetNoLag_cleaned', within=['height', 'tempos'], \n",
    "                              subject='PtxID', data=df_metrics_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "posthocs3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# result  = df_metrics_all.to_json(orient=\"split\")\n",
    "\n",
    "# f = open(\"jsondataframe.txt\", \"a\")\n",
    "# f.write(result)\n",
    "# f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = (df_metrics_all['tempos'] == '80') & (df_metrics_all['height'] == 'low') \n",
    "ptxAvMaxVels = df_metrics_all[mask].groupby('PtxID')['MaxVel_Real'].mean()\n",
    "ptxSDMaxVels = df_metrics_all[mask].groupby('PtxID')['MaxVel_Real'].std()\n",
    "SE = ptxSDMaxVels / np.sqrt(len(ptxSDMaxVels))\n",
    "xVals = np.arange(0,5)\n",
    "\n",
    "plt.errorbar(xVals, ptxAvMaxVels, SE, color='r')\n",
    "plt.plot(ptxAvMaxVels,'ko',ms=12)\n",
    "plt.xlim([-0.5,5.5])\n",
    "plt.xlabel('Participants')\n",
    "plt.ylim([-1,5])\n",
    "plt.ylabel('Peak Velocity (ms-1)')\n",
    "\n",
    "mask = (df_metrics_all['tempos'] == '120') & (df_metrics_all['height'] == 'low') \n",
    "ptxAvMaxVels = df_metrics_all[mask].groupby('PtxID')['MaxVel_Real'].mean()\n",
    "ptxSDMaxVels = df_metrics_all[mask].groupby('PtxID')['MaxVel_Real'].std()\n",
    "SE = ptxSDMaxVels / np.sqrt(len(ptxSDMaxVels))\n",
    "plt.errorbar(xVals, ptxAvMaxVels, SE, color='g')\n",
    "plt.plot(ptxAvMaxVels,'ko',ms=12)\n",
    "\n",
    "mask = (df_metrics_all['tempos'] == '160') & (df_metrics_all['height'] == 'low') \n",
    "ptxAvMaxVels = df_metrics_all[mask].groupby('PtxID')['MaxVel_Real'].mean()\n",
    "ptxSDMaxVels = df_metrics_all[mask].groupby('PtxID')['MaxVel_Real'].std()\n",
    "SE = ptxSDMaxVels / np.sqrt(len(ptxSDMaxVels))\n",
    "plt.errorbar(xVals, ptxAvMaxVels, SE, color='b')\n",
    "plt.plot(ptxAvMaxVels,'ko',ms=12)\n",
    "\n",
    "plt.legend(['80', '120', '160'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = (df_metrics_all['tempos'] == '80') & (df_metrics_all['height'] == 'low') \n",
    "ptxAvMaxVels = df_metrics_all[mask].groupby('PtxID')['PathOffsetNoLag_cleaned'].mean()\n",
    "ptxSDMaxVels = df_metrics_all[mask].groupby('PtxID')['PathOffsetNoLag_cleaned'].std()\n",
    "SE = ptxSDMaxVels / np.sqrt(len(ptxSDMaxVels))\n",
    "xVals = np.arange(0,5)\n",
    "\n",
    "# print('SD: ', len(ptxSDMaxVels), ' \\nSE: ', len(SE), ' \\nxVals: ', len(xVals))\n",
    "\n",
    "plt.errorbar(xVals, ptxAvMaxVels, SE, color='r')\n",
    "plt.legend(['80'])\n",
    "plt.plot(ptxAvMaxVels,'ko',ms=12)\n",
    "plt.xlim([-0.5,5])\n",
    "plt.xlabel('Participants')\n",
    "# plt.ylim([-1,5])\n",
    "plt.ylabel('Path Offset (cm)')\n",
    "\n",
    "mask = (df_metrics_all['tempos'] == '120') & (df_metrics_all['height'] == 'low') \n",
    "ptxAvMaxVels = df_metrics_all[mask].groupby('PtxID')['PathOffsetNoLag_cleaned'].mean()\n",
    "ptxSDMaxVels = df_metrics_all[mask].groupby('PtxID')['PathOffsetNoLag_cleaned'].std()\n",
    "SE = ptxSDMaxVels / np.sqrt(len(ptxSDMaxVels))\n",
    "plt.errorbar(xVals, ptxAvMaxVels, SE, color='g')\n",
    "plt.plot(ptxAvMaxVels,'ko',ms=12)\n",
    "\n",
    "mask = (df_metrics_all['tempos'] == '160') & (df_metrics_all['height'] == 'low') \n",
    "ptxAvMaxVels = df_metrics_all[mask].groupby('PtxID')['PathOffsetNoLag_cleaned'].mean()\n",
    "ptxSDMaxVels = df_metrics_all[mask].groupby('PtxID')['PathOffsetNoLag_cleaned'].std()\n",
    "SE = ptxSDMaxVels / np.sqrt(len(ptxSDMaxVels))\n",
    "plt.errorbar(xVals, ptxAvMaxVels, SE, color='b')\n",
    "plt.plot(ptxAvMaxVels,'ko',ms=12)\n",
    "\n",
    "# plt.legend(['80', '120', '160'])\n",
    "\n",
    "plt.ylim([2,7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = (df_metrics_all['tempos'] == '80') & (df_metrics_all['height'] == 'low') \n",
    "ptxAvMaxVels = df_metrics_all[mask].groupby('PtxID')['EndError_cleaned'].mean()\n",
    "ptxSDMaxVels = df_metrics_all[mask].groupby('PtxID')['EndError_cleaned'].std()\n",
    "SE = ptxSDMaxVels / np.sqrt(len(ptxSDMaxVels))\n",
    "xVals = np.arange(0,5)\n",
    "plt.errorbar(xVals, ptxAvMaxVels, SE, color='r')\n",
    "plt.plot(ptxAvMaxVels,'ko',ms=12)\n",
    "plt.xlim([-0.5,5])\n",
    "plt.xlabel('Participants')\n",
    "# plt.ylim([-1,5])\n",
    "plt.ylabel('EndError (cm)')\n",
    "\n",
    "mask = (df_metrics_all['tempos'] == '120') & (df_metrics_all['height'] == 'low') \n",
    "ptxAvMaxVels = df_metrics_all[mask].groupby('PtxID')['EndError_cleaned'].mean()\n",
    "ptxSDMaxVels = df_metrics_all[mask].groupby('PtxID')['EndError_cleaned'].std()\n",
    "SE = ptxSDMaxVels / np.sqrt(len(ptxSDMaxVels))\n",
    "plt.errorbar(xVals, ptxAvMaxVels, SE, color='g')\n",
    "plt.plot(ptxAvMaxVels,'ko',ms=12)\n",
    "\n",
    "mask = (df_metrics_all['tempos'] == '160') & (df_metrics_all['height'] == 'low') \n",
    "ptxAvMaxVels = df_metrics_all[mask].groupby('PtxID')['EndError_cleaned'].mean()\n",
    "ptxSDMaxVels = df_metrics_all[mask].groupby('PtxID')['EndError_cleaned'].std()\n",
    "SE = ptxSDMaxVels / np.sqrt(len(ptxSDMaxVels))\n",
    "plt.errorbar(xVals, ptxAvMaxVels, SE, color='b')\n",
    "plt.plot(ptxAvMaxVels,'ko',ms=12)\n",
    "\n",
    "# plt.legend(['80', '120', '160'])\n",
    "\n",
    "# plt.ylim([-1,15])\n",
    "\n",
    "# print('SD: ', len(ptxSDMaxVels), ' \\nSE: ', len(SE), ' \\nxVals: ', len(xVals))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ptxIds = pd.unique(df_metrics_all['PtxID'])\n",
    "print('Participants: ', ptxIds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "\n",
    "plt.subplot(1,2,1)\n",
    "sns.barplot(x='tempos', y = 'PathOffsetNoLag_cleaned', data=df_metrics_all)\n",
    "plt.title('Path Offset (cm)')\n",
    "plt.xlabel('Tempo')\n",
    "# plt.ylim([0, 75])\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "sns.barplot(x='tempos', y = 'EndError_cleaned', data=df_metrics_all)\n",
    "# plt.ylim([0, 10])\n",
    "plt.title('Target Hit Error (cm)')\n",
    "plt.xlabel('Tempo')\n",
    "\n",
    "# plt.subplot(1,4,3)\n",
    "# sns.barplot(x='tempos', y = 'Lag', data=df_metrics_all)\n",
    "# plt.ylim([0, 100])\n",
    "# plt.title('Target Hit Error (cm)')\n",
    "# plt.xlabel('Tempo')\n",
    "\n",
    "# plt.subplot(1,4,4)\n",
    "# sns.barplot(x='tempos', y = 'MaxVel_Virt', palette='Blues', data=df_metrics_all)\n",
    "# sns.barplot(x='tempos', y = 'MaxVel_Real', palette='Greens', data=df_metrics_all)\n",
    "# plt.ylim([0, 10])\n",
    "# plt.title('Target Hit Error (cm)')\n",
    "# plt.xlabel('Tempo')\n",
    "plt.legend(['Virtual','Real'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# print('Participants: ', ptxIds)\n",
    "# print('Heights: ', pd.unique(df_all['height']))\n",
    "# print('Tempos: ', pd.unique(df_all['tempos']))\n",
    "\n",
    "# ptxNum = 0\n",
    "# h_mask_low = (df_all['tempos'] == '80') & (df_all['PtxID'] == ptxIds[ptxNum])\n",
    "# h_mask_mid = (df_all['tempos'] == '120') & (df_all['PtxID'] == ptxIds[ptxNum])\n",
    "# h_mask_hig = (df_all['tempos'] == '160') & (df_all['PtxID'] == ptxIds[ptxNum])\n",
    "# h_mask_nh = (df_all['tempos'] == 'noTempo') & (df_all['PtxID'] == ptxIds[ptxNum])\n",
    "\n",
    "# print('\\n Ptx name: ', ptxIds[ptxNum])\n",
    "# print('Num of 80 trials: ' , len(df_all[h_mask_low]['TrialNum'].values))\n",
    "# print('Num of 120 trials: ' , len(df_all[h_mask_mid]['TrialNum'].values))\n",
    "# print('Num of 160 trials: ' , len(df_all[h_mask_hig]['TrialNum'].values))\n",
    "# print('Num of no tempo trials: ' , len(df_all[h_mask_nh]['TrialNum'].values))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bYqucWyWp0Qv"
   },
   "source": [
    "#@title Default title text\n",
    "# TO-DOs\n",
    "- [x] Make sure rotations do not jumpt back to zero from 360 i.e. cap rotations to a specific region \n",
    "- [x] Create a nice data frame with all the values in the correct format, and add more parameters to the dataframe\n",
    "- [x] Create average plots with standard deviation as shaded area\n",
    "\n",
    "\n",
    "- [ ] In \"ComputeErrors\" function implement more extensive data frame with\n",
    "  - [ ] Participant ID\n",
    "  - [ ] Target ID and location \n",
    "  - [ ] Height, tempo and trial number "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "masktemp = (df_all['gameObjectName'] == 'realFingerTip')\n",
    "trialsofinterest = pd.unique(df_all[masktemp].TrialNum)\n",
    "    \n",
    "ptxMask = (df_all['gameObjectName'] == 'r_index_fingernail_marker') & (df_all['TrialNum'] == trialsofinterest[5])\n",
    "ptx = pd.unique(df_all[ptxMask]['PtxID'].values)[1]\n",
    "ptx\n",
    "\n",
    "# ptx = 'Poppy' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "81ZmjEQQpcd4",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ptx = pd.unique(df_all['PtxID'])\n",
    "ptxNum = 3\n",
    "print('Participant: ', ptx[ptxNum])\n",
    "\n",
    "mask = (df_all['PtxID'] == ptx[ptxNum]) & (df_all['height'] == 'high') & (df_all['tempos'] == '120')& (df_all['targetID'] == 'row_C3')  & (df_all['gameObjectName'] == 'r_index_fingernail_marker')\n",
    "real_mask = (df_all['PtxID'] == ptx[ptxNum]) & (df_all['height'] == 'high') & (df_all['tempos'] == '120')& (df_all['targetID'] == 'row_C3')  & (df_all['gameObjectName'] == 'realFingerTip')\n",
    "\n",
    "plt.plot(df_all[real_mask]['xPos'].values[-1-150:-1], df_all[real_mask]['zPos'].values[-1-150:-1],'k-o')\n",
    "plt.plot(df_all[mask]['xPos'].values[-1-150:-1], df_all[mask]['zPos'].values[-1-150:-1],'m-o')\n",
    "\n",
    "plt.title('Example Reaching Trial')\n",
    "plt.ylabel('Z-Axis / m')\n",
    "plt.xlabel('X-Axis / m')\n",
    "plt.legend(['Real','Virtual'])\n",
    "# plt.xlim([-1.175, -0.8])\n",
    "# plt.ylim([0, 0.55])\n",
    "\n",
    "plt.savefig(ptx[ptxNum] + \"_\" + str(np.round(time.time())) + '_ExampleReach.png', dpi=600, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ptx = 'Susan'\n",
    "mask = (df_all['PtxID'] == ptx[ptxNum]) & (df_all['height'] == 'low') & (df_all['tempos'] == '80')  & (df_all['gameObjectName'] == 'r_index_fingernail_marker')\n",
    "realmask = (df_all['PtxID'] == ptx[ptxNum]) & (df_all['height'] == 'low') & (df_all['tempos'] == '80')  & (df_all['gameObjectName'] == 'realFingerTip')\n",
    "\n",
    "plt.plot(df_all[realmask]['xPos'].values[-1-1500:-1], df_all[realmask]['zPos'].values[-1-1500:-1],'k-o')\n",
    "plt.plot(df_all[mask]['xPos'].values[-1-1500:-1], df_all[mask]['zPos'].values[-1-1500:-1],'m-o', ms = 3)\n",
    "\n",
    "plt.title('Example Reaches')\n",
    "plt.ylabel('Z-Axis / m')\n",
    "plt.xlabel('X-Axis / m')\n",
    "\n",
    "plt.savefig(str(np.round(time.time())) + '_ExampleReaches.png', dpi=600, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_1 = (df_all['PtxID'] == ptx[ptxNum]) & (df_all['height'] == 'mid') & (df_all['tempos'] == '120')& (df_all['targetID'] == 'row_A1')  & (df_all['gameObjectName'] == 'r_index_fingernail_marker')\n",
    "trials = pd.unique(df_all[mask_1]['TrialNum'])\n",
    "\n",
    "if len(trials) < 1:\n",
    "    mask_1 = (df_all['PtxID'] == ptx[ptxNum]) & (df_all['height'] == 'noHeight') & (df_all['tempos'] == '120')& (df_all['targetID'] == 'row_A1')  & (df_all['gameObjectName'] == 'r_index_fingernail_marker')\n",
    "    trials = pd.unique(df_all[mask_1]['TrialNum'])\n",
    "\n",
    "trials\n",
    "\n",
    "# mask_2 = (df_all['TrialNum'] == trials[5]) & (df_all['PtxID'] == 'Davide') & (df_all['height'] == 'noHeight') & (df_all['tempo'] == '120')& (df_all['targetID'] == 'row_A1')  & (df_all['gameObjectName'] == 'r_index_fingernail_marker')\n",
    "# plt.plot(df_all[mask_2]['zPos'],'r-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Save resultant dataframe data to csv\n",
    "\n",
    "compression_opts = dict(method='zip',\n",
    "                        archive_name='df_all.csv')  \n",
    "# df_all.to_csv('df_all.zip', index=False,\n",
    "#           compression=compression_opts)  \n",
    "\n",
    "df_all.to_csv('df_all.csv', index=False)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_metrics_all.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Mixed Effects Model Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "from statsmodels.tools.sm_exceptions import ConvergenceWarning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df_metrics_all.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = (8,5)\n",
    "sns.set_theme(style=\"whitegrid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Sort by target id\n",
    "data_sort = data.sort_values(by = 'TargetID')\n",
    "\n",
    "sns.set()\n",
    "ax = sns.pointplot(data=data_sort, x='TargetID', y='EndError_cleaned',hue='height', dodge=True,\n",
    "              capsize=.1, errwidth=1, palette='rocket')\n",
    "\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "# plt.legend(bbox_to_anchor=(1.05, 1.0), loc='upper left')\n",
    "plt.xlabel('Target IDs', fontsize=14)\n",
    "plt.ylabel('Error (cm)', fontsize=14)\n",
    "plt.xticks([0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17],\n",
    "           ['A1','A2','A3','A4','A5','A6','B1','B2','B3','B4','B5','B6','C1','C2','C3','C4','C5','C6'])\n",
    "\n",
    "ax.set_facecolor((0.95,0.95,0.95))\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.title('Positional Error',fontsize=16.5)\n",
    "\n",
    "plt.savefig(str(np.round(time.time())) + '_Error_IP.png', dpi=600, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort by target id\n",
    "data_sort = data.sort_values(by = 'TargetID')\n",
    "\n",
    "sns.set()\n",
    "ax = sns.pointplot(data=data_sort, x='TargetID', y='Lag',hue='height', dodge=True,\n",
    "              capsize=.1, errwidth=1, palette='rocket')\n",
    "\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "# plt.legend(bbox_to_anchor=(1.05, 1.0), loc='upper left')\n",
    "plt.xlabel('Target IDs', fontsize=14)\n",
    "plt.ylabel('Tempo (bpm)', fontsize=14)\n",
    "plt.xticks([0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17],\n",
    "           ['A1','A2','A3','A4','A5','A6','B1','B2','B3','B4','B5','B6','C1','C2','C3','C4','C5','C6'])\n",
    "\n",
    "ax.set_facecolor((0.95,0.95,0.95))\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.title('Tempos',fontsize=16.5)\n",
    "\n",
    "plt.savefig(str(np.round(time.time())) + '_Lag_IP.png', dpi=600, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = (data_sort['height'] == 'low') & (data_sort['TargetLateral'] == 'left')\n",
    "left_low = data_sort[mask]['EndError_cleaned'].mean()\n",
    "\n",
    "mask = (data_sort['height'] == 'low') & (data_sort['TargetLateral'] == 'right')\n",
    "right_low = data_sort[mask]['EndError_cleaned'].mean()\n",
    "\n",
    "mask = (data_sort['height'] == 'low') & (data_sort['TargetLateral'] == 'center')\n",
    "center_low = data_sort[mask]['EndError_cleaned'].mean()\n",
    "\n",
    "\n",
    "\n",
    "mask = (data_sort['height'] == 'mid') & (data_sort['TargetLateral'] == 'left')\n",
    "left_mid = data_sort[mask]['EndError_cleaned'].mean()\n",
    "\n",
    "mask = (data_sort['height'] == 'mid') & (data_sort['TargetLateral'] == 'right')\n",
    "right_mid = data_sort[mask]['EndError_cleaned'].mean()\n",
    "\n",
    "mask = (data_sort['height'] == 'mid') & (data_sort['TargetLateral'] == 'center')\n",
    "center_mid = data_sort[mask]['EndError_cleaned'].mean()\n",
    "\n",
    "\n",
    "\n",
    "mask = (data_sort['height'] == 'high') & (data_sort['TargetLateral'] == 'left')\n",
    "left_high = data_sort[mask]['EndError_cleaned'].mean()\n",
    "\n",
    "mask = (data_sort['height'] == 'high') & (data_sort['TargetLateral'] == 'right')\n",
    "right_high = data_sort[mask]['EndError_cleaned'].mean()\n",
    "\n",
    "mask = (data_sort['height'] == 'high') & (data_sort['TargetLateral'] == 'center')\n",
    "center_high = data_sort[mask]['EndError_cleaned'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "peri_low = np.nanmean([left_low, right_low])\n",
    "center_low\n",
    "\n",
    "peri_mid = np.nanmean([left_mid, right_mid])\n",
    "center_mid\n",
    "\n",
    "peri_high = np.nanmean([left_high,right_high])\n",
    "center_high\n",
    "\n",
    "x = ['PeriLow', 'CenterLow', 'PeriMid', 'CenterMid', 'PeriHigh','CenterHigh']\n",
    "y = [peri_low, center_low, peri_mid, center_mid, peri_high, center_high]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "vals = list(zip(x,y)) # Create a two column matrix \n",
    "error_data_av = pd.DataFrame(vals, columns =['X','Y'])\n",
    "error_data_av"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_center = [center_low, center_mid, center_high]\n",
    "y_peri = [peri_low, peri_mid, peri_high]\n",
    "\n",
    "print('M center: ', np.nanmean(y_center), ' SD: ', np.nanstd(y_center))\n",
    "print('M peri: ', np.nanmean(y_peri), ' SD: ', np.nanstd(y_peri))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sns.boxplot(x = ['PeriLow', 'CenterLow', 'PeriMid', 'CenterMid', 'PeriHigh','CenterHigh'], y= y)\n",
    "\n",
    "plt.figure()\n",
    "sns.barplot(x='X',y='Y', data=error_data_av, palette='Blues')\n",
    "plt.title('Error', fontsize=titleFontSize)\n",
    "plt.xlabel('Positions')\n",
    "plt.ylabel('Error (cm)')\n",
    "# plt.ylim([-10,200])\n",
    "plt.grid(False)\n",
    "plt.legend(bbox_to_anchor=(1.05, 1.0), loc='upper left')\n",
    "# plt.savefig(str(np.round(time.time())) + '_Delay_Tempos.png', dpi=600, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # Sort by target id\n",
    "# data_sort = data.sort_values(by = 'TargetID')\n",
    "# # sns.set(rc={'axes.facecolor':'white', 'figure.facecolor':'white'})\n",
    "\n",
    "# sns.set()\n",
    "# sns.pointplot(data=data_sort, x='TargetID', y='PathOffsetNoLag_cleaned',hue='height', dodge=True,\n",
    "#               capsize=.1, errwidth=1, palette='colorblind')\n",
    "\n",
    "# plt.xticks(rotation=45)\n",
    "\n",
    "# plt.legend(bbox_to_anchor=(1.05, 1.0), loc='upper left')\n",
    "# plt.xlabel('Target IDs')\n",
    "# plt.ylabel('Offset (cm)')\n",
    "# plt.xticks([0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17],['A1','A2','A3','A4','A5','A6',\n",
    "#                                                          'B1','B2','B3','B4','B5','B6',\n",
    "#                                                          'C1','C2','C3','C4','C5','C6'])\n",
    "\n",
    "# ax.set_facecolor((0.95,0.95,0.95))\n",
    "# sns.set_style(\"whitegrid\")\n",
    "\n",
    "# plt.savefig(str(np.round(time.time())) + '_PathOffset_TargID_Height_IP.png', dpi=600, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = (10,5)\n",
    "sns.set_theme(style=\"whitegrid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Sort by target id\n",
    "#---------------------------------------------------------------------------------------------------------\n",
    "#------------------------------------- Path offset tempos  ------------------------------------------\n",
    "#---------------------------------------------------------------------------------------------------------\n",
    "plt.subplot(121)\n",
    "plt.tight_layout(pad=3.0)\n",
    "data_sort = data.sort_values(by = 'TargetID')\n",
    "# sns.set(rc={'axes.facecolor':'white', 'figure.facecolor':'white'})\n",
    "\n",
    "sns.set()\n",
    "ax = sns.pointplot(data=data_sort, x='TargetID', y='PathOffsetNoLag_cleaned',hue='tempos', dodge=True,\n",
    "              capsize=.1, errwidth=1, palette='rocket')\n",
    "\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "# plt.legend(bbox_to_anchor=(1.05, 1.0), loc='upper left')\n",
    "plt.xlabel('Target IDs', fontsize=14)\n",
    "plt.ylabel('Offset (cm)', fontsize=14)\n",
    "plt.xticks([0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17],['A1','A2','A3','A4','A5','A6',\n",
    "                                                         'B1','B2','B3','B4','B5','B6',\n",
    "                                                         'C1','C2','C3','C4','C5','C6'])\n",
    "plt.title('Path Offset > TargetID - Tempo',fontsize=16.5)\n",
    "ax.set_facecolor((0.95,0.95,0.95))\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "# plt.savefig(str(np.round(time.time())) + '_PathOffset_TargID_Tempo_IP.png', dpi=600, bbox_inches='tight')\n",
    "\n",
    "#---------------------------------------------------------------------------------------------------------\n",
    "#------------------------------------- Path offset heights ------------------------------------------\n",
    "#---------------------------------------------------------------------------------------------------------\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.tight_layout(pad=3.0)\n",
    "data_sort = data.sort_values(by = 'TargetID')\n",
    "# sns.set(rc={'axes.facecolor':'white', 'figure.facecolor':'white'})\n",
    "\n",
    "sns.set()\n",
    "ax = sns.pointplot(data=data_sort, x='tempos', y='PathOffsetNoLag_cleaned',hue='height', dodge=True,\n",
    "              capsize=.1, errwidth=1, palette='rocket')\n",
    "\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "# plt.legend(bbox_to_anchor=(1.05, 1.0), loc='upper left')\n",
    "plt.xlabel('Tempo', fontsize=14)\n",
    "plt.ylabel('Offset (cm)', fontsize=14)\n",
    "# plt.xticks([0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17],['A1','A2','A3','A4','A5','A6',\n",
    "#                                                          'B1','B2','B3','B4','B5','B6',\n",
    "#                                                          'C1','C2','C3','C4','C5','C6'])\n",
    "plt.title('Path Offset > Tempo - Heights',fontsize=16.5)\n",
    "ax.set_facecolor((0.95,0.95,0.95))\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "plt.savefig(str(np.round(time.time())) + '_PathOffset_TargID_Tempo_Height_IP.png', dpi=600, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = (data_sort['tempos'] == 80) & (data_sort['height'] == 'low') \n",
    "mean_80_low = data_sort[mask]['PathOffsetNoLag_cleaned']\n",
    "\n",
    "mask = mask = (data_sort['tempos'] == 120) & (data_sort['height'] == 'low') \n",
    "mean_120_low = data_sort[mask]['PathOffsetNoLag_cleaned']\n",
    "\n",
    "mask = mask = (data_sort['tempos'] == 160) & (data_sort['height'] == 'low') \n",
    "mean_160_low = data_sort[mask]['PathOffsetNoLag_cleaned']\n",
    "\n",
    "mask = (data_sort['tempos'] == 80) & (data_sort['height'] == 'mid') \n",
    "mean_80_mid = data_sort[mask]['PathOffsetNoLag_cleaned']\n",
    "\n",
    "mask = mask = (data_sort['tempos'] == 120) & (data_sort['height'] == 'mid') \n",
    "mean_120_mid = data_sort[mask]['PathOffsetNoLag_cleaned']\n",
    "\n",
    "mask = mask = (data_sort['tempos'] == 160) & (data_sort['height'] == 'mid') \n",
    "mean_160_mid = data_sort[mask]['PathOffsetNoLag_cleaned']\n",
    "\n",
    "mask = (data_sort['tempos'] == 80) & (data_sort['height'] == 'high') \n",
    "mean_80_high = data_sort[mask]['PathOffsetNoLag_cleaned']\n",
    "\n",
    "mask = mask = (data_sort['tempos'] == 120) & (data_sort['height'] == 'high') \n",
    "mean_120_high = data_sort[mask]['PathOffsetNoLag_cleaned']\n",
    "\n",
    "mask = mask = (data_sort['tempos'] == 160) & (data_sort['height'] == 'high') \n",
    "mean_160_high = data_sort[mask]['PathOffsetNoLag_cleaned']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_80_low = np.nanmean(mean_80_low)\n",
    "sd_80_low = np.nanstd(mean_80_low)\n",
    "m_120_low = np.nanmean(mean_120_low)\n",
    "sd_120_low = np.nanstd(mean_120_low)\n",
    "m_160_low = np.nanmean(mean_160_low)\n",
    "sd_160_low = np.nanstd(mean_160_low)\n",
    "m_80_mid = np.nanmean(mean_80_mid)\n",
    "sd_80_mid = np.nanstd(mean_80_mid)\n",
    "m_120_mid = np.nanmean(mean_120_mid)\n",
    "sd_120_mid = np.nanstd(mean_120_mid)\n",
    "m_160_mid = np.nanmean(mean_160_mid)\n",
    "sd_160_mid = np.nanstd(mean_160_mid)\n",
    "m_80_high = np.nanmean(mean_80_high)\n",
    "sd_80_high = np.nanstd(mean_80_high)\n",
    "m_120_high = np.nanmean(mean_120_high)\n",
    "sd_120_high = np.nanstd(mean_120_high)\n",
    "m_160_high = np.nanmean(mean_160_high)\n",
    "sd_160_high = np.nanstd(mean_160_high)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Mean 80 low:      ', np.round(m_80_low,2), '    SD 80 low:   ', np.round(sd_80_low,2))\n",
    "print('Mean 120 low:      ', np.round(m_120_low,2), '    SD 80 low:   ', np.round(sd_120_low,2))\n",
    "print('Mean 160 low:      ', np.round(m_160_low,2), '    SD 80 low:   ', np.round(sd_160_low,2))\n",
    "\n",
    "print('Mean 80 mid:      ', np.round(m_80_mid,2), '    SD 80 mid:   ', np.round(sd_80_mid,2))\n",
    "print('Mean 120 mid:      ', np.round(m_120_mid,2), '    SD 120 mid:   ', np.round(sd_120_mid,2))\n",
    "print('Mean 160 mid:      ', np.round(m_160_mid,2), '    SD 160 mid:   ', np.round(sd_160_mid,2))\n",
    "\n",
    "print('Mean 80 high:      ', np.round(m_80_high,2), '    SD 80 high:   ', np.round(sd_80_high,2))\n",
    "print('Mean 120 high:      ', np.round(m_120_high,2), '    SD 120 high:   ', np.round(sd_120_high,2))\n",
    "print('Mean 160 high:      ', np.round(m_160_high,2), '    SD 160 high:   ', np.round(sd_160_high,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "data[\"tempos\"] = data[\"tempos\"].astype(str)\n",
    "data[\"tempos\"].values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"TrialNum\"] = data[\"TrialNum\"].astype(str)\n",
    "data[\"TrialNum\"].values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv('QuestData_Metrics2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# model1 = smf.ols(formula='EndError_cleaned ~ (tempos * height * TargetID)', data=data).fit() \n",
    "# model1.summary()\n",
    "\n",
    "# model = smf.mixedlm(\"EndError_cleaned ~ (TargetID * height)\", data, groups=data['PtxID'])\n",
    "# res = model.fit() # method=[\"lbfgs\"]\n",
    "# res.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# md = smf.mixedlm(\"EndError_cleaned ~ height * TargetID\", data, groups=data[\"PtxID\"])\n",
    "# mdf = md.fit()\n",
    "# mdf.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distortion Analysis\n",
    "### For each height and tempo create a dataframe with all the movement end-points for each target for the distortion analysis "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "targetID = pd.unique(df_all['targetID'])\n",
    "participants = pd.unique(df_all['PtxID'])\n",
    "\n",
    "height = 'low'\n",
    "tempo = '120'\n",
    "trow = 'B'\n",
    "\n",
    "df_terminal_pos = None \n",
    "\n",
    "for p in participants:\n",
    "    \n",
    "    target = []\n",
    "\n",
    "    for t in targetID: # Go through all the targets but ...\n",
    "\n",
    "        if trow in t: # ... only select one specific row \n",
    "            \n",
    "            \n",
    "#           mask_virtualfinger = (df_all['height'] == height) & (df_all['tempo'] == tempo)  & (df_all['PtxID'] == p)  & (df_all['targetID'] == t)  & (df_all['gameObjectName'] == 'r_index_fingernail_marker')\n",
    "#           mask_realfinger = (df_all['height'] == height) & (df_all['tempo'] == tempo) & (df_all['PtxID'] == p)  & (df_all['targetID'] == t)  & (df_all['gameObjectName'] == 'realFingerTip')\n",
    "\n",
    "            Row_A_point_x = []\n",
    "            Row_A_point_z = []\n",
    "            ste_x = []\n",
    "            ste_z = []\n",
    "\n",
    "            vRow_A_point_x = []\n",
    "            vRow_A_point_z = []\n",
    "            vste_x = []\n",
    "            vste_z = []\n",
    "            \n",
    "            target.append(t)\n",
    "\n",
    "\n",
    "            mask_1 = (df_all['PtxID'] == p) & (df_all['height'] == height) & (df_all['tempos'] == tempo) & (df_all['targetID'] == t)  & (df_all['gameObjectName'] == 'r_index_fingernail_marker')\n",
    "            trials = pd.unique(df_all[mask_1]['TrialNum'])\n",
    "            \n",
    "            if len(trials) < 1:\n",
    "                mask_1 = (df_all['PtxID'] == p) & (df_all['height'] == 'noHeight') & (df_all['tempos'] == tempo) & (df_all['targetID'] == t)  & (df_all['gameObjectName'] == 'r_index_fingernail_marker')\n",
    "                trials = pd.unique(df_all[mask_1]['TrialNum'])\n",
    "            \n",
    "            for tr in trials:\n",
    "                \n",
    "                mask_2_virt = (df_all['TrialNum'] == tr) & (df_all['PtxID'] == p) & (df_all['height'] == height) & (df_all['tempos'] == tempo)& (df_all['targetID'] == t)\n",
    "                \n",
    "                # Virtual finger tip terminal position\n",
    "                end_points_x = df_all[mask_2_virt & (df_all['gameObjectName'] == 'r_index_fingernail_marker')]['xPos'].values[-1-30:-1]\n",
    "                end_points_z = df_all[mask_2_virt & (df_all['gameObjectName'] == 'r_index_fingernail_marker')]['zPos'].values[-1-30:-1]\n",
    "                vRow_A_point_x.append(np.round(np.nanmean(end_points_x, axis=0),5))\n",
    "                vste_x.append(np.round(np.nanstd(end_points_x) / np.sqrt(len(end_points_x)), 5))\n",
    "                vRow_A_point_z.append(np.round(np.nanmean(end_points_z, axis=0),5))\n",
    "                vste_z.append(np.round(np.nanstd(end_points_z) / np.sqrt(len(end_points_z)), 5))\n",
    "\n",
    "                # Real finger tip terminal position            \n",
    "                end_points_x = df_all[mask_2_virt & (df_all['gameObjectName'] == 'realFingerTip')]['xPos'].values[-1-30:-1]\n",
    "                end_points_z = df_all[mask_2_virt & (df_all['gameObjectName'] == 'realFingerTip')]['zPos'].values[-1-30:-1]\n",
    "                Row_A_point_x.append(np.round(np.nanmean(end_points_x, axis=0),5))\n",
    "                ste_x.append(np.round(np.nanstd(end_points_x) / np.sqrt(len(end_points_x)), 5))\n",
    "                Row_A_point_z.append(np.round(np.nanmean(end_points_z, axis=0),5))\n",
    "                ste_z.append(np.round(np.nanstd(end_points_z) / np.sqrt(len(end_points_z)), 5))\n",
    "                \n",
    "            # Average across the trials \n",
    "            vRow_A_point_x = np.nanmean(vRow_A_point_x, axis=0)\n",
    "            vste_x = np.nanmean(vste_x, axis=0)\n",
    "            vRow_A_point_z = np.nanmean(vRow_A_point_z, axis=0)\n",
    "            vste_z = np.nanmean(vste_z, axis=0)\n",
    "            Row_A_point_x = np.nanmean(Row_A_point_x, axis=0)\n",
    "            ste_x = np.nanmean(ste_x, axis=0)\n",
    "            Row_A_point_z = np.nanmean(Row_A_point_z, axis=0)\n",
    "            ste_z = np.nanmean(ste_z, axis=0)\n",
    "\n",
    "#             dataList = zip(Row_A_point_x, Row_A_point_z, ste_x, ste_z, vRow_A_point_x , vRow_A_point_z, vste_x, vste_z, t)\n",
    "#             tmpDF = pd.DataFrame(dataList, columns=['Real_X_Pos','Real_Z_Pos', 'Real_SE_X',  'Real_SE_Z', 'Virtual_X_Pos','Virtual_Z_Pos', 'Virtual_SE_X',  'Virtual_SE_Z', 'targetID'])    \n",
    "\n",
    "            dataList = [Row_A_point_x, Row_A_point_z, ste_x, ste_z, vRow_A_point_x , vRow_A_point_z, vste_x, vste_z, t, p, height, tempo]\n",
    "            tmpDF = pd.DataFrame([dataList], columns = ['Real_X_Pos', 'Real_Z_Pos', 'Real_X_SE','Real_Z_SE', 'Virt_X_Pos','Virt_Z_Pos','Virt_X_SE','Virt_Z_SE','TargetID','PtxID','Height','Tempo'])\n",
    "#             tmpDF = pd.DataFrame.from_dict(dataDict.items())\n",
    "#             tmpDF = pd.DataFrame.from_dict(dataDict.items(), orient = 'index').T\n",
    "#             tmpDF = pd.DataFrame(list(dataDict.items()), orient = 'index', columns = ['Real_X_Pos', 'Real_Z_Pos','Virt_X_Pos','Virt_Z_Pos','Real_X_SE','Real_Z_SE','Virt_X_SE','Virt_Z_SE','TargetID','PtxID','Height','Tempo'])\n",
    "\n",
    "            # Save info to dataframe\n",
    "            if df_terminal_pos is None:\n",
    "                df_terminal_pos = tmpDF\n",
    "            else:\n",
    "                df_terminal_pos = pd.concat((df_terminal_pos, tmpDF))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "targetID = pd.unique(df_all['targetID'])\n",
    "print(targetID)\n",
    "participants = pd.unique(df_all['PtxID'])\n",
    "print(participants)\n",
    "temps = pd.unique(df_all['tempos'])\n",
    "print(temps)\n",
    "hts = pd.unique(df_all['height'])\n",
    "print(hts)\n",
    "\n",
    "mask_1 = (df_all['PtxID'] == 'Pete') & (df_all['height'] == 'low') & (df_all['tempos'] == '80') & (df_all['targetID'] == 'row_B2')  & (df_all['gameObjectName'] == 'r_index_fingernail_marker')\n",
    "trials = pd.unique(df_all[mask_1]['TrialNum'])\n",
    "    \n",
    "trials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Sort indeces according to target id:\n",
    "df_terminal_pos = df_terminal_pos.sort_values(by=['TargetID', 'PtxID'])\n",
    "\n",
    "mymask = (df_terminal_pos['PtxID'] == ptx) \n",
    "plt.plot(df_terminal_pos[mymask]['TargetID'], df_terminal_pos[mymask]['Real_Z_Pos'],'r-o')\n",
    "plt.plot(df_terminal_pos[mymask]['TargetID'], df_terminal_pos[mymask]['Virt_Z_Pos'],'g-o')\n",
    "plt.legend(['Real','Virtual'])\n",
    "plt.title('Virtual end-position distortion')\n",
    "plt.ylabel('Position / m')\n",
    "plt.xlabel('Target ID')\n",
    "\n",
    "print('Participant: ', ptx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def undistort_point(undistortion_params,r_distorted):\n",
    "    undistorted = r_distorted*(1 + undistortion_params[0] * r_distorted\n",
    "                               + undistortion_params[1] * r_distorted**2\n",
    "                               + undistortion_params[2] * r_distorted**3)\n",
    "    return(undistorted)\n",
    "\n",
    "def fun(undistortion_params,r_distorted, un_distorted):\n",
    "    #Compute residuals.\n",
    "    undistorted = undistort_point(undistortion_params, r_distorted)\n",
    "#     return((undistorted - np.linspace(np.nanmean(r_distorted,axis=0),np.nanmean(r_distorted,axis=0),len(r_distorted)))).ravel()\n",
    "    return((undistorted - un_distorted)).ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r_distorted = df_terminal_pos[mymask]['Virt_Z_Pos']\n",
    "un_distorted = df_terminal_pos[mymask]['Real_Z_Pos']\n",
    "\n",
    "x0 = np.zeros(3).ravel()\n",
    "res = least_squares(fun, x0,  verbose=2, ftol=1e-12,loss='linear', args=([r_distorted, un_distorted]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "xvalz = [0,1,2,3,4,5]\n",
    "undistorted = undistort_point(res.x,r_distorted)    \n",
    "plt.plot(xvalz, r_distorted,label='Oculus Quest',alpha=0.5,color='b',marker = 'o', ms = 10)\n",
    "plt.plot(xvalz, undistorted,label='Corrected',alpha=0.5,color='g',marker = 'o', ms = 10,linewidth = 8)\n",
    "plt.plot(xvalz, un_distorted,label='MoCap',alpha=0.85,color='r',marker = 'o', ms = 10)\n",
    "# plt.plot(xvalz, np.linspace(np.nanmean(r_distorted,axis=0),np.nanmean(r_distorted[0],axis=0),len(r_distorted)),label='target',alpha=0.85,color='r')\n",
    "plt.title('Positional Distortion Metric')\n",
    "plt.xlabel('Target ID')\n",
    "plt.ylabel('Forward Position / m')\n",
    "plt.xlim([-1, 6])\n",
    "plt.ylim([0.2, 0.4])\n",
    "plt.xticks([-1,0,1,2,3,4,5,6],['','1','2','3','4','5','6',''])\n",
    "\n",
    "plt.legend()\n",
    "plt.savefig(ptx + \"_\" + str(np.round(time.time())) + '_DistortionMetric.png', dpi=600, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.round(res.x)) # "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Old code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2D97TQjuSezL"
   },
   "outputs": [],
   "source": [
    "def ComputeErrors2(df_in):\n",
    "    masktemp = (df_in['gameObjectName'] == 'realFingerTip')\n",
    "    adaptationTrialNumbers = pd.unique(df_in[masktemp].TrialNum)\n",
    "    # np.random.shuffle(adaptationTrialNumbers)\n",
    "\n",
    "    df_out = None # If arrays are to be saved use this \n",
    "    dat_List = []\n",
    "\n",
    "    ptxes = pd.unique(df_in['PtxID'])\n",
    "    \n",
    "    for ptx in ptxes:\n",
    "        print('Ptx: ', ptx)\n",
    "        \n",
    "        for i in range(len(adaptationTrialNumbers)):\n",
    "            realFingerMask = (df_in['PtxID'] == ptx) & (df_in['gameObjectName'] == 'realFingerTip') & (df_in['TrialNum'] == adaptationTrialNumbers[i])\n",
    "            virtualFingerMask = (df_in['PtxID'] == ptx) & (df_in['gameObjectName'] == 'r_index_fingernail_marker') & (df_in['TrialNum'] == adaptationTrialNumbers[i])\n",
    "            ptxMask = (df_in['PtxID'] == ptx) & (df_in['gameObjectName'] == 'r_index_fingernail_marker') & (df_in['TrialNum'] == adaptationTrialNumbers[i])\n",
    "\n",
    "            # timeMask = df.loc[(df['gameObjectName'] == 'realFingerTip') & (df['trialNumber'] == adaptationTrialNumbers[i]), ['time']]\n",
    "            timeMask = df_in['TrialNum'] == adaptationTrialNumbers[i]\n",
    "\n",
    "            try:\n",
    "\n",
    "                plt.figure(1) # All positions\n",
    "                plt.plot(df_in[realFingerMask].xPos, df_in[realFingerMask].zPos,'r')\n",
    "                plt.plot(df_in[virtualFingerMask].xPos, df_in[virtualFingerMask].zPos,'g')\n",
    "                plt.title('X-Z Position / m')\n",
    "                plt.gca().set_aspect('equal', adjustable='box')\n",
    "                plt.gca().set_aspect('equal', adjustable='box')\n",
    "                plt.legend(['Real','Virtual'])\n",
    "\n",
    "                ax = plt.figure(2)\n",
    "                tangXZ_Real = np.sqrt(np.power(df_in[realFingerMask].xPos,2) + np.power(df_in[realFingerMask].zPos,2))\n",
    "                tangXZ_Virt = np.sqrt(np.power(df_in[virtualFingerMask].xPos,2) + np.power(df_in[virtualFingerMask].zPos,2))\n",
    "\n",
    "                print('Trial: ', i)\n",
    "\n",
    "                try:\n",
    "                    tangXZ_Real_Vel = np.abs(np.diff(savgol_filter(tangXZ_Real, 75, 4)))\n",
    "                    tangXZ_Virt_Vel = np.abs(np.diff(savgol_filter(tangXZ_Virt, 75, 4)))\n",
    "\n",
    "\n",
    "                    print('Past issue: ', i)\n",
    "\n",
    "                    plt.plot(tangXZ_Real_Vel,'r')\n",
    "                    plt.plot(tangXZ_Virt_Vel,'g')\n",
    "                    plt.title('Lateral Velocity (x-z axis) $\\mathregular{ms^{-1}}$')\n",
    "\n",
    "                    # Sampling Frequency and Time\n",
    "                    times = df_in[timeMask].time.tolist()\n",
    "                    val = np.where(times == numpy.amin(times))\n",
    "                    startTimeIdx = val[0][0]\n",
    "                    # print('min: ', startTimeIdx)\n",
    "\n",
    "                    print('Past 2nd issue: ', i)\n",
    "\n",
    "                    plt.figure(3) \n",
    "                    plt.plot(times)\n",
    "                    plt.plot(startTimeIdx, times[startTimeIdx], 'rx')\n",
    "                    # print('Movement duration: ', times[-1], 's')\n",
    "                    startMovIdx = int(np.round((startTimeIdx/10))) # Convert between time series and movement array by dividing by 10? \n",
    "                    sampleFreq = np.round(len(tangXZ_Real_Vel[startMovIdx:])/times[-1])\n",
    "                    # print('Sampling Freq: ', sampleFreq)\n",
    "\n",
    "                    print('Past 3rd issue: ', i)\n",
    "\n",
    "                    #--------------- Cross-Correlation ---------------------------------------------\n",
    "                    corr = np.correlate(tangXZ_Real_Vel - np.mean(tangXZ_Real_Vel), \n",
    "                                      tangXZ_Virt_Vel - np.mean(tangXZ_Virt_Vel),\n",
    "                                      mode='full')\n",
    "                    sampleDifference = np.argmax(tangXZ_Virt_Vel[20:]) - np.argmax(tangXZ_Real_Vel[20:])\n",
    "\n",
    "                    print('Past correlation issue: ', i)\n",
    "\n",
    "                    if sampleDifference > 50:\n",
    "                        lag = (sampleDifference  * (1/sampleFreq)) * 1000\n",
    "                        print('Lag is too large: ' , lag)\n",
    "                    else: \n",
    "                        lag = (sampleDifference  * (1/sampleFreq)) * 1000\n",
    "                        # print('Lag: ', np.round(lag), 'ms')\n",
    "\n",
    "                        #--------------- Positional-Error ---------------------------------------------\n",
    "                        # MSE = np.sum(np.power(np.abs((tangXZ_Real.values[20:] - tangXZ_Virt.values[20:]),2))) / len(tangXZ_Real.values[20:])\n",
    "\n",
    "                        print('Past if statement: ', i)\n",
    "\n",
    "                        MSE = np.round(np.sum(np.power(np.abs(tangXZ_Real.values[20:] - tangXZ_Virt.values[20:]),2)) / len(tangXZ_Real.values[20:]),4) * 100 # Convert to cm\n",
    "                        endPosError = (np.round(np.nanmean(np.abs(tangXZ_Virt.values[-1-30:-1] - tangXZ_Real.values[-1-30:-1])),3)/30) * 100 # Convert to cm\n",
    "                        posError = np.round(np.nanmean(np.abs(tangXZ_Virt.values[20:] - tangXZ_Real.values[20:])),3) * 100 # Convert to cm\n",
    "                        velError = np.round(np.nanmean(np.abs(tangXZ_Virt_Vel[20:] - tangXZ_Real_Vel[20:])),3) # Convert to cm\n",
    "\n",
    "                        # print('Mean Square Error: ', MSE, 'cm')\n",
    "                        # print('Target Hit Error: ', endPosError, 'cm')\n",
    "                        # print('Average Positional Error: ', posError, 'cm')\n",
    "                        # print('Velocity Error: ', velError, 'ms-1')\n",
    "\n",
    "                        dat_List.append([lag, MSE, endPosError, posError, velError, times[-1]])\n",
    "\n",
    "\n",
    "\n",
    "                except Exception as e:\n",
    "                    print('MY_ERROR: Size of array: ', len(tangXZ_Real))\n",
    "                    zeroArray = np.tile(np.zeros, (1, 100))\n",
    "                    tangXZ_Real_Vel = zeroArray\n",
    "                    tangXZ_Virt_Vel = zeroArray\n",
    "\n",
    "            except Exception as ex:\n",
    "                print('My_Error_2: ', ex)\n",
    "    \n",
    "        \n",
    "        df_tmp = pd.DataFrame(dat_List, columns =['Lag' ,'MSE_Error', 'Hit_Error', 'AvPos_Error', 'Vel_Error', 'Time'])\n",
    "        df_tmp.insert(0, 'PtxID', ptx, True)\n",
    "\n",
    "        if df_out is None:\n",
    "            df_out = df_tmp\n",
    "        else:\n",
    "            df_out =df_out.loc[:,~df_out.columns.duplicated()]\n",
    "            df_out = pd.concat((df_out, df_tmp))            \n",
    "        # times = np.arange(0,)\n",
    "\n",
    "    df_out =df_out.loc[:,~df_out.columns.duplicated()]\n",
    "    return df_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vrs9N_QeWiga"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ux1CtEO3jKjX"
   },
   "outputs": [],
   "source": [
    "df_bmp160_low_errors = ComputeErrors2(df_bmp160_low)\n",
    "df_bmp160_mid_errors = ComputeErrors2(df_bmp160_mid)\n",
    "df_bmp160_high_errors = ComputeErrors2(df_bmp160_high)\n",
    "\n",
    "df_bmp120_low_errors = ComputeErrors2(df_bmp120_low)\n",
    "df_bmp120_mid_errors = ComputeErrors2(df_bmp120_mid)\n",
    "df_bmp120_high_errors = ComputeErrors2(df_bmp120_high)\n",
    "\n",
    "df_bmp80_low_errors = ComputeErrors2(df_bmp80_low)\n",
    "df_bmp80_mid_errors = ComputeErrors2(df_bmp80_mid)\n",
    "df_bmp80_high_errors = ComputeErrors2(df_bmp80_high)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Processed ptx: ', pd.unique(df_bmp160_low['PtxID']))\n",
    "print('Original ptx: ', pd.unique(df_all['PtxID']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bmp160_mid_errors\n",
    "# df_bmp80_low_errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FRc8FCyipTmP"
   },
   "outputs": [],
   "source": [
    "def AverageSTE(dataz):\n",
    "    ste = np.nanstd(dataz) / np.sqrt(len(dataz))\n",
    "    avr = np.nanmean(dataz)\n",
    "\n",
    "    return avr,ste\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u9b9aN7xVnCG"
   },
   "outputs": [],
   "source": [
    "\n",
    "# sns.displot(df_bmp160_mid_errors, x=\"Lag\", binwidth=10)\n",
    "xVals = np.arange(len(df_bmp160_high_errors.Lag))\n",
    "# g = sns.relplot(x=xVals, y=\"Lag\", hue=\"Hit_Error\", data=df_bmp160_mid_errors)\n",
    "g = sns.relplot(x=xVals, y=\"Hit_Error\", size=\"Lag\", sizes=(10, 200), data=df_bmp160_high_errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bmp120_mid_errorsArr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kK_huP-Hl_Q_",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# MSE, endPosError, posError, velError\n",
    "bmp80_low_errorsArr = np.array(df_bmp80_low_errors)\n",
    "bmp120_mid_errorsArr = np.array(df_bmp120_mid_errors)\n",
    "bmp160_high_errorsArr = np.array(df_bmp160_high_errors)\n",
    "\n",
    "plt.figure()\n",
    "# plt.plot(bmp80_low_errorsArr[:,0],'r-o')\n",
    "plt.plot(bmp120_mid_errorsArr[:,1],'g-o')\n",
    "plt.plot(bmp160_high_errorsArr[:,2],'b-o')\n",
    "plt.legend(['End PosErr','Path Error'])\n",
    "plt.ylim([-0.5, 100])\n",
    "plt.title('Error Between Quest and MoCap')\n",
    "plt.xlabel('Trial Number')\n",
    "plt.ylabel('Error / cm')\n",
    "\n",
    "# plt.figure()\n",
    "# av,se = AverageSTE(bmp120_mid_errorsArr[:,0])\n",
    "# plt.errorbar([0,1,2], [av, np.nan, np.nan], [np.nan, np.nan, se],ms=15,color='r')\n",
    "\n",
    "# plt.legend(['MSE','End PosErr','Average Error'])\n",
    "\n",
    "plt.savefig(str(np.round(time.time())) + '_Errors_EndPointPath.png', dpi=600, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "D6VybF_oEJa9"
   },
   "outputs": [],
   "source": [
    "# # Set theme\n",
    "# sns.set_style('whitegrid')\n",
    " \n",
    "# # Violin plot\n",
    "# sns.violinplot(x='xPos', y='zPos', data=df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bmp160_mid_errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1YQLHoxo8eCM",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "trials = np.arange(0, len(df_bmp80_low_errors))\n",
    "df_bmp80_low_errors.insert(0, 'TrialNum', trials)\n",
    "\n",
    "trials = np.arange(0, len(df_bmp120_low_errors))\n",
    "df_bmp120_low_errors.insert(0, 'TrialNum', trials)\n",
    "\n",
    "trials = np.arange(0, len(df_bmp160_low_errors))\n",
    "df_bmp160_low_errors.insert(0, 'TrialNum', trials)\n",
    "\n",
    "trials = np.arange(0, len(df_bmp80_mid_errors))\n",
    "df_bmp80_mid_errors.insert(0, 'TrialNum', trials)\n",
    "trials = np.arange(0, len(df_bmp120_mid_errors))\n",
    "df_bmp120_mid_errors.insert(0, 'TrialNum', trials)\n",
    "trials = np.arange(0, len(df_bmp160_mid_errors))\n",
    "df_bmp160_mid_errors.insert(0, 'TrialNum', trials)\n",
    "\n",
    "trials = np.arange(0, len(df_bmp80_high_errors))\n",
    "df_bmp80_high_errors.insert(0, 'TrialNum', trials)\n",
    "trials = np.arange(0, len(df_bmp120_high_errors))\n",
    "df_bmp120_high_errors.insert(0, 'TrialNum', trials)\n",
    "trials = np.arange(0, len(df_bmp160_high_errors))\n",
    "df_bmp160_high_errors.insert(0, 'TrialNum', trials)\n",
    "\n",
    "df_bmp160_high_errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bmp160_high_errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V8mlBaLKEg2k"
   },
   "outputs": [],
   "source": [
    "# Density Plot\n",
    "# sns.kdeplot(df.xPos, df.zPos)\n",
    "fig, axs = plt.subplots(3, 3, figsize=(12, 5), sharey=True)\n",
    "fig.suptitle('AvPos_Error / cm')\n",
    "\n",
    "axs[0, 0].set_title('Low 80 BPM')\n",
    "sns.distplot(df_bmp80_low_errors.AvPos_Error, ax=axs[0,0])\n",
    "\n",
    "axs[0, 1].set_title('LOW 120 BPM')\n",
    "sns.distplot(df_bmp120_low_errors.AvPos_Error, ax=axs[0,1])\n",
    "\n",
    "axs[0, 2].set_title('LOW 160 BPM')\n",
    "sns.distplot(df_bmp160_low_errors.AvPos_Error, ax=axs[0,2])\n",
    "\n",
    "axs[1, 0].set_title('MID 80 BPM')\n",
    "sns.distplot(df_bmp80_mid_errors.AvPos_Error, ax=axs[1,0])\n",
    "\n",
    "axs[1, 1].set_title('MID 120 BPM')\n",
    "sns.distplot(df_bmp120_mid_errors.AvPos_Error, ax=axs[1,1])\n",
    "\n",
    "axs[1, 2].set_title('MID 160 BPM')\n",
    "sns.distplot(df_bmp160_mid_errors.AvPos_Error, ax=axs[1,2])\n",
    "\n",
    "axs[2, 0].set_title('HIGH 80 BPM')\n",
    "sns.distplot(df_bmp80_high_errors.AvPos_Error, ax=axs[2,0])\n",
    "\n",
    "axs[2, 1].set_title('HIGH 120 BPM')\n",
    "sns.distplot(df_bmp120_high_errors.AvPos_Error, ax=axs[2,1])\n",
    "\n",
    "axs[2, 2].set_title('HIGH 160 BPM')\n",
    "sns.distplot(df_bmp160_high_errors.AvPos_Error, ax=axs[2,2])\n",
    "\n",
    "plt.figure()\n",
    "plt.title('HIGH 160 BPM')\n",
    "sns.distplot(df_bmp160_high_errors.AvPos_Error)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "48otdlXFijD4"
   },
   "source": [
    "## Create master metric table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bmp80_low_errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9J5zIXv3iiNZ",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_all_errors = None\n",
    "\n",
    "df_bmp80_low_errors.insert(0, 'height', 0, True)\n",
    "df_bmp80_low_errors.insert(0, 'tempo', 80, True)\n",
    "df_all_errors = df_bmp80_low_errors \n",
    "\n",
    "df_bmp120_low_errors.insert(0, 'height', 0, True)\n",
    "df_bmp120_low_errors.insert(0, 'tempo', 120, True)\n",
    "df_all_errors =  df = pd.concat((df_all_errors, df_bmp120_low_errors))\n",
    "\n",
    "df_bmp160_low_errors.insert(0, 'height', 0, True)\n",
    "df_bmp160_low_errors.insert(0, 'tempo', 160, True)\n",
    "df_all_errors =  df = pd.concat((df_all_errors, df_bmp160_low_errors))\n",
    "\n",
    "df_bmp80_mid_errors.insert(0, 'height', 1, True)\n",
    "df_bmp80_mid_errors.insert(0, 'tempo', 80, True)\n",
    "df_all_errors =  df = pd.concat((df_all_errors, df_bmp80_mid_errors))\n",
    "\n",
    "df_bmp120_mid_errors.insert(0, 'height', 1, True)\n",
    "df_bmp120_mid_errors.insert(0, 'tempo', 120, True)\n",
    "df_all_errors =  df = pd.concat((df_all_errors, df_bmp120_mid_errors))\n",
    "\n",
    "df_bmp160_mid_errors.insert(0, 'height', 2, True)\n",
    "df_bmp160_mid_errors.insert(0, 'tempo', 160, True)\n",
    "df_all_errors =  df = pd.concat((df_all_errors, df_bmp160_mid_errors))\n",
    "\n",
    "df_bmp80_high_errors.insert(0, 'height', 2, True)\n",
    "df_bmp80_high_errors.insert(0, 'tempo', 80, True)\n",
    "df_all_errors =  df = pd.concat((df_all_errors, df_bmp80_high_errors))\n",
    "\n",
    "df_bmp120_high_errors.insert(0, 'height', 2, True)\n",
    "df_bmp120_high_errors.insert(0, 'tempo', 120, True)\n",
    "df_all_errors =  df = pd.concat((df_all_errors, df_bmp120_high_errors))\n",
    "\n",
    "df_bmp160_high_errors.insert(0, 'height', 2, True)\n",
    "df_bmp160_high_errors.insert(0, 'tempo', 160, True)\n",
    "df_all_errors =  df = pd.concat((df_all_errors, df_bmp160_high_errors))\n",
    "\n",
    "df_all_errors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PbUgTYb6N53H"
   },
   "source": [
    "# TODO\n",
    "- [ ] Incorporate targets into the analysis\n",
    "- [ ] Cleanup the notebook "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5yuNDMqUWVSz"
   },
   "outputs": [],
   "source": [
    "# sns.regplot(x='TrialNum', y='Lag', x_bins=4, data=df_bmp120_high_errors, x_estimator=np.mean)\n",
    "# sns.violinplot(x='TrialNum', y='Lag', data=df_bmp120_high_errors)\n",
    "# sns.regplot(x=\"TrialNum\", y=\"Vel_Error\", data=df_bmp120_high_errors,x_estimator=np.mean, logx=True)\n",
    "\n",
    "# sns.relplot(x=\"TrialNum\", y=\"MSE_Error\", kind=\"line\", data=df_bmp120_high_errors)\n",
    "\n",
    "# sns.set_theme(style=\"whitegrid\")\n",
    "# sns.violinplot(data=df_bmp120_high_errors, x=\"Vel_Error\", y=\"MSE_Error\", hue=\"Lag\", split=False, inner=\"quart\", linewidth=1) #,palette={\"Yes\": \"b\", \"No\": \".85\"})\n",
    "# sns.despine(left=True)\n",
    "\n",
    "sns.set_theme(style=\"darkgrid\")\n",
    "cmap = sns.cubehelix_palette(rot=-.2, as_cmap=True)\n",
    "g = sns.relplot(\n",
    "    data=df_all_errors,\n",
    "    x=\"TrialNum\", y=\"Hit_Error\", hue=\"height\", size = \"tempo\", sizes=(75, 225), palette=\"Blues\",alpha=0.85\n",
    ")\n",
    "plt.ylim([-0.25, 5])\n",
    "plt.xlabel('Trial Number')\n",
    "plt.ylabel('Positional Error / cm')\n",
    "plt.title('Path Error Between Quest and MoCap')\n",
    "\n",
    "plt.savefig(str(np.round(time.time())) + 'Path Error.png', dpi=600, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_all_errors.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "\n",
    "# g1 = sns.barplot(x='tempo', y=\"height\", palette=\"Reds\", data=df_all_errors, estimator=np.nanmean)\n",
    "# # plt.ylim([0, 0.15])\n",
    "# plt.xlabel('Tempo')\n",
    "\n",
    "ptxMeanErrTH = df_all_errors.groupby(['PtxID', 'tempo', 'height'])['Hit_Error'].mean()\n",
    "df_Tempo_Height = ptxMeanErrTH.to_frame()\n",
    "\n",
    "\n",
    "sns.scatterplot(x = 'tempo', y='height', size='Hit_Error', sizes=(100, 400), alpha=.5, data=df_Tempo_Height)\n",
    "\n",
    "plt.legend(bbox_to_anchor=(1.05, 1.0), loc='upper left')\n",
    "plt.title('Average Hit Error / m',fontsize=14)\n",
    "plt.xlabel('Tempo (bpm)')\n",
    "plt.xticks([80,120,160],['Slow','Medium','Fast'])\n",
    "plt.ylabel('Height')\n",
    "plt.yticks([0,1,2],['Low','Mid','Heigh'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.subplot(221)\n",
    "g1 = sns.barplot(x='tempo', y=\"Hit_Error\", palette=\"Reds\", data=df_all_errors, estimator=np.nanmean)\n",
    "plt.ylim([0, 0.15])\n",
    "plt.xlabel('Tempo')\n",
    "\n",
    "plt.subplot(222)\n",
    "g2 = sns.barplot(x='tempo', y=\"AvPos_Error\", palette=\"Reds\", data=df_all_errors, estimator=np.nanmean)\n",
    "plt.ylim([0, 10])\n",
    "plt.xlabel('Tempo')\n",
    "\n",
    "plt.subplot(223)\n",
    "g1 = sns.barplot(x='height', y=\"Hit_Error\", palette=\"Blues\", data=df_all_errors, estimator=np.nanmean)\n",
    "plt.ylim([0, 0.15])\n",
    "plt.xlabel('Height')\n",
    "\n",
    "plt.subplot(224)\n",
    "g2 = sns.barplot(x='height', y=\"AvPos_Error\", palette=\"Blues\", data=df_all_errors, estimator=np.nanmean)\n",
    "plt.ylim([0, 10])\n",
    "plt.xlabel('Height')\n",
    "\n",
    "# plt.title('Average \\n' + 'metric' + \"\\n \\n\")\n",
    "# plt.ylabel('metric' + \" / Degrees\") #\n",
    "# plt.xlabel('Phases')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sNnwLATemqfH",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ax = sns.barplot(x=\"height\", y=\"AvPos_Error\", palette=\"Greens\", data=df_all_errors, estimator=np.nanmean)\n",
    "plt.title('Cummulative Path Error \\n Between Quest and MoCap \\n \\n')\n",
    "plt.ylabel('Positional Error / cm')\n",
    "plt.xlabel('Height')\n",
    "plt.xticks([0,1,2],['Low','Mid','High'])\n",
    "\n",
    "plt.savefig(str(np.round(time.time())) + 'Cummulative Error Height.png', dpi=600, bbox_inches='tight')\n",
    "\n",
    "statannot.add_stat_annotation(\n",
    "    ax,\n",
    "    data=df_all_errors,\n",
    "    x='tempo',\n",
    "    y='AvPos_Error',\n",
    "    hue='height',\n",
    "#     box_pairs=[(\"80\", '80'), ('80','80'), ('80', '80')],\n",
    "    test=\"t-test_ind\",\n",
    "    text_format=\"star\",\n",
    "    loc=\"outside\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lowMask = (df_all_errors['height'] == 0) & (df_all_errors['tempo'] == 120)\n",
    "midMask = (df_all_errors['height'] == 1) & (df_all_errors['tempo'] == 120)\n",
    "highMask = (df_all_errors['height'] == 2) & (df_all_errors['tempo'] == 120)\n",
    "\n",
    "sci.ttest_ind(df_all_errors[midMask]['AvPos_Error'] , df_all_errors[highMask]['AvPos_Error'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4y8dcMpucf7W"
   },
   "outputs": [],
   "source": [
    "# import scipy.stats as sci \n",
    "\n",
    "# PlotErrorBars2(bmaxZVels, 'r')\n",
    "# PlotErrorBars2(amaxZVels, 'g')\n",
    "\n",
    "# print('baseline: ', np.shape(bmaxZVels), 'adaptation: ', np.shape(amaxZVels))\n",
    "# h = sci.ttest_ind(bmaxZVels, amaxZVels)\n",
    "# print('ttest: ', h)\n",
    "\n",
    "# plt.xticks(ticks = [0], labels=['Z'])\n",
    "# plt.ylabel('Max Velocity ms-1')\n",
    "# plt.xlabel('Axis')\n",
    "# plt.title('Average Max Velocity')\n",
    "# plt.legend(['Baseline','Adaptation'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Reshape and average standard errors:\n",
    "# print('Arr: ', vRow_A_point_z)\n",
    "\n",
    "# # Sort indeces according to target id:\n",
    "# targetSortedIdx = np.argsort(target)\n",
    "# targets = np.take(target,targetSortedIdx)\n",
    "\n",
    "# vrow_A_point_z = np.take(vRow_A_point_z, targetSortedIdx)\n",
    "\n",
    "# print('\\n target: ', target, ' targets: ', targets)\n",
    "\n",
    "# xVals = np.linspace(0,len(targets)-1, len(targets))\n",
    "# print('Size of X vals: ', np.shape(xVals), ' Vals: ', np.shape(vrow_A_point_z), ' SE: ', np.shape(ste_z))\n",
    "\n",
    "# # Real finger tip \n",
    "# plt.figure()\n",
    "# plt.subplot(121)\n",
    "# plt.errorbar(xVals, vrow_A_point_z, vste_z, color = 'b', ms=8, alpha=0.5)\n",
    "# plt.plot(vrow_A_point_z,'ko',ms=8,alpha=1.0)\n",
    "# plt.xticks(xVals, targets, rotation=45)\n",
    "# plt.ylim([0, 0.6])\n",
    "# plt.xlim([-1, 6])\n",
    "# plt.xlabel('Target ID')\n",
    "# plt.ylabel('Virtual Finger Forward Position / m')\n",
    "# plt.title(p + ' Virtual')\n",
    "\n",
    "# # Real finger tip \n",
    "# row_A_point_z = np.take(Row_A_point_z,targetSortedIdx)\n",
    "# targets = np.take(target,targetSortedIdx)\n",
    "\n",
    "# plt.subplot(122)\n",
    "# plt.errorbar(xVals, row_A_point_z, ste_z, color = 'b', ms=8, alpha=0.5)\n",
    "# plt.plot(row_A_point_z,'ko',ms=8,alpha=1.0)\n",
    "# plt.xticks(xVals, targets, rotation=45)\n",
    "# plt.ylim([0, 0.6])\n",
    "# plt.xlim([-1, 6])\n",
    "# plt.xlabel('Target ID Row: ' + trow)\n",
    "# plt.ylabel('Real Finger Forward Position / m')\n",
    "# plt.title(p + ' Real')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#     try:\n",
    "#         if len(vste_x) > 24:\n",
    "#             vste_x = np.delete(vste_x, -1)\n",
    "#         if len(vste_z) > 24:\n",
    "#             vste_z = np.delete(vste_z, -1)\n",
    "#         if len(ste_x) > 24:\n",
    "#             ste_x = np.delete(ste_x, -1)\n",
    "#         if len(ste_z) > 24:\n",
    "#             ste_z = np.delete(ste_z, -1)\n",
    "#     except Exception as e:\n",
    "#         vste_x = np.zeros(6)\n",
    "#         vste_z = np.zeros(6)\n",
    "#         ste_x = np.zeros(6)\n",
    "#         ste_z = np.zeros(6)\n",
    "# \n",
    "#     vste_x = np.nanmean(np.reshape(vste_x, (4, 6)), axis=0)\n",
    "#     vste_z = np.nanmean(np.reshape(vste_z, (4, 6)), axis=0)\n",
    "#     ste_x = np.nanmean(np.reshape(ste_x, (4, 6)), axis=0)\n",
    "#     ste_z = np.nanmean(np.reshape(ste_z, (4, 6)), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_motions = None\n",
    "# np.set_printoptions(suppress=True)\n",
    "# newArrLength = 100\n",
    "\n",
    "# # mask = (df_all['PtxID'] == 'Susan') & (df_all['gameObjectName'] == 'realFingerTip') & (df_all['height'] == 'low') & (df_all['tempos'] == '80') & (df_all['TrialNum'] == '42')\n",
    "# ptxIds = pd.unique(df_all['PtxID'])\n",
    "# print('Participants: ', ptxIds)\n",
    "# # Ptx number: \n",
    "# # 0 = Susan\n",
    "# # 1 = Davide\n",
    "# # 2 = Joe\n",
    "# # 3 = Poppy\n",
    "# # 4 = Katrina\n",
    "# # 5 = Max\n",
    "# # 6 = Pete\n",
    "\n",
    "# includedPtxs = [0,1,3,4,6]\n",
    "\n",
    "# ptxNum = 2 \n",
    "# tempores = 'noTempo'\n",
    "# heightes = 'low'\n",
    "\n",
    "# trials = np.arange(0,72)\n",
    "\n",
    "# # for partici in includedPtxs:\n",
    "\n",
    "# for trial in trials:\n",
    "    \n",
    "#     print('Trial number: ', trial)\n",
    "    \n",
    "#     try:\n",
    "#         mask = (df_all['TrialNum'] == str(trial)) & (df_all['PtxID'] == ptxIds[ptxNum]) & (df_all['tempos'] == tempores) & (df_all['gameObjectName'] == 'realFingerTip') & (df_all['height'] == heightes)\n",
    "#         mask_virt = (df_all['TrialNum'] == str(trial)) & (df_all['PtxID'] == ptxIds[ptxNum]) & (df_all['tempos'] == tempores) & (df_all['gameObjectName'] == 'r_index_fingernail_marker') & (df_all['height'] == heightes)\n",
    "\n",
    "#         # Compute actual sampling rate\n",
    "#         timetaken = df_all[mask]['time'].values\n",
    "#         timetaken2 = ResizeArray(timetaken, newArrLength)\n",
    "#         timetaken3 = np.round(timetaken2,1)\n",
    "#         timetaken3 = timetaken3.tolist()\n",
    "#         indexOfStart = timetaken3.index(0.0) # indexOfStart = np.where(timetaken == 0.0)\n",
    "#         samplingRate = np.round(1.0 / ((len(timetaken3)-indexOfStart) / timetaken3[-1]), 4)\n",
    "#         print('Sampling Rate: ', np.round(1.0/samplingRate))\n",
    "        \n",
    "#         # Get individual velocities for real hand ---------------------------------------\n",
    "#         pos_x = ResizeArray(df_all[mask]['xPos'].values, newArrLength)\n",
    "#         pos_xf = savgol_filter(pos_x, 21, 9)\n",
    "#         vel_x = np.gradient(pos_xf / samplingRate)\n",
    "\n",
    "#         pos_y = ResizeArray(df_all[mask]['yPos'].values, newArrLength)\n",
    "#         pos_yf = savgol_filter(pos_y, 21, 9)\n",
    "#         vel_y = np.gradient(pos_yf / samplingRate)\n",
    "\n",
    "#         pos_z = ResizeArray(df_all[mask]['zPos'].values, newArrLength)\n",
    "#         pos_zf = savgol_filter(pos_z, 21, 9)\n",
    "#         vel_z = np.gradient(pos_zf / samplingRate)\n",
    "#         vel_type_1 = np.sqrt(np.power(vel_x,2) + np.power(vel_y,2) + np.power(vel_z,2))\n",
    "#         vel_type_1f = savgol_filter(vel_type_1, 21, 9)\n",
    "\n",
    "#         # Get individual velocities for virtual hand ---------------------------------------\n",
    "#         pos_xv = ResizeArray(df_all[mask_virt]['xPos'].values, newArrLength)\n",
    "#         pos_xfv = savgol_filter(pos_xv, 21, 9)\n",
    "#         vel_xv = np.gradient(pos_xfv / samplingRate)\n",
    "\n",
    "#         pos_yv = ResizeArray(df_all[mask_virt]['yPos'].values, newArrLength)\n",
    "#         pos_yfv = savgol_filter(pos_yv, 21, 9)\n",
    "#         vel_yv = np.gradient(pos_yfv / samplingRate)\n",
    "\n",
    "#         pos_zv = ResizeArray(df_all[mask_virt]['zPos'].values, newArrLength)\n",
    "#         pos_zfv = savgol_filter(pos_zv, 21, 9)\n",
    "#         vel_zv = np.gradient(pos_zfv / samplingRate)\n",
    "        \n",
    "#         vel_type_1v = np.sqrt(np.power(vel_xv,2) + np.power(vel_yv,2) + np.power(vel_zv,2))\n",
    "#         vel_type_1fv = savgol_filter(vel_type_1v, 21, 9)\n",
    "#         # ------------------------------------------------------------------------------\n",
    "        \n",
    "#         pos_tx = df_all[mask]['xTPos'].values\n",
    "#         pos_ty = df_all[mask]['yTPos'].values\n",
    "#         pos_tz = df_all[mask]['zTPos'].values\n",
    "\n",
    "#         # Pos\n",
    "#         plt.figure()\n",
    "#         plt.subplot(1,3,1)\n",
    "#         plt.plot(pos_x, pos_z,'k-o')\n",
    "#         plt.plot(pos_xv, pos_zv,'m-o')\n",
    "#         plt.plot(pos_tx, pos_tz,'r-o',ms=10)\n",
    "#         plt.plot([pos_x, pos_xv], [pos_z, pos_zv],'b-',alpha=0.5) # Connection line between trajectories to indicate delays\n",
    "#         plt.legend(['XZ Motion','XZ Virtual','Target'])\n",
    "        \n",
    "#         # Last positional data point in different colours for visualisation purposes \n",
    "#         plt.subplot(1,3,3)\n",
    "#         plt.plot(pos_x[-1-5:-1], pos_z[-1-5:-1],'k-o', ms=8)\n",
    "#         plt.plot(pos_xv[-1-5:-1], pos_zv[-1-5:-1],'m-o', ms=6)\n",
    "#         plt.plot([pos_x[-1], pos_xv[-1]], [pos_z[-1], pos_zv[-1]], 'r-o', ms=15, alpha=0.5)\n",
    "        \n",
    "# #         plt.legend(bbox_to_anchor=(1.05, 1.0), loc='upper left')\n",
    "# #         plt.axis('equal')\n",
    "    \n",
    "#         # Vel\n",
    "#         plt.subplot(1,3,2)\n",
    "#         plt.plot(vel_type_1,'r-')\n",
    "#         plt.plot(vel_type_1f,'g-')\n",
    "#         plt.plot(vel_type_1fv,'m-')\n",
    "#         startIdx = indexOfStart\n",
    "#         plt.plot([startIdx,startIdx],[0,2.75],'k--',linewidth=3)\n",
    "#         plt.title('Vel_Type_1')\n",
    "#         plt.legend(['Raw','Filtered','Filt Virt','Start'])\n",
    "# #         plt.legend(bbox_to_anchor=(1.05, 1.0), loc='upper left')\n",
    "# #         plt.axis('equal')\n",
    "#         plt.ylim([-0.1, 5])\n",
    "        \n",
    "#         # Velocity around start of trial \n",
    "#         realVel = vel_type_1f[startIdx-15:startIdx+15]\n",
    "#         virtVel = vel_type_1fv[startIdx-15:startIdx+15]\n",
    "        \n",
    "#         lag = CrossCorr(realVel, virtVel, samplingRate)\n",
    "#         print('Lag: ', np.round(lag, 2), ' ms')\n",
    "        \n",
    "#         maxVel = np.max(realVel)\n",
    "#         maxVelVirt = np.max(virtVel)\n",
    "#         print('Max vel: \\n', 'Real: ', np.round(maxVel,2),'\\n', 'Virt: ', np.round(maxVelVirt,2))\n",
    "        \n",
    "#         real = np.asarray([pos_x,pos_z])\n",
    "#         virt = np.asarray([pos_xv,pos_zv])\n",
    "        \n",
    "#         manhattanErr = np.sum(np.abs(real-virt)) / len(pos_x)\n",
    "        \n",
    "#         ns = 5\n",
    "#         p1m = [np.nanmean(pos_x[-1-ns:-1]), np.nanmean(pos_z[-1-ns:-1])]\n",
    "#         p2m = [np.nanmean(pos_xv[-1-ns:-1]), np.nanmean(pos_zv[-1-ns:-1])]\n",
    "#         distance2 = np.round(math.sqrt( ((p1m[0]-p2m[0])**2)+((p1m[1]-p2m[1])**2) ) * 100,2)\n",
    "        \n",
    "#         print('Per sample path offset (Manhattan err): ', np.round(manhattanErr * 100,2), ' cm')\n",
    "#         print('End point err: ', distance2, ' cm')\n",
    "        \n",
    "#         plt.tight_layout()\n",
    "#         plt.show()\n",
    "        \n",
    "#     except Exception as e:\n",
    "#         print('Err: ', e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "QuestAccu_Analysis_v1.ipynb",
   "provenance": [
    {
     "file_id": "1x8GseTUqsxeAXxk9jb9HS5G_eJAGY3ij",
     "timestamp": 1633430734454
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
